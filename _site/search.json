[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Andreas Handel - Brief professional bio",
    "section": "",
    "text": "Research focus\nI work on data analytics and modeling of infectious diseases, mainly influenza, tuberculosis and norovirus. Lately, I have also worked a lot on COVID-19. I uses mathematical models, computational simulations and statistical analysis to understand the dynamics of pathogens on different spatial and temporal scales. I work both on the within-host level (the immunology, virology, microbiology scale) and the population level (the epidemiology, ecology, evolution scale). The ultimate goal of my work is to help design better intervention and control strategies against infectious diseases, both for individual patients and on the population level.\nDescriptions of some current research projects can be found on my research group website. Slides for recent talks and presentations are located on this website and can be found here.\n\n\nPrevious positions\nFrom 2004 - 2009, I was a Postdoctoral Fellow in the Department of Biology at Emory University in the group of Rustom Antia. During my Postdoc I made the transition from physics to infectious disease modeling.\nFrom 1999 - 2004, I did a PhD in theoretical physics at the Center for Nonlinear Science and School of Physics, Georgia Institute of Technology. I worked with Roman Grigoriev on the topic of localized control of spatially extended, nonlinear dynamical systems.\nI grew up in Germany and started my undergrad studies in 1996 at the University of Stuttgart, in Physics. I received my B.S. (or what could be considered the German equivalent) in 1999, after which I moved to the U.S.\n\n\nAdditional, completely irrelevant information\nMy current Erdős number is (as far as I’m aware) 4. (Paul Erdős -> Ernst Gabor Straus -> Raymond M. Redheffer -> Sergei S. Pilyugin -> Andreas Handel)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andreas Handel",
    "section": "",
    "text": "I am a Professor in the Department of Epidemiology and Biostatistics, College of Public Health, at the University of Georgia (UGA).\nMy research and teaching focus on data analysis and modeling of infectious diseases.\nThis website contains a short bio and products such as blog posts, presentations, and various tools and resources related to teaching and research.\nMore research related details can be found on my research group site. Some overlap between the two sites might occur."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\nHugo\n\n\nWebsite\n\n\n\n\nI recently moved this website from a Hugo/blogdown/Wowchemy setup to Quarto. This contains a few thoughts and tips.\n\n\n\n\n\n\nOct 1, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR Package\n\n\nVisualization\n\n\n\n\nA brief post showing how to use the flowdiagramr R package to make flowcharts.\n\n\n\n\n\n\nJun 11, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nBayesian\n\n\n\n\nSome more musings and explorations that didn’t fit into the main posts of this series.\n\n\n\n\n\n\nFeb 25, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nBayesian\n\n\n\n\nPart 3 of a tutorial showing how to fit Bayesian models using the brms package.\n\n\n\n\n\n\nFeb 24, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nBayesian\n\n\n\n\nPart 2 of a tutorial showing how to fit Bayesian models using the rethinking package.\n\n\n\n\n\n\nFeb 23, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nBayesian\n\n\n\n\nPart 1 of a tutorial showing how to specify models and simulate data for a longitudinal multilevel setup.\n\n\n\n\n\n\nFeb 22, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nA TidyTuesday exercise\n\n\n\n\nR\n\n\nData Analysis\n\n\nTidy Tuesday\n\n\n\n\nAn exploration of some papers catalogued in NBER\n\n\n\n\n\n\nSep 30, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR Markdown\n\n\nGitHub\n\n\nWebsite\n\n\ndistill\n\n\n\n\nThe following are step-by-step instructions for creating a website using R Markdown and the distill R package (through R Studio) and GitHub.\n\n\n\n\n\n\nMar 21, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR Markdown\n\n\nGitHub\n\n\nWebsite\n\n\nTutorial\n\n\n\n\nThe following are step-by-step instructions for creating a fairly basic but still useful website using R Markdown (through R Studio) and Github.\n\n\n\n\n\n\nJan 11, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTutorial\n\n\nR Markdown\n\n\n\n\nExamples for automatically creating customized PDF or Word documents using R and R Markdown\n\n\n\n\n\n\nOct 15, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nR Markdown\n\n\nWord\n\n\nTutorial\n\n\n\n\nTurning R Markdown into a Word document that contains custom styling\n\n\n\n\n\n\nOct 7, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\nGrant Writing\n\n\n\n\nSome code and examples showing how to generate a conflict of interest statement required by some funding agencies in an almost completely automated manner.\n\n\n\n\n\n\nMay 29, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nSome examples using bibliometrix\n\n\n\n\nR\n\n\nData Analysis\n\n\nbibliometrics\n\n\n\n\nCode and examples showing how to analyze meta-data for a set of publications using the bibilometrix R package.\n\n\n\n\n\n\nFeb 2, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nSome examples using Google Scholar\n\n\n\n\nR\n\n\nData Analysis\n\n\nbibliometrics\n\n\n\n\nSome code and examples showing how to process and analyze meta-data for a set of publications using the scholar R package.\n\n\n\n\n\n\nFeb 1, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 2 - adding GitHub to the workflow\n\n\n\n\nblogdown\n\n\nHugo\n\n\nGitHub\n\n\nTutorial\n\n\nWebsite\n\n\n\n\nThis is part 2/2 of the website development posts, where we’ll move the website to GitHub.\n\n\n\n\n\n\nJan 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nPart 1 - blogdown, Hugo and Netlify\n\n\n\n\nblogdown\n\n\nHugo\n\n\nGitHub\n\n\nTutorial\n\n\nWebsite\n\n\n\n\nThe following are step-by-step instructions for creating your own website using blogdown, Hugo and Netlify.\n\n\n\n\n\n\nJan 19, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nResearch\n\n\n\n\nI posted my lists with materials related to research and teaching as a website.\n\n\n\n\n\n\nJan 8, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nData Analysis\n\n\nTeaching\n\n\n\n\nSome simple data analysis applied to a mid-semester student survey of my Modern Applied Data Analysis course.\n\n\n\n\n\n\nOct 23, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\nA TidyTuesday exercise\n\n\n\n\nR\n\n\nData Analysis\n\n\nTidy Tuesday\n\n\n\n\nAn analysis of TidyTuesday data for pizza restaurants and their ratings.\n\n\n\n\n\n\nOct 12, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "href": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "title": "Analysis of pizza restaurants",
    "section": "",
    "text": "When I taught the course in fall 2019, one of the weekly assignments for the students was to participate in TidyTuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with TidyTuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week’s data was all about Pizza. More on the data is here.\n\n\nLoading packages\n\nlibrary('readr')\nlibrary('ggplot2')\nlibrary(\"dplyr\")\nlibrary(\"cowplot\")\nlibrary(\"plotly\")\nlibrary(\"forcats\")\nlibrary(\"geosphere\")\nlibrary(\"emoji\")\n\n\n\nData loading\nLoad date following TidyTueday instructions.\n\npizza_jared <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv\")\npizza_barstool <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv\")\npizza_datafiniti <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv\")\n\n\n\nAnalysis Ideas\nSee the TidyTuesday website for a codebook. These are 3 datasets. Looks like the 1st dataset is ratings of pizza places through some (online?) survey/poll, the 2nd dataset again has ratings of pizza places from various sources, and the 3rd dataset seems to have fairly overlapping information to the 2nd dataset.\nNote: When I looked at the website, the codebook for the 3rd dataset seemed mislabeled. Might be fixed by now.\nPossibly interesting questions I can think of:\n\nFor a given pizza restaurant, how do the different ratings/scores agree or differ?\nAre more expensive restaurants overall rated higher?\nIs there some systematic dependence of rating on location? Do restaurants located in a certain area in general get rated higher/lower compared to others?\n\nI think those are good enough questions to figure out, let’s see how far we get.\n\n\nInitial data exploration\nStart with a quick renaming and general check.\n\n#saves typing\nd1 <- pizza_jared \nd2 <- pizza_barstool \nd3 <- pizza_datafiniti \nglimpse(d1)\n\nRows: 375\nColumns: 9\n$ polla_qid   <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5…\n$ answer      <chr> \"Excellent\", \"Good\", \"Average\", \"Poor\", \"Never Again\", \"Ex…\n$ votes       <dbl> 0, 6, 4, 1, 2, 1, 1, 3, 1, 1, 4, 2, 1, 1, 0, 1, 1, 0, 3, 0…\n$ pollq_id    <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5…\n$ question    <chr> \"How was Pizza Mercato?\", \"How was Pizza Mercato?\", \"How w…\n$ place       <chr> \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza …\n$ time        <dbl> 1344361527, 1344361527, 1344361527, 1344361527, 1344361527…\n$ total_votes <dbl> 13, 13, 13, 13, 13, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 5, 5, 5,…\n$ percent     <dbl> 0.0000, 0.4615, 0.3077, 0.0769, 0.1538, 0.1429, 0.1429, 0.…\n\nglimpse(d2)\n\nRows: 463\nColumns: 22\n$ name                                 <chr> \"Pugsley's Pizza\", \"Williamsburg …\n$ address1                             <chr> \"590 E 191st St\", \"265 Union Ave\"…\n$ city                                 <chr> \"Bronx\", \"Brooklyn\", \"New York\", …\n$ zip                                  <dbl> 10458, 11211, 10017, 10036, 10003…\n$ country                              <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US…\n$ latitude                             <dbl> 40.85877, 40.70808, 40.75370, 40.…\n$ longitude                            <dbl> -73.88484, -73.95090, -73.97411, …\n$ price_level                          <dbl> 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, …\n$ provider_rating                      <dbl> 4.5, 3.0, 4.0, 4.0, 3.0, 3.5, 3.0…\n$ provider_review_count                <dbl> 121, 281, 118, 1055, 143, 28, 95,…\n$ review_stats_all_average_score       <dbl> 8.011111, 7.774074, 5.666667, 5.6…\n$ review_stats_all_count               <dbl> 27, 27, 9, 2, 1, 4, 5, 17, 14, 6,…\n$ review_stats_all_total_score         <dbl> 216.3, 209.9, 51.0, 11.2, 7.1, 16…\n$ review_stats_community_average_score <dbl> 7.992000, 7.742308, 5.762500, 0.0…\n$ review_stats_community_count         <dbl> 25, 26, 8, 0, 0, 3, 4, 16, 13, 4,…\n$ review_stats_community_total_score   <dbl> 199.8, 201.3, 46.1, 0.0, 0.0, 13.…\n$ review_stats_critic_average_score    <dbl> 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0…\n$ review_stats_critic_count            <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ review_stats_critic_total_score      <dbl> 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0…\n$ review_stats_dave_average_score      <dbl> 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1…\n$ review_stats_dave_count              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ review_stats_dave_total_score        <dbl> 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1…\n\nglimpse(d3)\n\nRows: 10,000\nColumns: 10\n$ name            <chr> \"Shotgun Dans Pizza\", \"Sauce Pizza Wine\", \"Mios Pizzer…\n$ address         <chr> \"4203 E Kiehl Ave\", \"25 E Camelback Rd\", \"3703 Paxton …\n$ city            <chr> \"Sherwood\", \"Phoenix\", \"Cincinnati\", \"Madison Heights\"…\n$ country         <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", …\n$ province        <chr> \"AR\", \"AZ\", \"OH\", \"MI\", \"MD\", \"MD\", \"CA\", \"CA\", \"FL\", …\n$ latitude        <dbl> 34.83230, 33.50927, 39.14488, 42.51667, 39.28663, 39.2…\n$ longitude       <dbl> -92.18380, -112.07304, -84.43269, -83.10663, -76.56698…\n$ categories      <chr> \"Pizza,Restaurant,American restaurants,Pizza Place,Res…\n$ price_range_min <dbl> 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 25, 25, 25, 25, 0, 0, 0…\n$ price_range_max <dbl> 25, 25, 25, 40, 25, 25, 25, 25, 25, 25, 40, 40, 40, 40…\n\n\nThe first question I have is if the pizza places in the 3 datasets are the same or at least if there is decent overlap. If not, then one can’t combine the data.\n\nd1names = unique(d1$place)\nd2names = unique(d2$name)\nd3names = unique(d3$name)\nsum(d1names %in% d2names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 22\n\nsum(d1names %in% d3names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 9\n\nsum(d2names %in% d3names)\n\n[1] 66\n\n\n22 restaurants out of 56 in dataset 1 are also in dataset 2. Only 9 overlap between dataset 1 and 3. 66 are shared between datasets 2 and 3.\nThe last dataset has no ratings, and if I look at the overlap of dataset 1 and 2, I only get a few observations. So I think for now I’ll focus on dataset 2 and see if I can address the 3 questions I posed above with just that dataset. Maybe I’ll have ideas for the other 2 datasets as I go along (would be a shame to not use them.)\n\n\nRatings agreement analysis\nOk, I’ll focus on dataset 2 now and look closer at the scores/rating. From the codebook, it’s not quite clear to me what the different scores and counts in dataset 2 actually mean, so let’s look closer to try and figure that out.\nFrom the glimpse function above, I can’t see much of a difference between average and total score. Let’s look at that. Here are a few plots comparing the different score-related variables.\n\nplot(d2$review_stats_community_total_score,d2$review_stats_community_average_score)\n\n\n\nplot(d2$review_stats_community_total_score - d2$review_stats_community_average_score* d2$review_stats_community_count)\n\n\n\nplot(d2$review_stats_critic_total_score-d2$review_stats_critic_average_score)\n\n\n\nplot(d2$review_stats_dave_total_score-d2$review_stats_dave_average_score)\n\n\n\nplot(d2$review_stats_all_total_score- (d2$review_stats_community_total_score+d2$review_stats_critic_total_score+d2$review_stats_dave_total_score))  \n\n\n\n\nOk, so based on the plots above, and a few other things I tried, it seems that average score is total score divided by number of counts, and the all score is just the sum of dave, critic and community.\nSo to address my first question, I’ll look at correlations between average scores for the 3 types of reviewers, namely dave, critic and community.\nHowever, while playing around with the data in the last section, I noticed a problem. Look at the counts for say critics and the average score.\n\ntable(d2$review_stats_critic_count)\n\n\n  0   1   5 \n401  61   1 \n\ntable(d2$review_stats_critic_average_score)\n\n\n   0    4  4.3  4.5  4.8    5  5.1  5.4  5.5  5.7  5.8  5.9 5.96    6  6.2 6.31 \n 401    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 6.5  6.6  6.7 6.76  6.8  6.9    7  7.2  7.3  7.4  7.6 7.76  7.8  7.9    8  8.1 \n   3    1    1    1    2    1    5    2    2    1    1    1    2    1    4    2 \n 8.5  8.7  8.8    9  9.3  9.4  9.8   10   11 \n   3    1    1    1    1    2    1    4    1 \n\n\nA lot of restaurants did not get reviewed by critics, and the score is coded as 0. That’s a problem since if we take averages and such, it will mess up things. This should really be counted as NA. So let’s create new average scores such that any restaurant with no visits/reviews gets an NA as score.\n\nd2 <- d2 %>% mutate( comm_score = ifelse(review_stats_community_count == 0 ,NA,review_stats_community_average_score)) %>%\n             mutate( crit_score = ifelse(review_stats_critic_count == 0 ,NA,review_stats_critic_average_score)) %>%\n             mutate( dave_score = ifelse(review_stats_dave_count == 0 ,NA,review_stats_dave_average_score)) \n\nNow let’s plot the 3.\n\np1 <- d2 %>% ggplot(aes(x=comm_score, y = crit_score)) + geom_point() + geom_smooth(method = \"lm\")\np2 <- d2 %>% ggplot(aes(x=comm_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\np3 <- d2 %>% ggplot(aes(x=crit_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nLooks like there is some agreement between Dave, the critics and the community on the ratings of various pizza places, though there is a good bit of variation.\nI think it would be fun to be able to click on specific points to see for a given score which restaurant that is. For instance I’m curious which restaurant has a close to zero score from both the community and Dave (bottom left of plot B).\nI think that can be done with plotly, let’s google it.\nOk, figured it out. This re-creates the 3 scatterplots from above and when one moves over the dots, it shows restaurant name.\n\nplot_ly(d2, x = ~comm_score, y = ~crit_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\nplot_ly(d2, x = ~comm_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\nplot_ly(d2, x = ~crit_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\nSo apparently the lousy restaurant that got a 1 from the community and almost 0 from Dave is called Amtrak. I’m wondering if that refers to pizza on Amtrak trains? Just for the heck of it and because I’m curious, let’s look at that entry.\n\nd2 %>% filter(name == \"Amtrak\") %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\naddress1\ncity\nzip\ncountry\nlatitude\nlongitude\nprice_level\nprovider_rating\nprovider_review_count\nreview_stats_all_average_score\nreview_stats_all_count\nreview_stats_all_total_score\nreview_stats_community_average_score\nreview_stats_community_count\nreview_stats_community_total_score\nreview_stats_critic_average_score\nreview_stats_critic_count\nreview_stats_critic_total_score\nreview_stats_dave_average_score\nreview_stats_dave_count\nreview_stats_dave_total_score\ncomm_score\ncrit_score\ndave_score\n\n\n\n\nAmtrak\n234 W 31st St\nNew York\n10001\nUS\n40.74965\n-73.9934\n0\n3\n345\n0.54\n2\n1.08\n1\n1\n1\n0\n0\n0\n0.08\n1\n0.08\n1\nNA\n0.08\n\n\n\n\n\nI googled the address, and it seems to be indeed Amtrak. Note to self: Never order pizza on an Amtrak train.\n\n\nPrice vs ratings analysis\nNext, let’s look at possible impact of restaurant price level on rating.\n\ntable(d2$price_level)\n\n\n  0   1   2   3 \n 21 216 218   8 \n\n\nThere isn’t much spread, most pizza places are in the middle. Maybe not too surprising. Let’s look at a few plots to see if there is a pattern. First, we should recode price level as a factor.\n\nd2 <- d2 %>% mutate(price = as.factor(price_level))\n\n\np1 <- d2 %>% ggplot(aes(x=price, y=comm_score)) + geom_violin() + geom_point()\np2 <- d2 %>% ggplot(aes(x=price, y=crit_score)) + geom_violin() + geom_point()\np3 <- d2 %>% ggplot(aes(x=price, y=dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\nHard to tell if there’s a trend. Could do some stats to look in more detail, but since this exercise focuses on exploring, I won’t do that. Instead I’ll leave it at that.\n\n\nRating versus location\nOk, on to the last of the questions I started out with. Maybe there are some areas where restaurants are in general better? Or maybe an area where diners are more critical? Let’s see if there is some correlation between ratings and location.\n\ntable(d2$country)\n\n\n US \n463 \n\nsort(table(d2$city))\n\n\n        Alpharetta            Augusta             Austin         Austintown \n                 1                  1                  1                  1 \n        Blacksburg          Braintree           Brockton            Buffalo \n                 1                  1                  1                  1 \n        Charleston          Charlotte      Chestnut Hill           Chilmark \n                 1                  1                  1                  1 \n        Clearwater            Clifton         Coralville      Daytona Beach \n                 1                  1                  1                  1 \n          Dearborn        Dennis Port              DUMBO        East Meadow \n                 1                  1                  1                  1 \n             Edina          Elizabeth             Elmont         Gansevoort \n                 1                  1                  1                  1 \n              Gary       Hampton Bays          Hopkinton       Howard Beach \n                 1                  1                  1                  1 \n        Huntington          Iowa City            Jackson Jacksonville Beach \n                 1                  1                  1                  1 \n       Jersey City        Kew Gardens          Lakeville      Lawrenceville \n                 1                  1                  1                  1 \n              Lynn    Manhattan Beach       Mashantucket              Miami \n                 1                  1                  1                  1 \n    Middle Village       Mount Vernon      New Hyde Park      New York City \n                 1                  1                  1                  1 \n   North Arlington             Nutley         Oak Bluffs           Oak Lawn \n                 1                  1                  1                  1 \n     Oklahoma City             Orange         Palm Beach           Pembroke \n                 1                  1                  1                  1 \n         Princeton             Ramsey           Randolph             Revere \n                 1                  1                  1                  1 \n        Rutherford      San Francisco           Sandwich        Southampton \n                 1                  1                  1                  1 \n         Stoughton              Tampa     Vineyard Haven     West Melbourne \n                 1                  1                  1                  1 \n   West Palm Beach       West Roxbury             Woburn            Yonkers \n                 1                  1                  1                  1 \n   East Rutherford          Edgartown       Elmwood Park            Hyannis \n                 2                  2                  2                  2 \n       Miami Beach       Philadelphia         Saint Paul       Santa Monica \n                 2                  2                  2                  2 \n          Stamford              Bronx       Indianapolis          Lexington \n                 2                  3                  3                  3 \n        Morgantown        San Antonio          San Diego          Ann Arbor \n                 3                  3                  3                  4 \n        Louisville          New Haven      Staten Island         Youngstown \n                 4                  4                  4                  4 \n           Atlanta            Chicago           Columbus            Hoboken \n                 6                  6                  6                  6 \n         Nantucket   Saratoga Springs        Minneapolis          Las Vegas \n                 6                  6                  8                 11 \n            Boston           Brooklyn           New York \n                13                 20                251 \n\n\nOk so all restaurants are in the US, and most are in New York. We could look at NY versus “rest of the cities”. Though isn’t Brooklyn (the 2nd largest entry) basically a part of New York? I’m not enough of an expert on all things NY to be sure (for any real analysis, you need to know a good bit about the subject matter, or work closely with a subject matter expert. If not, more likely than not something dumb will happen).\nFor now, I assume that it’s different enough, and make 2 categories, NY and “other” and see if there are differences. Let’s try.\n\np1 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = comm_score)) + geom_violin() + geom_point()\np2 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = crit_score)) + geom_violin() + geom_point()\np3 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\nLooks like the community in NY gives lower scores compared to other locations, less noticeable difference for critics and Dave.\nOk, the next analysis might not make much sense, but why not check if there is a North-South or East-West trend related to ratings. Maybe restaurants are better in one of those directions? Or people in the South are more polite and give better scores? 😁. I’m mostly doing this because longitude and latitude are continuous variables, so I can make a few more scatterplots. I don’t have any real goal for this otherwise.\n\np1 <- d2 %>%  ggplot(aes(x=longitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=longitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=longitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSo as we go from the west (-120) to the east (-70), there is a trend in restaurants getting higher scores, by all 3 groups. I guess as we are moving closer to Italy, the pizza quality goes up? 😃.\nNext, let’s look at latitude.\n\np1 <- d2 %>%  ggplot(aes(x=latitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=latitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=latitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSo doesn’t seem as much of a trend going from South (25) to North (45). That finding of course fully confirms our “closer to Italy” theory!\nOk, I was going to leave it at that with location, but since I’m already going down a crazy rabbit hole regarding Italy, let’s do it for real: We’ll take both longitude and latitude of each restaurant and use it compute the distance of each location to Naples, the home of Pizza. And then we’ll plot that and see.\nSince I have no idea how to do that, I need Google. Fortunately, the first hit worked, found this one: https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r\nLet’s try.\n\ncoord_naples=cbind(rep(14.2,nrow(d2)),rep(40.8,nrow(d2)))  #location of naples\ncoord_restaurants = cbind(d2$longitude,d2$latitude)\ndistvec = rep(0,nrow(d2))\nfor (n in 1:nrow(d2))\n{\n  distvec[n] = distm( coord_restaurants[n,], coord_naples[n,], fun = distGeo)\n}\nd2$distvec = distvec / 1609 #convert to miles since we are in the US :)\n\nIt’s not tidyverse style, which I tried first but couldn’t get it to work. The trusty old for-loop seems to always work for me. I checked the numbers in distvec, they look reasonable.\nOk, let’s redo the plots above, now with distance to Naples.\n\np1 <- d2 %>%  ggplot(aes(x=distvec, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=distvec, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=distvec, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12, nrow = 3)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHm ok, no smoking gun. Looks like there is a bit of a trend that the further away you are from Naples, the lower the score. But really not much.\n\n\nHyping our result\nBut since this distance-from-Naples makes such a good story, let’s see if I can hype it.\nFirst, to increase potential statistical strength, I’ll combine all 3 scores into an overall mean, i.e. similar ot the all variable in the original. I don’t trust that one since I don’t know if they averaged over 0 instead of properly treating it as NA. Of course I could check, but I’m just re-creating it here.\n\nd2$all_score = rowMeans(cbind(d2$dave_score,d2$crit_score,d2$comm_score),na.rm=TRUE)\n\nOk, let’s check if correlation between this new score and distance is significant!\n\n#compute a linear fit and p-value (it's significant!)\nfit=lm(d2$all_score ~ d2$distvec, data = d2)\nsummary(fit)\n\n\nCall:\nlm(formula = d2$all_score ~ d2$distvec, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7854 -0.5866  0.3027  0.9612  2.3686 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.9895014  0.7008802  12.826  < 2e-16 ***\nd2$distvec  -0.0004772  0.0001525  -3.129  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.478 on 459 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.02089,   Adjusted R-squared:  0.01875 \nF-statistic: 9.791 on 1 and 459 DF,  p-value: 0.001865\n\npval=anova(fit)$`Pr(>F)`[1]\nprint(pval)\n\n[1] 0.001865357\n\n\nIt is signficant, p<0.05! We hit pay dirt! Let’s make a great looking figure and go tell the press!\n\n#make final plot\np1 <- d2 %>%  ggplot(aes(x=distvec, y = all_score)) + geom_point(shape = 21, colour = \"black\", fill = \"red\",  size = 2 ) + geom_smooth(method = 'lm', se = TRUE, color = \"darkgreen\", size = 2) + xlab('Distance from Naples (miles)') + ylab('Pizza Quality (score)') + ylim(c(2.5,max(d2$all_score))) + theme_bw() +theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=\"bold\")) + annotate(\"text\", x=6000, y=9, label= paste(\"p =\",round(pval,4)),size = 12) \nggsave('pizzadistance.png')\nknitr::include_graphics(\"pizzadistance.png\")\n\n\n\n\n\n\nThe “press release”\nA novel study of pizza restaurants in the US found a clear, statistically significant correlation between the distance of the restaurant to Naples and the quality of the pizza as determined by the community and expert restaurant critics. The study authors attribute the finding to the ability of restaurants that are closer to Naples to more easily get genuine fresh and high quality ingredients, such as the famous San Marzano tomatoes.\n\n\n\n\n\n\n\n\n\n\n\nSummary\nThat was a fun exploration. It was the first time I played with the tidyverse data. I had no idea which direction it was going to go, and ideas just came as I was doing it. I’m sure there is interesting stuff in datasets 1 and 3 as well, but I already spent several hours on this and will therefore call it quits now.\nWhile the exercise was supposed to focus on cleaning/wrangling and visualizing, I couldn’t resist going all the way at the end and producing a statistically significant and somewhat plausible sounding finding. If this were a “real” study/analysis, such a nice result would be happily accepted by most analysts/authors, hyped by a university press release and - if the result is somewhat interesting/cute, picked up by various media outlets.\nI had no idea at the beginning what I was going to analyze, I did that longitude/latitude analysis on a whim, and if I hadn’t found this correlation and had that crazy distance to Italy idea, nothing would have happened. But now that I have a significant result and a good story to go with, I can publish! It’s not really much sillier than for instance the Chocolate and Nobel Laureates paper paper.\nWhat I illustrated here (without having had any plan to do so), is a big, general problem in secondary data analysis. It’s perfectly ok to do secondary analyses, and computing significance is also (kinda) ok, but selling exploratory (fishing) results as inferential/causal/confirmatory is wrong - and incredibly widespread. If you want to sharpen your critical thinking skills related to all those supposed significant and real findings in science we see a lot, a great (though at times sobering) read is Andrew Gelman’s blog where he regularly picks apart studies/results like the one I did here or the chocolate and Nobel laureates one. And now I’ll go eat some chocolate so I can increase my chances for a Nobel prize.\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Analysis of Pizza Restaurants},\n  date = {2019-10-12},\n  url = {https://www.andreashandel.com/posts/2019-10-02-tidy-tuesday-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Analysis of Pizza Restaurants.”\nOctober 12, 2019. https://www.andreashandel.com/posts/2019-10-02-tidy-tuesday-exploration."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html",
    "href": "posts/2020-01-02-blogdown-website-1/index.html",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "",
    "text": "Update: Hugo and the Academic (now Wowchemy) kept changing rapidly and became, in my opinion, rather complex and confusing. Once Quarto arrived, I switched this website over (see my post on that here), and I think it’s much easier to use. I therefore don’t really recommend the setup described here anymore, though it might still be useful for some.\nThe following are step-by-step instructions for creating your own website using blogdown, Hugo and Netlify.\nIn part 2, you will learn how to add GitHub to your workflow to make things even more automated and efficient."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Get a Netlify account",
    "text": "Get a Netlify account\nGo to the Netlify website and sign up for an account. Follow the sign-up steps to set up your account."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older version of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Playing with widgets",
    "text": "Playing with widgets\nAll (or at least most) content goes into the content folder and its subfolders. Content is generally written in (R)Markdown. For this tutorial, you don’t need to know much Rmarkdown, but at some point you will have to learn it. Fortunately, (R)Markdown is very easy to learn. See e.g. the RMarkdown section on this page, check out this nice interactive tutorial or this cheatsheet.\nThe Wowchemy/Academic theme, and many other modern websites, use a layout that employs widgets, which are components of a website that are individually formatted and styled. On the demo site you just created, you see many different sections, each is a widget. Which widgets you want is controlled by files in the /content/home/ folder. Go into that folder (from within RStudio) and open the demo.md file. You will see a bunch of text. Some commands are between +++ signs, this is called the TOML (or YAML if it’s 3 ---) header. These are instructions for the layout. The text below is what is actually shown on the site.\nAs you stare at the content of the file, you might recognize that it corresponds to the 2nd block of the demo website with the dark bubble content. Let’s say you don’t want this particular widget on your home page. The easiest way is to set active = false. (You can also delete the whole file if you are sure you don’t want it). Do that. You should notice 2 things. In the bottom-left RStudio window (the R console) you should see a bit of code showing that the website was re-built and updated in real time. If you re-load the page in your browser, the widget and its content should be gone. You can try turning off other sections of the main page using this approach."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Making things personal",
    "text": "Making things personal\nNow let’s open the about.md file. You will notice that it doesn’t really contain any content. Instead, it pulls the content from another location, namely content in the authors folder. Go into /content/authors/admin/ and open the _index.md file. There you see the content that is displayed on the main page. Modify it with your personal information. Once you save your changes, you should see the website automatically being rebuilt. If you have, add a picture of yourself and replace the current avatar.jpg file. (Your picture needs to have that name). Also, while not required, you might want to rename the folder from admin to your name. Make sure this corresponds to the name you list in the _index.md file."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Cleaning up for now",
    "text": "Cleaning up for now\nLet’s turn off all other parts of the main site apart from the about widget. The easiest way is to remove all files apart from the index.md and about.md files. You probably don’t want to completely delete them (since you might want to use them later), thus I recommend you move them to some other folder on your computer. For instance you can make a folder called myfiles as a subfolder of your website folder and move the files into that folder.\nIf all of this worked, there should be a main page containing only a brief description of yourself."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "A word on error messages.",
    "text": "A word on error messages.\nIf you make some changes that break the site, you will see an error message in the R console and the site won’t re-compile until you fix the problem. You often have to be careful to write things exactly as specified, and often with the right indentation, etc. Some fiddling is at times required. If you are stuck and think you broke it too badly, you can either look in the Wowchemy theme documentation or go into the themes/starter-academic/exampleSite folder and find the corresponding file you are editing there and see how it needs to look."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "config.yaml",
    "text": "config.yaml\nI mentioned above that TOML/YAML is a language/structure used by Hugo to control all kinds of things. Most files have some TOML/YAML part, a few files are nothing but TOML and control a lot of settings. Let’s look at the most important files. The first one is config.yaml (sometimes also called config.toml) located in the main website folder. Find and open it. You will see that it lists as title Academic. Change that to e.g. Website of YOURNAME. You will see this change show up on the main site. You can try what happens if you write something in the copyright section. The rest of this file doesn’t need further editing for now."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "menus.toml",
    "text": "menus.toml\nLet’s go into the config/_default/ folder and open the menus.toml file. You’ll see that those correspond to the menu buttons on the main page. Most of them don’t work since we removed the widgets. For now, let’s go ahead and disable (by placing # symbols in front) all entries apart from the Posts block of text."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "params.toml",
    "text": "params.toml\nOpen params.toml. This file lets you specify and control a lot of things. Try setting a different theme. Then read through the other parts. We won’t change them for now, but you might want to come back to them."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "More edits",
    "text": "More edits\nCongratulations, you have built a website and wrote a blog post! Of course there is a lot you can do next. Write more posts, look at all the different elements/widgets you can turn on and modify, etc. As mentioned, the Academic theme has a lot of features. If you like what you see, continue exploring. If you think you want something simpler, check out other Hugo themes until you find one you like, then customize it. A lot of things are very similar across all Hugo themes (e.g. the TOML/YAML bits and the folder structure), but some details differ, so it’s good to pick a theme before you really start customizing it.\nBut for now, we’ll leave it at this. There is one more crucial step missing though."
  },
  {
    "objectID": "posts/2020-01-04-blogdown-website-2/index.html",
    "href": "posts/2020-01-04-blogdown-website-2/index.html",
    "title": "Creating a website in less than 1 hour - part 2",
    "section": "",
    "text": "If you haven’t seen part 1 yet, I suggest you read through that first. In that part, I covered how to create a website using blogdown, Hugo, and Netlify.\n\nRequired skills\nI assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nI assume your website is at a stage as described at the end of part 1.\n\n\nQuick tool overview\nThe only new tool for this part is GitHub. GitHub is a very powerful and common way of developing projects like code, research projects or a website. Github is a great tool to be familiar with in general, and most importantly, it very nicely integrates with the other tools to make things seamless and automated. As you learned in part 1, it is not strictly needed, but it makes updating automatic and is such an overall useful tool to be exposed to that I’m including it in the setup.\n\n\nGet a GitHub account\nIf you do not already have a GitHub account, create one. Note that GitHub is widely used professionally, and you might want to allow other people to see your GitHub presence. I, therefore, recommend using a future-proof, professional user name.\nNote that Git and GitHub (which are technically different, here I’m using GitHub to refer to both) can be initially confusing, mainly because they use a lot of specialized terminology. I will try and walk you through all steps for getting a website up and running in detail, but you might have to look up a few things on the way. If you are completely new to GitHub, I recommend you take a quick look at this page (and links provided there) so you can get some idea of what it’s all about.\n\n\nGet Gitkraken\nDownload and install Gitkraken, link it with your GitHub account. Gitkraken is a graphical Git/GitHub client that makes a lot of tasks related to GitHub easier. It’s not strictly needed, and if you already have your own way of using Git/GitHub (e.g. with another client or the command line) you can stick with that. There is also an option to use Git/GitHub through RStudio, which is fine for most things, but overall Gitkraken is more powerful. So if you plan to use GitHub more in the future, I recommend using it. In the following, I assume you are using Gitkraken. If you interface with GitHub some other way, you will have to adjust those specific instructions accordingly.\n\n\nRecommended, but optional: Upgrade GitHub (& Gitkraken)\nOn GitHub, by default, all repositories are public (a repository is the collection of files and folders that make up a specific GitHub project, such as your website.) If you have public repositories, you need to be careful about files that shouldn’t be shared publicly (e.g. because of copyright restrictions or because it might violate data privacy). Normally, if you want private repositories, you have to pay. As student or educator, you can get private repositories for free.\nIf you are a student, I strongly recommend you get the GitHub student developer pack. This gives you access to private repositories. You also get 1 year of free Gitkraken Pro access. While the free version of Gitkraken works well, you can’t access private repositories with it. Often, being able to use private repositories is useful.\nEducators can also get a free GitHub Pro account here. As far as I’m aware, there is no free Gitkraken Pro for educators, but it’s fairly cheap. So if you want to use a private repository for your website (I don’t know why you would), you need to pay for Gitkraken Pro or use a different way to manage your GitHub repositories.\n\n\nCreate a GitHub repository\n\nLog into GitHub, click on ‘Repositories’ and create a new repository (green button). Choose a repository (repo) name that tells you what’s in the repo (e.g. YOURNAME-website). You can give it the same name as you named your main website directory/project in part 1, but that’s not required. Check the box Initialize this repository with a README. Set the .gitignore option to R, you can leave the license at none. Click create repository.\nClone the repository from GitHub to your local computer (using Gitkraken or whatever software/method you decided to use). Place the local folder in a location on your computer where it is not synced with some other software (e.g., Dropbox, OneDrive).\n\n\n\nMove your website folder to the GitHub repo\n\nFind the main folder of your website and move all of it into your newly created GitHub repository. To make sure everything is up-to-date, close RStudio before doing so.\nOpen the repository you just created in Gitkraken. In the top right of Gitkraken, there should be a notification about changed files. Click View changes, then Stage. Write a commit message, commit the changes. You should see the master with the computer symbol above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line.\nIf your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e. someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this project, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of this action.\nGo back to GitHub.com and to your repository. You should see all your files in there.\n\n\n\nConnecting GitHub to Netlify\n\nThe last step is to set up Netlify so it can automatically monitor your GitHub repository and process any changes into an updated website. To do so, log into your Netlify account. Select your website. Under Site Settings find the Build and deploy menu. Under Continuous deployment click on Link site to Git. Choose GitHub and the follow the steps to link your webpage repository. Once finished, Netlify will monitor that repository and automatically pull any updates and rebuild your website.\n\n\n\nThe new workflow\nThe workflow for your website using GitHub now works as follows:\n\nOpen your website in Rstudio by clicking on the .Rproj file. Load blogdown with library(blogdown). Make any edits you want. While you make your edits, you can run serve_site() to see your updates.\n\nOnce you are done with your updates, restart R (to shut down serve_site()) and run build_site(). While Netlify will automatically run Hugo on your files, it won’t do any processing of Rmarkdown files, therefore you need to run the blogdown build command.\nUse Gitkraken to push/pull and therefore sync changes between your local computer (or multiple computers if you work on more than one machine) and Github.com.\n\nNetlify will monitor your Github repository and when it sees changes, automatically rebuild your website. Note that this means that if you start working on a document and don’t finish it, and then push to GitHub, your half-finished document shows up. To avoid that, you can set draft: true in your TOML/YAML heading for the document in progress.\n\n\nMore Information\nFor the non-GitHub aspects of this, see the resources mentioned at the end of part 1. For some more GitHub related information, see e.g. here.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Creating a Website in Less Than 1 Hour - Part 2},\n  date = {2020-01-20},\n  url = {https://www.andreashandel.com/posts/2020-01-04-blogdown-website-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Creating a Website in Less Than 1 Hour -\nPart 2.” January 20, 2020. https://www.andreashandel.com/posts/2020-01-04-blogdown-website-2."
  },
  {
    "objectID": "posts/2020-01-08-resources-website/index.html",
    "href": "posts/2020-01-08-resources-website/index.html",
    "title": "Research and teaching resources website",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Research and Teaching Resources Website},\n  date = {2020-01-08},\n  url = {https://www.andreashandel.com/posts/2020-01-08-resources-website},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Research and Teaching Resources\nWebsite.” January 8, 2020. https://www.andreashandel.com/posts/2020-01-08-resources-website."
  },
  {
    "objectID": "posts/2020-02-01-publications-analysis-1/index.html",
    "href": "posts/2020-02-01-publications-analysis-1/index.html",
    "title": "Using R to analyze publications - part 1",
    "section": "",
    "text": "Notes\n\nAs of this writing, the scholar R package seems semi-dormant and not under active development. If Google changes their API for Scholar and the package isn’t updated, the below code might stop working.\nA problem I keep encountering with Google Scholar is that it starts blocking requests, even after what I consider are not that many attempts to retrieve data. I notice that when I try to pull references from Google Scholar using JabRef and also with the code below. If that happens to you, try a different computer, or clear cookies. This is a well known problem and if you search online, you find others complaining about it. I haven’t found a great solution yet, other than not using the Google Scholar data. I describe such an approach in part 2 of this post. However, some analyses are only able with Google Scholar information.\nTo minimize chances of getting locked out by Google, I wrote the code below such that it only sends a request if there isn’t a local file already containing that data. To refresh data, delete the local files.\n\n\n\nRequired packages\n\nlibrary(scholar)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\n\n\n\nGet all citations for an individual\nFirst, I’m using Google Scholar to get all citations for a specific author (in this case, myself).\n\n#Define the person to analyze\nscholar_id=\"bruHK0YAAAAJ\" \n#either load existing file of publications\n#or get a new one from Google Scholar\n#delete the file to force an update\nif (file.exists('citations.Rds'))\n{\n  cites <- readRDS('citations.Rds')\n} else {\n  #get citations\n  cites <- scholar::get_citation_history(scholar_id) \n  saveRDS(cites,'citations.Rds')\n}\n\n\n\nCompare citations for different time periods\nFor my purpose, I want to compare citations between 2 time periods (my Assistant Professor time and my Associate Professor time). I’m splitting them into 2. I’m doing this analysis at the beginning of 2020 and want only full years. The code snippets below give me what I need, two time periods 2009-2014 and 2014-2019.\n\nperiod_1_start = 2009\nperiod_2_start = 2015\ncites_1 <- cites %>% dplyr::filter((year>=period_1_start & year<period_2_start ))\n#remove last year since it's not a full year\ncites_2 <- cites %>% dplyr::filter((year>=period_2_start & year<2020 )) \n\nFitting a linear model to both time segments to look at increase in citations over both periods.\n\nfit1=lm(cites ~ year, data = cites_1)\nfit2=lm(cites ~ year, data = cites_2)\ninc1 = fit1$coefficients[\"year\"]\ninc2 = fit2$coefficients[\"year\"] \nprint(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))\n\n[1] \"Annual increase for periods 1 and 2 are 22.257143, 43.100000\"\n\n\nMaking a figure to show citation count increases\n\n# combine data above into single data frame\n#add a variable to indicate period 1 and period 2\ncites_1$group = \"1\"\ncites_2$group = \"2\"\ncites_df = rbind(cites_1,cites_2)\nxlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]\n#make the plot and show linear fit lines\np1 <- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) + \n      geom_point(size = I(4)) + \n      geom_smooth(method=\"lm\",aes(group = group), se = F, size=1.5) + \n      scale_x_continuous(name = \"Year\", breaks = xlabel, labels = xlabel) +     scale_y_continuous(\"Citations according to Google Scholar\") +\n      theme_bw(base_size=14) + theme(legend.position=\"none\") + \n      geom_text(aes(NULL,NULL),x=2010.8,y=150,label=\"Average annual \\n increase 22%\",color=\"black\",size=5.5) +\n      geom_text(aes(NULL,NULL),x=2017,y=150,label=\"Average annual \\n increase 43%\",color=\"black\",size=5.5) \n\n#open a new graphics window\n#note that this is Windows specific. Use quartz() for MacOS\nww=5; wh=5; \nwindows(width=ww, height=wh)                    \nprint(p1)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\ndev.print(device=png,width=ww,height=wh,units=\"in\",res=600,file=\"citations.png\")\n\npng \n  2 \n\n\n\n\nGetting list of publications\nAbove I got citations, but not publications. This function retrieves all publications for a specific author and returns it as a data frame.\n\n#get all pubs for an author (or multiple)\nif (file.exists('publications.Rds'))\n{\n  publications <- readRDS('publications.Rds')\n} else {\n  #get citations\n  publications <- scholar::get_publications(scholar_id) \n  saveRDS(publications,'publications.Rds')\n}\n\n\n\nQuick peek at publications\n\nglimpse(publications)\n\nRows: 90\nColumns: 8\n$ title   <fct> \"Severe outcomes are associated with genogroup 2 genotype 4 no…\n$ author  <fct> \"R Desai, CD Hembree, A Handel, JE Matthews, BW Dickey, S McDo…\n$ journal <fct> \"Clinical infectious diseases\", \"BMC public health\", \"Journal …\n$ number  <fct> \"55 (2), 189-193\", \"11 (S1), S7\", \"7 (42), 35-47\", \"3 (12)\", \"…\n$ cites   <dbl> 163, 158, 129, 124, 123, 115, 105, 89, 71, 71, 55, 53, 52, 49,…\n$ year    <dbl> 2012, 2011, 2010, 2007, 2006, 2012, 2006, 2017, 2016, 2008, 20…\n$ cid     <fct> 1979732925283755485, 10982184786304722425, 1038596204985444772…\n$ pubid   <fct> 5nxA0vEk-isC, _FxGoFyzp5QC, 9yKSN-GCB0IC, d1gkVwhDpl0C, u5HHmV…\n\n\nThis shows the variables obtained in the data frame. One thing I notice is that this contains more entries than I have peer-reviewed publications. Since most people’s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis. That is not ideal. I’ll do/show a few more possible analyses, but decided to do the analyses below using the approach in part 2.\n\n\nMaking a table of journals and impact factors\nThis used to work, but as of 2022-09-10 when I tried to re-run, it failed. Seems like get_impactfactor() doesn’t exist anymore. Maybe they got in trouble with the owners of ImpactFactor? Leaving it here, but code chunk below doesn’t run.\nThe scholar package has a function that allows one to get impact factors for journals. This data doesn’t actually come from Google Scholar, instead the package comes with an internal spreadsheet/table with impact factors. Looking a bit into the scholar package indicates that the data was taken from some spreadsheet posted on ResearchGate (probably not fully legal). Either way, let’s give it a try.\n\n#here I only want publications since 2015\npub_reduced <- publications %>% dplyr::filter(year>2014)\n# my guess is they got in trouble with the owners of ImpactFactor?\nifdata <- scholar::get_impactfactor(pub_reduced$journal) \n#Google SCholar collects all kinds of 'publications'\n#including items other than standard peer-reviewed papers\n#this sorts and removes some non-journal entries  \niftable <- ifdata %>% dplyr::arrange(desc(ImpactFactor) ) %>% tidyr::drop_na()\nknitr::kable(iftable)\n\nOK so this doesn’t quite work. I know for instance that I didn’t publish anything in Cancer Journal for Clinicians and the 2 Rheumatology entries are workshop presentations. Oddly, when I look at publications$journal there is no Cancer Journal listed. Somehow this is a bug created by the get_impactfactor() function. I could fix that by hand. The bigger problem is what to do with all those publications that are not peer-reviewed papers. I could remove them from my Google scholar profile. But I kind of want to keep them there since some of them link to useful stuff. I could alternatively manually clean things at this step. This somewhat defeats the purpose of automation.\n\n\nGetting list of co-authors\nAnother useful piece of information to have, e.g. for NSF grants, is a table with all co-authors. Unfortunately, get_publications() only pulls from the main Google Scholar page, which cuts off the author list. To get all authors, one needs to run through each paper using get_complete_authors(). The problem is that Google cuts off access if one sends too many queries. If you get error messages, it might be that Google blocked you. See the Notes section.\n\nallauthors = list()\nif (file.exists('allauthors.Rds'))\n{\n  allauthors <- readRDS('allauthors.Rds')\n} else {\n  for (n in 1:nrow(publications)) \n  {\n    allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)\n  }\n  saveRDS(allauthors,'allauthors.Rds')\n}\n\nTheoretically, if the above code runs without Google blocking things, I should end up with a list of all co-authors which I could then turn into a table. The problem is still that it pulls all entries on my Google Scholar profile, and not just peer-reviewed papers. With a bit of cleaning I could get what I need. But overall I don’t like this approach too much.\n\n\nDiscussion\nWhile the scholar package has some nice features, it has 2 major problems:\n\nGoogle blocking the script if it decides too many requests are made (that can happen quickly).\nSince most people’s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis.\n\nI do keep all my published, peer-reviewed papers in a BibTeX bibliography file in my reference manager (I’m using Zotero and/or Jabref). I know that file is clean and only contains peer-reviewed papers. Unfortunately, the scholar package can’t read in such data. In part 2 of this post series, I’ll use a different R package to produce the journal and author tables I tried making above.\nThe one feature only available through Google Scholar is the citation record and the analysis I did at the beginning if this post.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Using {R} to Analyze Publications - Part 1},\n  date = {2020-02-01},\n  url = {https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Using R to Analyze Publications - Part\n1.” February 1, 2020. https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1."
  },
  {
    "objectID": "posts/2020-02-02-publications-analysis-2/index.html",
    "href": "posts/2020-02-02-publications-analysis-2/index.html",
    "title": "Using R to analyze publications - part 2",
    "section": "",
    "text": "Required packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(bibliometrix)\n\n\n\nLoading data\nOld: I keep all references to my published papers in a BibTeX file, managed through Zotero/Jabref. I know this file is clean and correct. I’m loading it here for processing. If you don’t have such a file, make one using your favorite reference manager. Or create it through a saved search on a bibliographic database, as explained on the bibliometrix website.\nNew: In the current version of bibliometrix, reading in my bibtex file failed. A fairly good alternative is to go to your NIH “My Bibliography” (which anyone with NIH funding needs to have anyway) and export it in MEDLINE format. Then read in the file with the code below. As of the time of writing this, it requires the Github version of bibliometrix.\n\n#read bib file, turn file of references into data frame\npubs <- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix documentation.\n\n\nAnalyzing 2 time periods\nFor my purpose, I want to analyze 2 different time periods and compare them. Therefore, I split the data frame containing publications, then run the analysis on each.\n\n#get all pubs for an author (or multiple)\nperiod_1_start = 2009\nperiod_2_start = 2015\n#here I want to separately look at publications in the 2 time periods I defined above\npubs_old <- data.frame(pubs) %>% dplyr::filter((PY>=period_1_start & PY<period_2_start ))\npubs_new <- data.frame(pubs) %>% dplyr::filter(PY>=period_2_start)\nres_old <- bibliometrix::biblioAnalysis(pubs_old, sep = \";\") #perform analysis\nres_new <- bibliometrix::biblioAnalysis(pubs_new, sep = \";\") #perform analysis\n\n\n\nGeneral information\nThe summary functions provide a lot of information in a fairly readable format. I apply them here to both time periods so I can compare.\nTime period 1\n\nsummary(res_old, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2009 : 2014 \n Sources (Journals, Books, etc)        12 \n Documents                             19 \n Annual Growth Rate %                  3.71 \n Document Average Age                  10.3 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n clinical trial;journal article;research support, non-u.s. gov't                                               1 \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               1 \n journal article                                               2 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               3 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.                               1 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;review;systematic review      1 \n journal article;research support, non-u.s. gov't                                               3 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, u.s. gov't, p.h.s.                               1 \n journal article;review                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    148 \n Author's Keywords (DE)                148 \n \nAUTHORS\n Authors                               45 \n Author Appearances                    80 \n Authors of single-authored docs       0 \n \nAUTHORS COLLABORATION\n Single-authored docs                  0 \n Documents per Author                  0.422 \n Co-Authors per Doc                    4.21 \n International co-authorships %        0 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2009        5\n    2010        2\n    2011        1\n    2012        3\n    2013        2\n    2014        6\n\nAnnual Percentage Growth Rate 3.713729 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1   HANDEL A            19  HANDEL A                         5.55\n2   ANTIA R              6  ANTIA R                          1.78\n3   DOHERTY PC           3  LONGINI IM JR                    1.00\n4   LA GRUTA NL          3  DOHERTY PC                       0.56\n5   LONGINI IM JR        3  LA GRUTA NL                      0.56\n6   THOMAS PG            3  THOMAS PG                        0.56\n7   PILYUGIN SS          2  BEAUCHEMIN CA                    0.50\n8   ROHANI P             2  LI Y                             0.50\n9   STALLKNECHT D        2  ROHANI P                         0.50\n10  TURNER SJ            2  ROZEN DE                         0.50\n\n\nTop manuscripts per citations\n\n                              Paper                                   DOI TC TCperYear NTC\n1  ZHENG N, 2014, PLOS ONE                   10.1371/JOURNAL.PONE.0105721  0         0 NaN\n2  HANDEL A, 2014, PROC BIOL SCI             10.1098/RSPB.2013.3051        0         0 NaN\n3  NGUYEN TH, 2014, J IMMUNOL                10.4049/JIMMUNOL.1303147      0         0 NaN\n4  LI Y, 2014, J THEOR BIOL                  10.1016/J.JTBI.2014.01.008    0         0 NaN\n5  HANDEL A, 2014, J R SOC INTERFACE         10.1098/RSIF.2013.1083        0         0 NaN\n6  CUKALAC T, 2014, PROC NATL ACAD SCI U S A 10.1073/PNAS.1323736111       0         0 NaN\n7  HANDEL A, 2013, PLOS COMPUT BIOL          10.1371/JOURNAL.PCBI.1002989  0         0 NaN\n8  THOMAS PG, 2013, PROC NATL ACAD SCI U S A 10.1073/PNAS.1222149110       0         0 NaN\n9  JACKWOOD MW, 2012, INFECT GENET EVOL      10.1016/J.MEEGID.2012.05.003  0         0 NaN\n10 DESAI R, 2012, CLIN INFECT DIS            10.1093/CID/CIS372            0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles   Freq SCP MCP MCP_Ratio\n1 USA             14 0.7778  11   3     0.214\n2 AUSTRALIA        3 0.1667   2   1     0.333\n3 CANADA           1 0.0556   1   0     0.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    CANADA                  0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                          Sources       \n1  JOURNAL OF THE ROYAL SOCIETY INTERFACE                                                                               \n2  JOURNAL OF THEORETICAL BIOLOGY                                                                                       \n3  JOURNAL OF IMMUNOLOGY (BALTIMORE MD. : 1950)                                                                         \n4  PLOS ONE                                                                                                             \n5  PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA                                      \n6  BMC EVOLUTIONARY BIOLOGY                                                                                             \n7  BMC PUBLIC HEALTH                                                                                                    \n8  CLINICAL INFECTIOUS DISEASES : AN OFFICIAL PUBLICATION OF THE INFECTIOUS DISEASES SOCIETY OF AMERICA                 \n9  EPIDEMICS                                                                                                            \n10 INFECTION GENETICS AND EVOLUTION : JOURNAL OF MOLECULAR EPIDEMIOLOGY AND EVOLUTIONARY GENETICS IN INFECTIOUS DISEASES\n   Articles\n1         3\n2         3\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles\n1      HUMANS                      13  HUMANS                      13\n2      MODELS  BIOLOGICAL           8  MODELS  BIOLOGICAL           8\n3      ANIMALS                      7  ANIMALS                      7\n4      COMPUTER SIMULATION          5  COMPUTER SIMULATION          5\n5      BIOLOGICAL EVOLUTION         4  BIOLOGICAL EVOLUTION         4\n6      MODELS  IMMUNOLOGICAL        4  MODELS  IMMUNOLOGICAL        4\n7      FEMALE                       3  FEMALE                       3\n8      MICE                         3  MICE                         3\n9      MUTATION                     3  MUTATION                     3\n10     AMINO ACID SEQUENCE          2  AMINO ACID SEQUENCE          2\n\n\nTime period 2\n\nsummary(res_new, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2015 : 2020 \n Sources (Journals, Books, etc)        22 \n Documents                             29 \n Annual Growth Rate %                  -9.71 \n Document Average Age                  4.72 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                             1 \n journal article                                               7 \n journal article;multicenter study;research support, n.i.h., extramural                                               1 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;review                                        1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.                                       1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.;review                                1 \n journal article;research support, non-u.s. gov't                                               4 \n journal article;research support, non-u.s. gov't;research support, n.i.h., extramural                                               1 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, n.i.h., extramural      1 \n letter                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    198 \n Author's Keywords (DE)                198 \n \nAUTHORS\n Authors                               209 \n Author Appearances                    332 \n Authors of single-authored docs       1 \n \nAUTHORS COLLABORATION\n Single-authored docs                  1 \n Documents per Author                  0.139 \n Co-Authors per Doc                    11.4 \n International co-authorships %        41.38 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2015        5\n    2016        7\n    2017        3\n    2018        6\n    2019        5\n    2020        3\n\nAnnual Percentage Growth Rate -9.711955 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1     HANDEL A          29    HANDEL A                      5.494\n2     WHALEN CC          7    ANTIA R                       0.810\n3     ANTIA R            5    SHEN Y                        0.723\n4     MARTINEZ L         5    WHALEN CC                     0.651\n5     SHEN Y             5    MCKAY B                       0.629\n6     LA GRUTA NL        4    EBELL MH                      0.571\n7     MCKAY B            4    THOMAS PG                     0.571\n8     THOMAS PG          4    LA GRUTA NL                   0.540\n9     ZALWANGO S         4    ROHANI P                      0.500\n10    DENHOLM JT         3    MARTINEZ L                    0.485\n\n\nTop manuscripts per citations\n\n                                                      Paper                                    DOI TC TCperYear NTC\n1  MCKAY B, 2020, PROC BIOL SCI                                      10.1098/RSPB.2020.0496         0         0 NaN\n2  MOORE JR, 2020, BULL MATH BIOL                                    10.1007/S11538-020-00711-4     0         0 NaN\n3  HANDEL A, 2020, NAT REV IMMUNOL                                   10.1038/S41577-019-0235-3      0         0 NaN\n4  MARTINEZ L, 2019, J INFECT DIS                                    10.1093/INFDIS/JIZ328          0         0 NaN\n5  WU T, 2019, NAT COMMUN                                            10.1038/S41467-019-10661-8     0         0 NaN\n6  MCKAY B, 2019, PLOS ONE                                           10.1371/JOURNAL.PONE.0217219   0         0 NaN\n7  DALE AP, 2019, J AM BOARD FAM MED SOCIOLOGICAL METHODS & RESEARCH 10.3122/JABFM.2019.02.180183   0         0 NaN\n8  WOLDU H, 2019, J APPL STAT                                        10.1080/02664763.2018.1470231  0         0 NaN\n9  HANDEL A, 2018, PLOS COMPUT BIOL                                  10.1371/JOURNAL.PCBI.1006505   0         0 NaN\n10 CASTELLANOS ME, 2018, INT J TUBERC LUNG DIS                       10.5588/IJTLD.18.0073          0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles  Freq SCP MCP MCP_Ratio\n1 USA             16 0.696   7   9     0.562\n2 AUSTRALIA        5 0.217   1   4     0.800\n3 GEORGIA          2 0.087   0   2     1.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    GEORGIA                 0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                                                       Sources       \n1  PLOS ONE                                                                                                                                          \n2  PLOS COMPUTATIONAL BIOLOGY                                                                                                                        \n3  THE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n4  THE LANCET. GLOBAL HEALTH                                                                                                                         \n5  THE LANCET. RESPIRATORY MEDICINE                                                                                                                  \n6  AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE                                                                                        \n7  BMC INFECTIOUS DISEASES                                                                                                                           \n8  BULLETIN OF MATHEMATICAL BIOLOGY                                                                                                                  \n9  ELIFE                                                                                                                                             \n10 EPIDEMICS                                                                                                                                         \n   Articles\n1         4\n2         2\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n          Author Keywords (DE)      Articles           Keywords-Plus (ID)     Articles\n1  HUMANS                                 23 HUMANS                                 23\n2  ANIMALS                                 8 ANIMALS                                 8\n3  FEMALE                                  7 FEMALE                                  7\n4  MALE                                    7 MALE                                    7\n5  MICE                                    6 MICE                                    6\n6  ADULT                                   5 ADULT                                   5\n7  CHILD                                   5 CHILD                                   5\n8  ADOLESCENT                              4 ADOLESCENT                              4\n9  ANTIVIRAL AGENTS/THERAPEUTIC USE        4 ANTIVIRAL AGENTS/THERAPEUTIC USE        4\n10 CHILD  PRESCHOOL                        4 CHILD  PRESCHOOL                        4\n\n\nNote that some values are reported as NA, e.g. the citations. Depending on which source you got the original data from, that information might be included or not. In my case, it is not.\n\n\nGetting a table of co-authors\nThis can be useful for NSF applications. For reasons nobody understands, that agency still asks for a list of all co-authors. An insane request in the age of modern science. If one wanted to do that, the following gives a table.\nUpdate: I have since created a short blog post describing how to do just that part in a bit more detail. It has a few additional components that might be useful, if interested check it out here.\nHere is the full table of my co-authors in the first period dataset.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_old$Authors[-1])\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nANTIA R\n6\n\n\nDOHERTY PC\n3\n\n\nLA GRUTA NL\n3\n\n\nLONGINI IM JR\n3\n\n\nTHOMAS PG\n3\n\n\nPILYUGIN SS\n2\n\n\nROHANI P\n2\n\n\nSTALLKNECHT D\n2\n\n\nTURNER SJ\n2\n\n\nAKIN V\n1\n\n\nBEAUCHEMIN CA\n1\n\n\nBIRD NL\n1\n\n\nBROWN J\n1\n\n\nCHADDERTON J\n1\n\n\nCUKALAC T\n1\n\n\nDESAI R\n1\n\n\nDICKEY BW\n1\n\n\nFUNG IC\n1\n\n\nHALL AJ\n1\n\n\nHALL D\n1\n\n\nHEMBREE CD\n1\n\n\nJACKWOOD MW\n1\n\n\nKEDZIERSKA K\n1\n\n\nKJER-NIELSEN L\n1\n\n\nKOTSIMBOS TC\n1\n\n\nLEBARBENCHON C\n1\n\n\nLEON JS\n1\n\n\nLEVIN BR\n1\n\n\nLI Y\n1\n\n\nLOPMAN B\n1\n\n\nMARGOLIS E\n1\n\n\nMATTHEWS JE\n1\n\n\nMCDONALD S\n1\n\n\nMIFSUD NA\n1\n\n\nMOFFAT JM\n1\n\n\nNGUYEN TH\n1\n\n\nPARASHAR UD\n1\n\n\nPELLICCI DG\n1\n\n\nROWNTREE LC\n1\n\n\nROZEN DE\n1\n\n\nWHALEN CC\n1\n\n\nYATES A\n1\n\n\nZARNITSYNA V\n1\n\n\nZHENG N\n1\n\n\n\n\n\nSince I have many more co-authors in the second period, I’m not printing a table with all, instead I’m just doing those with whom I have more than 2 joint publications.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_new$Authors[-1])\nauthortable <- authortable %>% dplyr::filter(Freq>2)\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nWHALEN CC\n7\n\n\nANTIA R\n5\n\n\nMARTINEZ L\n5\n\n\nSHEN Y\n5\n\n\nLA GRUTA NL\n4\n\n\nMCKAY B\n4\n\n\nTHOMAS PG\n4\n\n\nZALWANGO S\n4\n\n\nDENHOLM JT\n3\n\n\nEBELL M\n3\n\n\nMCBRYDE ES\n3\n\n\nSUMNER T\n3\n\n\nTRAUER JM\n3\n\n\n\n\n\n\n\nMaking a table of journals\nIt can be useful to get a list of all journals in which you published. I’m doing this here for the second time period. With just the bibliometrix package, I can get a list of publications and how often I have published in each.\n\njournaltable = data.frame(res_new$Sources)\n#knitr::kable(journaltable) #uncomment this to print the table\n\nAs mentioned in part 1 of this series of posts, the Impact Factor feature from the scholar package doesn’t work anymore. I’m leaving the old code in there in case it ever comes back. For now, there is no Impact Factor information. (I haven’t tried to figure out if there is another way to get it.)\nIt might also be nice to get some journal metrics, such as impact factors. While this is possible with the scholar package, the bibliometrix package doesn’t have it.\nHowever, the scholar package doesn’t really get that data from Google Scholar, instead it has an internal spreadsheet/table with impact factors (according to the documentation, taken - probably not fully legally - from some spreadsheet posted on ResearchGate). We can thus access those impact factors stored in the scholar package without having to connect to Google Scholar. As long as the journal names stored in the scholar package are close to the ones we have here, we might get matches.\n\n#library(scholar)\n#ifvalues = scholar::get_impactfactor(journaltable[,1], max.distance = 0.1)\n#journaltable = cbind(journaltable, ifvalues$ImpactFactor)\n#colnames(journaltable) = c('Journal','Number of Pubs','Impact Factor')\ncolnames(journaltable) = c('Journal','Number of Pubs')\nknitr::kable(journaltable)\n\n\n\n\n\n\n\n\nJournal\nNumber of Pubs\n\n\n\n\nPLOS ONE\n4\n\n\nPLOS COMPUTATIONAL BIOLOGY\n2\n\n\nTHE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n2\n\n\nTHE LANCET. GLOBAL HEALTH\n2\n\n\nTHE LANCET. RESPIRATORY MEDICINE\n2\n\n\nAMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE\n1\n\n\nBMC INFECTIOUS DISEASES\n1\n\n\nBULLETIN OF MATHEMATICAL BIOLOGY\n1\n\n\nELIFE\n1\n\n\nEPIDEMICS\n1\n\n\nEPIDEMIOLOGY AND INFECTION\n1\n\n\nFRONTIERS IN IMMUNOLOGY\n1\n\n\nJOURNAL OF APPLIED STATISTICS\n1\n\n\nJOURNAL OF THE AMERICAN BOARD OF FAMILY MEDICINE : JABFM\n1\n\n\nNATURE\n1\n\n\nNATURE COMMUNICATIONS\n1\n\n\nNATURE REVIEWS. IMMUNOLOGY\n1\n\n\nPHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY OF LONDON. SERIES B BIOLOGICAL SCIENCES\n1\n\n\nPLOS BIOLOGY\n1\n\n\nPROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA\n1\n\n\nPROCEEDINGS. BIOLOGICAL SCIENCES\n1\n\n\nTHE JOURNAL OF INFECTIOUS DISEASES\n1\n\n\n\n\n\nOk that worked somewhat. It couldn’t find several journals. The reported IF seem reasonable. But since I don’t know what year those IF are from, and if the rest is fully reliable, I would take this with a grain of salt.\n\n\nDiscussion\nThe bibliometrix package doesn’t suffer from the problems that I encountered in part 1 of this post when I tried the scholar package (and Google Scholar). The downside is that I can’t get some of the information, e.g. my annual citations. So it seems there is not (yet) a comprehensive solution, and using both packages seems best.\nA larger overall problem is that a lot of this information is controlled by corporations (Google, Elsevier, Clarivate Analytics, etc.), which might or might not allow R packages and individual users (who don’t subscribe to their offerings) to access certain information. As such, R packages accessing this information will need to adjust to whatever the companies allow.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Using {R} to Analyze Publications - Part 2},\n  date = {2020-02-02},\n  url = {https://www.andreashandel.com/posts/2020-02-02-publications-analysis-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Using R to Analyze Publications - Part\n2.” February 2, 2020. https://www.andreashandel.com/posts/2020-02-02-publications-analysis-2."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "title": "Text analysis of a mid-semester course survey",
    "section": "",
    "text": "Our center for teaching and learning administered a mid-semester survey to the students in my fall 2019 online Modern Applied Data Analysis course. I figured it would make for a nice and topical exercise if I performed some analysis of the survey results. Students agreed to have the - fully anonymous - results posted publicly. This is my quick and simple text analysis."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Some cleaning actions",
    "text": "Some cleaning actions\n\nd <- d %>% clean_names() #clean column names, which are the full questions\norig_quest <- data.frame(Number = paste0('Q',1:11), Question = names(d)) #save names and replace with simpler ones for now\nnames(d) = paste0('Q',1:11) #just call each column as Q1, Q2,... originallly asked question is stored in orig_quest\nkable(orig_quest) %>% kable_styling() #print them here for further reference  \n\n\n\n \n  \n    Number \n    Question \n  \n \n\n  \n    Q1 \n    whats_working_well_in_this_class_what_are_the_strengths_of_the_class_and_which_aspects_are_having_a_positive_impact_on_your_learning \n  \n  \n    Q2 \n    what_aos_not_working_so_well_in_this_class_what_aspects_are_having_a_less_positive_impact_on_your_learning \n  \n  \n    Q3 \n    what_specific_changes_do_you_think_should_be_made_to_improve_your_experience_in_this_class \n  \n  \n    Q4 \n    i_think_the_pace_of_this_class_is \n  \n  \n    Q5 \n    are_there_specific_modules_that_should_be_adjusted_and_how_5 \n  \n  \n    Q6 \n    the_quantity_of_material_covered_in_each_module_is \n  \n  \n    Q7 \n    are_there_specific_modules_that_should_be_adjusted_and_how_7 \n  \n  \n    Q8 \n    the_level_of_difficult_of_each_module_is \n  \n  \n    Q9 \n    are_there_specific_modules_that_should_be_adjusted_and_how_9 \n  \n  \n    Q10 \n    on_average_i_spend_this_many_hours_per_week_doing_work_for_this_course \n  \n  \n    Q11 \n    finally_what_is_your_gold_star_top_choice_number_one_recommendation_for_a_constructive_change_your_instructor_can_make_in_this_course \n  \n\n\n\n\n\nMore cleaning\n\nvisdat::vis_dat(d) #missing values\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n#looks like a few students left some entries blank. Should be ok. One student only answered 1 question. Quick look at entry.\nprint(d[12,2])\n\n# A tibble: 1 × 1\n  Q2                                       \n  <chr>                                    \n1 Elc system not work well for online class\n\n#ok, not too useful (though I agree with the statement). Let's remove that student/observation.\nd<- d[-12,]\n# most questions were free text, but some were specific choices, so should be grouped as factor.\nd <- d %>% dplyr::mutate_at(c(\"Q4\", \"Q6\",\"Q8\"), factor)\n#Q10 is number, should be numeric but was text field so different entries exist\n#small enough to print here\nprint(d$Q10)\n\n [1] \"20\"             \"15-20\"          \"15 or more\"     \"15\"            \n [5] \"14-16\"          \"12\"             \"15\"             \"10\"            \n [9] \"30\"             \"20\"             \"10 to 12 hours\" \">10 hours\"     \n[13] \"2\"              \"20\"            \n\n#ok, this is kinda bad style, but the dataset is so small that it's easiest to replace the non-numeric values by hand. I'll set them to their mean or the specified limit.\nd$Q10[c(2,3,5,11,12)] <- c(17.5,15,15,11,10)\nd$Q10 <- as.numeric(d$Q10)\nprint(d$Q10)\n\n [1] 20.0 17.5 15.0 15.0 15.0 12.0 15.0 10.0 30.0 20.0 11.0 10.0  2.0 20.0"
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Drawing first conclusions",
    "text": "Drawing first conclusions\n\nkable(orig_quest[c(4,6,8),]) %>% kable_styling()\n\n\n\n \n  \n      \n    Number \n    Question \n  \n \n\n  \n    4 \n    Q4 \n    i_think_the_pace_of_this_class_is \n  \n  \n    6 \n    Q6 \n    the_quantity_of_material_covered_in_each_module_is \n  \n  \n    8 \n    Q8 \n    the_level_of_difficult_of_each_module_is \n  \n\n\n\n\nd %>% dplyr::select(Q4, Q6, Q8) %>% summary()\n\n          Q4                Q6                Q8    \n just right:10   right amount:4   just right   :10  \n too fast  : 4   too much    :9   too difficult: 3  \n                 NA's        :1   NA's         : 1  \n\nplot(1:14,d$Q10, ylab = 'Time spent per week')\nlines(1:14,rep(12,14))\n\n\n\n\nBased on answers to questions 4,6 and 8, the majority of students think the pace and level of difficulty of the course is right but the amount of material covered is too much. Based on answer to Q10, students spend more time than my target (12 hours, solid line). Even accounting for some “inflation factor” (people generally over-estimate the time they spend on tasks like these, counting all the other things they do at the same time e.g., texting/email/FB/drinknig coffee/…), the overall amount seems too high, and it agrees with Q6 answers about too much material.\nFirst conclusion: Reduce weekly workload, probably best by reducing assigned reading (see text answers which I already glimpsed at 😃)."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Manual text analysis",
    "text": "Manual text analysis\n\n#dropping the question/variables analyzed above\nd <- d %>% dplyr::select( -c(\"Q4\", \"Q6\", \"Q8\", \"Q10\") )\n\nQuestions 5, 7 and 9 ask how modules should be adjusted regarding pace, quantity and difficulty, so it’s worth looking at those questions on their own.\n\nd2 <- d %>% dplyr::select( Q5, Q7, Q9)\nis.na(d2) #some students didn't write anything for any of those questions, remove before printing content.\n\n         Q5    Q7    Q9\n [1,] FALSE  TRUE FALSE\n [2,] FALSE FALSE FALSE\n [3,]  TRUE  TRUE  TRUE\n [4,] FALSE FALSE FALSE\n [5,] FALSE FALSE FALSE\n [6,]  TRUE  TRUE  TRUE\n [7,] FALSE FALSE FALSE\n [8,]  TRUE  TRUE  TRUE\n [9,]  TRUE  TRUE  TRUE\n[10,]  TRUE  TRUE  TRUE\n[11,] FALSE FALSE FALSE\n[12,]  TRUE  TRUE  TRUE\n[13,] FALSE  TRUE  TRUE\n[14,]  TRUE FALSE  TRUE\n\nd2 <- d2[which(rowSums(is.na(d2)) != 3),] #remove all rows/obersvations that have NA to all 3 questions\nnames(d2) <- c('too fast','too much','too hard')\nknitr::kable(d2) %>% kable_styling() ##show rest\n\n\n\n \n  \n    too fast \n    too much \n    too hard \n  \n \n\n  \n    All of them are to long except for module 2. \n    NA \n    The quizzes are becoming ridiculously difficult. \n  \n  \n    None, I have enjoyed the pace. \n    None, I think the content is appropriate. \n    The toughest module thus far in the course was the strings module. Though it should be noted, I think the content was very appropriate to cover the concepts of this module as they are quite difficult. I really benefited from completing some of the Regex crosswords while working through this chapter. \n  \n  \n    I think all modules need to be adjusted. \n    Considering we have only done the first half of the class and the shortest amount of time I have spent in one week on this class was the first module and I still did more than 9 hours of work for what should have been a half week seems unreasonable. For all other modules I spend over 12 hours each week on this class sometimes upwards of 25 hours in a single week. \n    I don't think the modules themselves are difficult just the content is being squeezed in and the time it takes doesn't correlate to the amount I feel like I'm learning, where I should be learning/proficient in much more than I currently am based on the amount of time spent. \n  \n  \n    The visualization module is pretty hefty in terms of how much time is spent. But it's all the same topic so I don't know how that would be split up. \n    see above \n    The modules are just difficult enough that the first couple times I try I struggle and it's hard, but after giving it a go it becomes much easier and I get it. So its enjoyably difficult. \n  \n  \n    I feel like I like the order and pace of the course materials. We can learn all of the material in the module in one week - it's just exhausting to do so. \n    Trim the reading in the modules and add more exploratory exercises. \n    I feel like the modules scale up in difficulty each week, but proportionally to the growth in our skills. I feel like I am really learning R for the first time! (After three other courses in R...) \n  \n  \n    In the tidyverse module, we used ggplot, but then we actually learned about how to do ggplot which would have been helpful before tidyverse. The R module was definitely a lot of material and the coding exercise was a steep learning curve for the first true coding exercise of the course. I understand sometimes the best way to learn something is struggling through, but I think it can be difficult for new students to make the sudden leap from follow a specific script in a book chapter to make up your own code. Maybe something as in altering existing scripts first? \n    While now there is a distinction on what is going to be on the quiz, there are multiple chapters and tutorials I haven‚Äôt read through yet. I‚Äôm actively searching to see what to prioritize first in a module. Module 4/5 also had a lot of (useful) material to work through. \n    I think working through the material for the quizzes themselves is just the right difficulty but the assessments range from not so difficult (Tidyverse) to spending a couple of hours googling and troubleshooting (first R and Visualization). While you said to spend no more than 1 or 2 hours on Visualization, it still took me 4 hours to figure out. \n  \n  \n    Not that I see so far. \n    NA \n    NA \n  \n  \n    NA \n    Most modules waver between just the right amount and a little too much. However, this goes hand in hand with what I have identified also as a strength of the course (great curation of resources), so perhaps adjustment isn't necessary... maybe it would be useful to have a more organized list of required and optional readings. \n    NA \n  \n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material (see above), level of difficulty overall ok but too fast/crowded. Again, solution is to reduce (required) material.\nNext, let’s look at “whats working/not working” questions.\n\nd2 <- d %>% dplyr::select( Q1, Q2)\nnames(d2) <- c('good','bad')\nknitr::kable(d2) %>% kable_styling() \n\n\n\n \n  \n    good \n    bad \n  \n \n\n  \n    The positive impact would be that I am learning completely new things I have had no exposure too. \n    This class is extremely overwhelming. It is online and the professor is the worst time estimator I have ever seen. He assigns way to much with absolutely no emphasis on what is important. As he is the ‚Äúexpert‚Äù in this subject he should be able to narrow it down and make this course more reasonable. I should not be spending 20-25 hours a week on one course that is 3 credits. \n  \n  \n    I really enjoy the exercises assigned for each module. I am receiving a good amount of background information from the modules/reading, however I feel I really start to understand the material once I have used it in practice. Additionally, a good portion of the exercises thus far are directly applicable to the analysis I will be using for my personal research which is excellent practice.\n\nI also really appreciate that the structure of the course. I really struggled with the basic work flow of R scripts and GitHub initially as I am newer to the program. However, the progressive flow that this course has created has allowed me to learn the process step-by-step and frequently connected back to information that was previously used in an earlier module. As a result, I have become very comfortable with the basic usage/flow of R and I am excited to move into the more detailed functions of the program in the latter half of the semester! \n    The only small issue I have encountered thus far with this course is with the quiz time limits. I am sometimes having trouble sifting through all of my notes quickly enough to properly answer the quiz questions. I take approximately 10-20 pages of notes on each module (depending on the size and amount of exercises) and I can sometime struggle to locate the specific material in my notes within these time constraints. I absolutely understand the need for a time limit with an online class structure, however 20 minutes can be a bit tight for some of the quizzes. A simple bump from a 20-30 minute time range to 30-40 minute time range would be more than enough to address all questions adequately, while still ensuring students review the module content beforehand. \n  \n  \n    I love the tidyverse package and thanks to instructions on R primer.\n\nI love R primer, because it raise the problem and then have a space to write code to try, and available solution. \n    Some materials (R data science, IDS) are useful too. However, I do not like them. The reason is that they give me the knowledge, and explain some simple code. Then, I have exercise part with more complicated questions. I do not know how to do it sometimes and got stuck. One example is chapter about regular expressions.\n\nAnother thing is that this class took me so much time per week. It is one third of my week. I still have other three classes, and research duties.\n\nI expect that taking class is the quicker and better way to learn than learning by myself. This class is not what I expect. It is too time-consuming because I mainly have to learn by myself. \n  \n  \n    The R primers are very useful for understanding the material, the exercises you give us, and some of the exercises in the IDS book (however, they need to be narrowed down to what is actually important). \n    Having 5+ chapters of reading each week isn't useful. The readings in general aren't useful for coding, at least for me. They should supplement and be a reference for the actual coding we are doing but not be the entire basis of the quizzes. I spend so much time reading and I don't actually understand any of it unless I'm doing the coding. However, expecting to do every exercise that comes with some of the reading isn't working because I currently spend at least 12 hours every week just doing the exercises that go with readings and I end up not retaining much because I'm overwhelmed. \n  \n  \n    I really enjoy having a structured way to learn r for data analysis instead of just learning it on my own. It makes it much more manageable and mentally-forgiving when there are other people learning/struggling at the same time as you. All of the data/resources are in one place and present in a timeline that makes it easier to understand and learn. \n\nI also really enjoyed how he makes us go back to the other student's repositories and work with them. It helps foster the feeling/attitude that we're classmates and get to know each other more even though it's an online course. \n    The resources are a lot to work through. I don't have previous R experience but this class is taking a lot of my time to work through the R for data science book. \n  \n  \n    The amount of information available. Strengths of the class are being able to work with other students to complete assignments and get ideas from one another. \n    The amount of work we have to do every week, including reading, quizzes, and lengthy assignments. Someone new to coding might find this extremely overwhelming considering they have other classes to work on as well as their own laboratory work. The quizzes make me feel like I have not learned anything because they are very weird, specific questions that I have to spend a long time going back through the reading to hunt for. \n  \n  \n    The course website is very thorough. It is very clear that Dr. Handel has put a lot of thought and effort into building the materials and course for this class. The writing is very clear and accessible. The site and materials are very consistent, which is helpful. \n\nThe RPrimers, IDS, and R4DS chapters are helpful (in that order, respectively). The class exercises are very helpful. I am finding myself coding more often and drafting unique codes, which was my goal from the start. This is the most well-rounded course in R I have taken. \n\nDr. Handel's exercises are by far the most helpful. We get to think through the material and use resources as-needed. These are exercises I will need in my own analysis. I have been using these codes as guidelines for working through my own data sets.\n\nI love the R-Primers as an introduction, and then the Exercises. I think we could take a stab at the exercises without the IDS and R4DS readings, or have those readings embedded into the exercise. \n\nSuch as...\"Try to make this figure\" If you are stuck, Chapter 4.5 might be a good a reference. \n    I'm lucky in that there are a five or six students in my department all taking this course. Since we see each other regularly, we can help each other along and make sense of the material. I can't imagine working through this course alone, with no face-to-face interaction. The group dynamic helps me read into what is important and helps trouble-shoot when there are problems. The eLC discussion board is not great for connecting with others...\n\nOutside of these few people that I am lucky to see in person, I think it is hard to connect with the other students to gauge if I am \"on the right track\" or \"falling behind\" or plain bad at coding. We don't engage much with other students, so it's hard to tell if they're succeeding or struggling as much as we are.  I keep thinking - \"am I the only one feeling this way?\" \n\nThe modules seem to be thick/dense for just one week's worth of work. As soon as I finish a module, I am exhausted by the material, but then have to start right back up again. I don't feel like I have enough time to play around with all of the functions we just learned before it's time to start learning new ones. I'd like more time to apply them to data sets in example exercises. \n\nI've been to a few meetings with my advisor now where I say that most of my week has been taken up with this course. Perhaps I need to work on my own time management and expectations for this course work. Of course, I do think the time investment will pay off! I am excited to start analyzing my own data set! \n\nTo get by, I feel like I have started skimming the readings, or reading them \"diagonally\" as some people may say. I have \"CTRL-F\" through the readings just for the quizzes. I don't think this is ideal, and I'm sure Dr. Handel wouldn't want us to resort to this. \n  \n  \n    I like the exercises in R. While my lack of previous experience means I have to spend more time trying to understand how to put complex displays together, what I learn from this stays with me a lot better. \n    I know that I review the material quite a bit for the quizzes yet continue to be surprised at what actually shows up on the quiz. This may be a personal problem as the class distribution shows that other individuals are doing well. However, I know that it continues to be frustrating. Also, the MADA course on GitHub is still a bit difficult to navigate and the info given for each module is a lot to absorb without there being an application element. \n  \n  \n    The supplementary readings are useful, but that's about all that's having a positive impact on my learning in this class. \n    The class being online is convenient, but if the class were in person and I could see step by step instructions, and everything would make more sense. Yes I understand you learn by making mistakes but taking 20+ hours every single time I do an assignment or reading is absolutely ridiculous due to the fact that all of my instructions are typed out, and there's no \"teaching\" in my opinion. This class structure is not conducive to my learning style. There aren't even any videos of the professor lecturing and walking us through lessons. I'm not absorbing any material by reading over 50+ pages then being thrown an assignment and an overly detailed quiz that doesn't even focus on main points but extremely detailed findings that DO NOT showcase what I've learned. \n  \n  \n    The class is quite well structured with nice use of different video resources. \n    Some of the assignments are very much like busy work and are time consuming without being particularly helpful and just end up putting one under pressure during the week whenever there are so many other draws on ones time at graduate school. \n  \n  \n    I do like the material we cover since it‚Äôs mostly relevant, I feel like I‚Äôm learning more of the ins and outs of R each week! I think the set up of the class with readings then an assessment works well. \n    There is a lot of material to cover each week. While I enjoy it and I‚Äôm interested, it is all mostly new to me and I feel like I have to rush through the readings to do the quiz by Friday. \n  \n  \n    This class requires a lot of coding, and I had no previous experience with coding, but we were provided with a lot of resources and exercises to start from the beginning. \n    There are too much reading assignments. It really takes a lot of time for this class. I cold barely finish these assignments every week, not alone to digest them and use them. \n  \n  \n    I think this course is set up very well. The modules are constructed in a manner that makes the content accessible and the exercises are well guided. \n    I would say that nothing related to the class is having a negative impact on my learning. \n  \n  \n    We receive a ton of excellent resources. Dr. Handel is very responsive to my feedback, which I greatly appreciate. I was very apprehensive about taking an online course, but my fears have been completely assuaged. This class has already been incredibly beneficial to me. \n    As I've already shared with Dr. Handel, the online quizzes are sometimes frustrating in that we are only asked 6 to 7 very specific questions on a great deal of material. I don't feel we're given enough time to complete it, and partial credit isn't awarded. However, as I mentioned above, Dr. Handel is very receptive to feedback and I have already spoken with him about my concerns with this issues. \n  \n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material, especially too much reading. R primers are good. Other resources are hit or miss. Quizzes are not working, need to be ditched or altered. Maybe more exercises. Find better alternative to eLC.\nFinally, the 2 remaining questions are about improvements, phrased in 2 different ways. Let’s look at them together.\n\nd2 <- d %>% dplyr::select( Q3, Q11)\nd2 <- d2[-12,] #this student didn't provide answers to either question\nnames(d2) <- c('specific suggested changes','number one recommendation')\nknitr::kable(d2) %>% kable_styling() \n\n\n\n \n  \n    specific suggested changes \n    number one recommendation \n  \n \n\n  \n    Less information per module. Most of these modules should be split into two weeks. \n    You need to put yourself in the eyes of a graduate student who is new to R and has a minimal stats background. As a graduate student in general public health field, what are the most important things to prepare is for publishing papers, understanding data, etc. \nWe should have spent two weeks on data wrangling and two weeks on data visualization. These are EXTREMELY important topics, and they were rushed through in one week. I do not think my understanding of how to do these things is good, and instead we are moving onto other topics that are less important. \n  \n  \n    As mentioned above, just a bit more time on the quizzes would be great! \n    A small increase in the time allotted to take the weekly quizzes. \n  \n  \n    I suggest that you can give the solution for this chapter ahead. We can learn through solution if we get stuck.\n\nTo reduce time to learn by myself, I suggest that professor can create R scripts with coding and explain what is the purpose of the code by text or better having a video to show how the code works in purpose.I think that this way will be better than the way that students have to go through many chapters of reading. If going through many chapters, students have to learn the new things and be able to finalize the knowledge at the same. This process is hard. So, I suggest that professor finalize the knowledge, then show it to us. You can indicate book in case they are still confused or do not understand, and need to read further. \n    For the knowledge that I learn through the R primer, I am happy with them. However, for other chapters that have to read through R4DS, and IDS, it is too time consuming and not an efficient way to learn. If you can have a better resources for the knowledge covered in R4DS and IDS, I am happy with this course. \n  \n  \n    More homework assignments that mimic the material that is in the readings, where we can use the readings as a reference/starting point. I feel like if we replace the quizzes/readings with an assignment that goes over what is generally important from the chapters you assign it would be more worthwhile. \n    No quizzes - replace with short assignments that reinforce the material presented in the reading each week. Still makes it so you have to read but not that you have to spend hours upon hours doings so. \n\nAlso as a side note - maybe introduce folders in the class github so that its organized by assignment so we can easily find things. \n  \n  \n    Specifically not much. \n    Establishing a working discussion board from the beginning would probably be the best thing. \n  \n  \n    I think that there should be more time allotted for the quizzes, simply because of the complexity of the questions as well as the vast amount of reading required. \n    Be a little more lenient on quizzes and giving out massive assignments. For an online course in coding, you're asking a lot from us. \n  \n  \n    It might be helpful to have \"coding drop in sessions\" where students can meet in a room, bring snacks, and discuss the course as a group. \n\nIt would be helpful if each modules was spread out over two weeks. It would be helpful to have more of Dr. Handel's exercises (not necessarily the IDS R4DS ones). \n    Trim the modules \n  \n  \n    I know that I do significantly better on the exercise portions of the class as I can see where the material is applicable but, again, this may be a personal preference. One blanket recommendation would be improving the navigational ability of the GitHub course site and breaking down the sections within each module to highlight where outside links need to be used. \n    NA \n  \n  \n    If this class is going to remain online I think the professor should record himself lecturing and have a split screen showing him using R instead of reading everything to get instructions. I think the supplementary reading/interactive learnings are helpful, but they should be in addition to teaching, not the only mode of learning. At this point I don't even feel like my professor is teaching me, a website and book are (barely). \n    Either change back to in person instruction or have video lectures with a computer split screen to show examples, THEN supplement with readings and interactive learning. The entire course just can't be a written instruction list and a website with definitions. \n  \n  \n    Make the assignments more relevant rather than busy work or at least have a required part and then then optional parts because to have to do a time consuming assignment each week isn't beneficial \n    NA \n  \n  \n    I liked the quiz this week (Module 7) because you specifically said that the exercises were optional in the reading so it did take the pressure off of having to go through multiple chapters. If the purpose of the quizzes are to make sure we are reading the material, then it was nice to focus on the content of the chapters. (And not lose 30 minutes to solving one exercise) \n    exercises from chapters not be covered on quizzes. :) \n  \n  \n    Again, the class is great. The only improvements are needed by me. \n    I do think the tidyverse is so massive an undertaking for a beginner that it could be split into two modules. \n  \n  \n    Nothing in particular comes to mind at the moment, as I've felt very comfortable voicing recommendations to Dr. Handel as they arise :) \n    Alternative way of testing for our reading each week? As of right now, the quizzes are precarious (one wrong answer can doom a grade) and time-constrained, and a pretty significant source of stress for me. However, as mentioned earlier, Dr. Handel and I are in communication about this! \n  \n\n\n\n\n\nConlusions from these answers: Reduce content per module (or alternatively increase time). Adjust or drop quizzes. More exercises. Record some lectures or provide links to recordings."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Automated text analysis",
    "text": "Automated text analysis\nSo this is likely not too useful, but I wanted to play around with some automated text analysis. Maybe the computer can figure out things I can’t?\nI don’t actually know how to do text analysis, so I’ll have to peek at the tidytext tutorial. Getting some ideas from this tutorial and the Text Mining with R book.\nTurn all answers into a long dataframe of words\n\nd2 <- d %>% tidyr::pivot_longer(cols = starts_with('Q'), names_to =\"question\", values_to = \"answers\") %>% \n            drop_na() %>%\n            unnest_tokens(word, answers, token = \"words\")\n\nLook at most frequent words.\n\nd2 %>%  count(word, sort = TRUE) \n\n# A tibble: 835 × 2\n   word      n\n   <chr> <int>\n 1 the     215\n 2 i       134\n 3 to      126\n 4 and      90\n 5 of       81\n 6 a        75\n 7 is       59\n 8 in       57\n 9 that     48\n10 are      47\n# … with 825 more rows\n\n\nThe usual words are the most frequent."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nSentiment analysis, look at most frequent positive and negative words.\n\nbing <- get_sentiments(\"bing\")\npositive <- bing %>% filter(sentiment == \"positive\")\nd2 %>% semi_join(positive) %>% nrow()\n\n[1] 158\n\nnegative <- get_sentiments(\"bing\") %>% filter(sentiment == \"negative\")\nd2 %>% semi_join(negative) %>% nrow()\n\n[1] 72\n\nbing_word_counts <- d2 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %>%\n  mutate(word = reorder(word, n)) \n\nPlot positive and negative words.\n\nbing_word_counts %>% ggplot(aes(word, n, fill = sentiment)) +\n  geom_bar( stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Counts\")\n\n\n\n\nAbout twice as many positive as negative words, i guess that’s good 😃. And the most frequent negative words do reflect that things are “too much”.\nLet’s look at sentiment per question. Higher values are more positive.\n\nquestion_sentiment <- d2 %>%\n      inner_join(bing) %>%\n      count(question, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative)\n\nggplot(question_sentiment, aes(question, sentiment)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\nNot surprising, the 1st question “what is working well” has lots of positive. Surprisingly, question 2, “what’s not working well” has fairly high positive sentiment. One problem could be that what I’m plotting is total counts, but I should probably normalize by total words written per question. Let’s try:\n\nwords_per_q <- d2 %>% group_by(question) %>% count()\nprint(words_per_q)\n\n# A tibble: 7 × 2\n# Groups:   question [7]\n  question     n\n  <chr>    <int>\n1 Q1         799\n2 Q11        393\n3 Q2        1304\n4 Q3         582\n5 Q5         193\n6 Q7         202\n7 Q9         240\n\n\nYep, looks like most words were written by far for Q2. Maybe not a good sign? But maybe ok, since this specifically solicited feedback on all aspects. So let’s replot sentiment, normalized by number of words.\n\nquestion_sentiment <- question_sentiment %>% mutate(sent_per_word = sentiment / words_per_q$n)\nggplot(question_sentiment, aes(question, sent_per_word)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\nOk, changed things a bit but not a lot. Q2 drop (expected) is most noticable change. Still, even for the “what’s not good” section, positive words dominate. That either means the course is quite good, or students are very optimistic or polite, or it might mean nothing at all."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Wordclouds",
    "text": "Wordclouds\nWhy not? Everyone loves a wordcloud, even if they are just fun to look at, right?\n\nd2 %>%\ninner_join(bing) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  wordcloud::comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nAt this point, I ran out of ideas for further text analysis. I didn’t think analysis by word pairs, or sentences, or such alternatives would lead to any further interesting results. I looked in the Text Mining with R book for some more ideas of what kind of analyses might be useful, but can’t come up with anything else. Not that the above ones are that useful either, but it was fun to try some text analysis, which is a type of data analysis I’m not very familiar with. So, I’ll stop this here. Feel free to play around yourself, you have access to the raw data and this script in the GitHub repository."
  },
  {
    "objectID": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "href": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "title": "Generating a conflict of interest form automatically",
    "section": "",
    "text": "Required packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(flextable)\n#remotes::install_github('massimoaria/bibliometrix')\nlibrary(bibliometrix)\n\n\n\nLoading data\nAs explained in a previous post, the currently best way to get all my papers is to download them from NIH’s “My Bibliography” and export it in MEDLINE format. Then read in the file with the code below.\n\n#read bib file, turn file of references into data frame\npubs <- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix website.\n\n\nGetting the right time period\nThis specific funding agency I’m currently writing a COI for (NIFA) requires co-authors of the last 3 years, so let’s get them. I don’t know if they mean 3 full years. I’m doing this mid-2020, so to be on safe side, I go back to 2017.\n\nperiod_start = 2017\npubs_new = pubs[pubs$PY>=period_start,]\n\nI need the full names of the authors. They are stored for each publication in the AF field. This is the only information I need for the COI form. I pull it out, then do a bit of processing to get it in the right shape, then remove duplicates and sort.\n\nallauthors = paste0(pubs_new$AF,collapse = \";\") #merge all authors into one vector\nallauthors2 = unlist(strsplit(allauthors, split =\";\"))\nauthors = sort(unique(allauthors2)) #split vector of authors, get unique authors\n\nNote that I originally did the above steps using biblioAnalysis(pubs_new). However, this function/approach broke in a recent version of the package, and I realized that I can just use a few base R commands to get what I need, which is the approach shown above. If you use the biblioAnalysis() function, the Authors are in the Authors field of the returned object.\n\n\nGetting a table of co-authors\nHere is the full table of my co-authors in the specified time period. I made a tibble that looks similar to what the COI document requires.\n\n#removing the 1st one since that's me\nauthortable = dplyr::tibble(Name = authors, \n                            \"Co-Author\" = 'x', \n                            Collaborator = '', \n                            'Advisees/Advisors' = '', \n                            'Other – Specify Nature' = '')\n\nFinally, I’m using the flextable package to make a decent looking table and save it to a word document.\n\nft <- flextable::flextable(authortable)\nflextable::autofit(ft)\n\n\n\nNameCo-AuthorCollaboratorAdvisees/AdvisorsOther – Specify NatureAHMED, HASANxALIKHAN, MALIHA AxAMANNA, IAN JxANTIA, ALICExANTIA, RUSTOMxBOOM, W HENRYxBULUSHEVA, IRINAxCARLSON, NICHOLE ExCASTELLANOS, M ExCASTELLANOS, MARIAxCHAKRABURTY, SRIJITAxCHEN, ENFUxCHENG, WEIxCOATES, P TOBYxCROFT, NATHAN PxDALE, ARIELLA PERRYxDENHOLM, J TxDOBBIN, KEVINxDUDEK, NADINE LxEBELL, MARKxEBELL, MARK HxEGGENHUIZEN, PETER JxFOREHAND, RONALDxFUGGER, LARSxGAN, POH YxGARCIA-SASTRE, ADOLFOxGREGERSEN, JON WxGUAN, JINGxHALLORAN, M ELIZABETHxHANDEL, AxHANDEL, ANDREASxHECKMAN, TIMOTHY GxHOLDSWORTH, STEPHEN RxHOLT, STEPHEN GxHOUBEN, R M G JxHUANG, HAODIxHUDSON, BILLY GxHUO, XIANGxHUYNH, MEGANxJOLOBA, MOSES LxKAKAIRE, RxKIRIMUNDA, SxKITCHING, A RICHARDxKIWANUKA, NxLA GRUTA, NICOLE LxLI, CHANGWEIxLI, CHAOxLI, YANxLING, FENGxLOH, KHAI LxLONGINI, IRA MxMALONE, LASHAUNDA LxMANICASSAMY, BALAJIxMARTINEZ, LxMARTINEZ, LEONARDOxMCBRYDE, E SxMCKAY, BRIANxMOORE, JAMES RxMU, LANxOOI, JOSHUA DxPAWELEK, KASIA AxPETERSEN, JANxPOWER, DAVID AxPURCELL, ANTHONY WxQUACH, TxQUINN, FREDERICK DxRAGONNET, RxRAMARATHINAM, SRI HxREID, HUGH HxROSSJOHN, JAMIExSETTE, ALESSANDROxSHEN, YExSIDNEY, JOHNxSLIFKA, MARKxSNG, XAVIER Y XxSTEIN, CATHERINE MxSUMNER, TxTAN, YU HxTHOMAS, PAUL GxTRAUER, J MxTSCHARKE, DAVID CxWAKIM, LINDA MxWANG, XIAOXIAOxWATSON, KATHERINE AxWHALEN, C CxWHALEN, CHRISTOPHER CxWILLETT, ZOE JxWOLDU, HxWOLDU, HENOKxWU, TINGxZALWANGO, SxZALWANGO, SARAHxZARNITSYNA, VERONIKAxZARNITSYNA, VERONIKA IxZHU, LIMEIx\n\nflextable::save_as_docx(\"my table\" = ft, path = \"COItable.docx\")\n\nI notice a few duplicates in the table that need to be removed. Of course I also need to remove myself. And for some, the full name doesn’t show. I need to fill in a few of the other columns and potentially add a few individuals who were not captured. So it’s not fully automated, but I can copy this table into the COI statement and the remaining edits are still annoying but not that terrible.\n\n\nDiscussion\nThese kinds of COI documents that ask for all co-authors are in my opinion antiquated and should go away. In the meantime using a somewhat automated approach makes the problem not too bad. I will have to make a few manual adjustments to the table, but overall it’s not too bad. I’m still glad that NIH does not require this.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Generating a Conflict of Interest Form Automatically},\n  date = {2020-05-29},\n  url = {https://www.andreashandel.com/posts/2020-05-29-automate-conflict-of-interest-form},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Generating a Conflict of Interest Form\nAutomatically.” May 29, 2020. https://www.andreashandel.com/posts/2020-05-29-automate-conflict-of-interest-form."
  },
  {
    "objectID": "posts/2020-10-07-custom-word-format/index.html",
    "href": "posts/2020-10-07-custom-word-format/index.html",
    "title": "Custom Word formatting using R Markdown",
    "section": "",
    "text": "Word template preparation\n\nCreate a new word document (either through RMarkdown -> Word, or just open Word and create a new empty one).\nOpen the word document. Write a bit of text. It doesn’t matter what you write, it’s just meant so you can create and apply new styles to it. For instance you can write Text for mystyle1.\nMark the text you wrote, click on the arrow to the left of the Styles box (see the red “circle” in the figure) and choose Create a style. Depending on your version of Word, this might be somewhere else. Create the formatting the way you want.\nRepeat to create as many custom styles as you want, save the word document into the folder of your RMarkdown file.\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown setup\nStart a new Rmarkdown document. Your YAML header should look something like this:\n---\ntitle: \"An example of formatting text blocks in Word\"\nauthor: \"Andreas Handel\"\ndocumentclass: article\nsite: bookdown::bookdown_site\noutput:\n  bookdown::word_document2: \n    toc: false\n    reference_docx: wordstyletemplate.docx\n---\nNote that I’m using bookdown as the output format here, but any others that can produce word output, e.g. the standard R Markdown format, should work equally well. The important part is the last line, which specifies the word document with the custom styles you created in the previous step.\n\n\nRMarkdown input content\nYou can now assign text blocks in your R markdown file specific styles. Here I created 3 styles called mystyle1/mystyle2/mystyle3 in the Word doc, and assign them to specific parts of the text. This example markdown text shows how to use the styles.\n# A regular section\n\nThis text is not getting a special format.\n\n# A formatted Section\n\n:::{custom-style=\"mystyle1\"}\nThis is formatted according to the _mystyle1_ format.\n:::\n\n# Another formatted block of text\n\nSome more regular text.\n\n:::{custom-style=\"mystyle2\"}\nNow text formatted based on _mystyle2_.\n:::\n\nMore regular text.\n\n:::{custom-style=\"mystyle3\"}\nThis format includes a border and it also works with an equation.\n$$Y = bX + c$$\n:::\n\nRegular text again.\n\n::: {custom-style=\"mystyle1\"}\n# With a header\n\nNote that the header formatting is overwritten.\n:::\n\n\nWord output\nThe resulting word document looks like this:\n\n\n\n\n\n\n\n\n\n\n\nSome notes\nOne thing you see in this example is that your own styles overwrite all others, so headers inside your custom style will just be formatted like your custom style. Some other quirks I noticed is that you seem to need empty lines before and after your custom style block. I seem to remember that formatting of R chunks works ok, but I also seem to recall that sometimes manual intervention is required.\nOverall, this approach gives you a good deal of flexibility for applying styling to your Word documents when writing in R Markdown, but there might still be some kinks. As I mentioned in the beginning, I ended up not using it for the project I had intended to use it (a review paper I wrote), so I don’t have a lot of real world experience beyond what I’m describing here.\n\n\nFurther Resources\nYou can get the Word template and the example R Markdown file I used to illustrate this here and here.\nI recently saw that the new R Markdown cookbook has a section describing word styling. I expect that more content will be added to the cookbook, so it’s worth checking regularly.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Custom {Word} Formatting Using {R} {Markdown}},\n  date = {2020-10-07},\n  url = {https://www.andreashandel.com/posts/2020-10-07-custom-word-format},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Custom Word Formatting Using R\nMarkdown.” October 7, 2020. https://www.andreashandel.com/posts/2020-10-07-custom-word-format."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "",
    "text": "Generating personalized documents is often useful. Since this is a very common task, programs like Word or similar software can do this. But I like to use R if I can. And the whole R Markdown system is perfectly suited for repeat generation of customized documents.\nI’m certainly not the first one to have the idea of using R, and in fact my initial approach is based on this prior post. Here, I describe a few ways of using R and R Markdown to auto-generate custom documents, and provide example code and explanations for anyone who might want to use this (including my future self)."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "LaTeX/PDF",
    "text": "LaTeX/PDF\nFor this approach, you start with a template file that contains some LaTeX commands, including the placeholders that will get personalized. Here is code for an example file, you can get the file here. We’ll look at the resulting output below.\n\n\n---\ntitle: \"\"\noutput: pdf_document\nclassoption: landscape\n---\n\n\\begin{center}\n\\includegraphics[height=4cm]{fig1.png}\n\\hfill\n\\includegraphics[height=4cm]{fig2.jpg} \\\\\n\\bigskip\n{\\Huge\\bf Certificate of Accomplishment } \\\\\n\\bigskip\n{\\Huge <<FIRSTNAME>> <<LASTNAME>> } \\\\\n\\bigskip\n{\\Large has successfully completed the course {\\it Generating Certificates with R} with a score of <<SCORE>> } \\\\\n\\bigskip\n{\\Huge Congratulations!}\n\\end{center}\n\n\nThis template places 2 images at the top, writes some text, and most importantly, adds some placeholder text that will be customized for each student with the script shown below. It doesn’t matter what placeholder text you write, as long as it’s unique such that when you do the replacement, only the instance you want replaced is actually changed. Enclosing with special characters such as << >> is a good option for this, but it’s not required.\nThe advantages of the LaTeX/PDF approach are that 1) LaTeX allows you to do a lot of formatting and customization of the template so it looks exactly the way you want it, 2) the end product is a PDF file, which is easy to print or share with those for whom they are meant. The disadvantage is that you need to know a bit of LaTeX to set up your template, or at least be willing to spend some time with Google until you found all the snippets of commands you need for the layout you want to have."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Word",
    "text": "Word\nFor this approach, you start with a template file that contains commands that lead to a decent looking Word document. Again, it needs to include the placeholders that will get personalized. Here is code for an example file, you can get the file here. We’ll look at the end result below.\n\n\n---\ntitle: \"\"\noutput: \n  word_document:\n    reference_docx: wordstyletemplate.docx\n---\n\n![Image](fig1.png)\n\n# Certificate of Accomplishment\n\n:::{custom-style=\"mystyle1\"}\n<<FIRSTNAME>>  <<LASTNAME>>\n:::\n\nhas successfully completed the course \"Generating Certificates with R\" with a score of <<SCORE>>\n\n:::{custom-style=\"mystyle2\"}\nCongratulations!\n:::\n\n```{r fig2, echo=FALSE, out.width=\"50%\"}\nknitr::include_graphics(\"fig2.jpg\")\n```\n\n\nNote that by default, going from R Markdown to Word doesn’t give you much ability to apply formatting. However, it is possible to do a decent amount of formatting using a word style template. I have another blog post which describes this approach, and I’m using it here.\nEven with the word style formatting, some things can’t be controlled well. Placement and sizing of figures is the main problem, no matter if you include the figures with basic Markdown commands or use the include_graphics() function. You’ll see the problem if you try to run this example (code below). As such, for something that includes figures, using the LaTeX/PDF workflow seems almost always better. A scenario where the Word setup might be useful is if you want to produce customized letters. The one main advantage of a Word output (in addition to not having to figuring out LaTeX commands) is that the output can be further edited if needed."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Other options",
    "text": "Other options\nI believe the PDF or Word outputs are best for most instances, but occasionally another format might be needed. You can use this overall approach to generate other outputs, for instance the standard R Markdown html output, or different versions of presentation slides (e.g. ioslides, Beamer, Powerpoint), etc. In principle, any R Markdown output format should work. You just need to alter your template file accordingly."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html",
    "href": "posts/2021-01-11-simple-github-website/index.html",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Hosting a website on GitHub is still a good idea. With the new Quarto framework available, I recommend using that setup. This page provides good instructions on how to start a Quarto website, and this page explains how to publish it on GitHub. Maybe I’ll get around to updating this post for Quarto soon. In the meantime, here is my short Hugo to Quarto transition guide.\nThe following blog post provides step-by-step instructions for creating a simple (and free) website using (R)Markdown and Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older verion of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "href": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Get Github up and running",
    "text": "Get Github up and running\nIf you are new to Github, you need to create an account. At some point, it would also be useful to learn more about what Git/Github is and how to use it, but for this purpose you actually don’t need to know much. If you want to read a bit about Git/Github, see e.g. this document, which I wrote for one of my courses.. But for now, you don’t need to know much about Git/Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/Github. I mosty use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free, if you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don’t need it for our purpose.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/Github client (e.g. the one that comes with RStudio), you can of course use that one too. Make sure you connect Gitkraken to your Github account."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html",
    "href": "posts/2021-03-21-simple-distill-website/index.html",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Distill is in my opinion at this point outdated and replaced by Quarto. Going forward, I recommend using the Quarto framework.\nThe following blog post provides step-by-step instructions for creating a website using R Markdown, the distill R package and GitHub."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you install a new R version.\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older version, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "href": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Get GitHub up and running",
    "text": "Get GitHub up and running\nIf you are new to GitHub, you need to create an account. At some point, it would also be useful to learn more about what Git/GitHub is and how to use it, but for this purpose you actually don’t need to know much. If you want to read a bit about Git/GitHub, see e.g. this document, which I wrote for one of my courses.."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/GitHub. I mostly use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free. If you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don’t need it for our purpose.\nOnce you have your GitHub account set up and Gitkraken installed, make sure you connect Gitkraken to your Github account.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/GitHub client (e.g. the one that comes with RStudio), you can of course use that one too."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "href": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Gitkraken",
    "text": "Gitkraken\n\nOpen GitKraken, go to File -> Init Repo -> Local Only. Give it the name of your main website directory, e.g. mywebsite. The Initialize In folder should be the folder above where you created the website, such that the Full path entry is the actual location of your website on your computer. For .gitignore Template you can choose R. The rest you can leave as is.\n\n\n\n\n\n\n\n\n\n\nOnce done, click Create repository. You should see a bunch of files ready for staging on the left. Click Stage all changes enter a commit message, commit. Then Click the Push button.\nAt this point, if you didn’t properly connect GitKraken and GitHub previously, you’ll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou’ll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your website folder name. Then click the green button. If things worked, your local website folder has been sent to GitHub and is ready to be turned into a website."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "href": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Github website",
    "text": "Github website\nFor the last step, go to your account on Github.com and find the repository for the website you just created. On the bar at the top, in the right corner there should be the Settings button. Click on it. Scroll down until you find the GitHub Pages section. Under Source, select Main and then choose /docs as the folder. Don’t choose a theme since we are using our own. Save those changes. If everything works (it could take a minute or so), your website is now live and public! Look right underneath the GitHub Pages section, there should be something like Your site is ready to be published at https://andreashandel.github.io/mywebsite/. Click on the link and your new site should show up.\nThat’s it. Now the hard part starts, creating good content. 😄"
  },
  {
    "objectID": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "href": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "title": "Analysis of economic papers",
    "section": "",
    "text": "One of the weekly assignments for the students is to participate in Tidy Tuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with Tidy Tuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week’s data was about an analysis of economic working papers catalogued by NBER. More on the data is here.\n\n\nLoading packages\nMake sure they are installed. Note: I don’t like loading meta-packages, such as the tidyverse. Doing so makes it really hard to figure our which packages are actually used. So I prefer to only load what I need.\n\nlibrary('ggplot2')\nlibrary('readr')\nlibrary('dplyr')\nlibrary('stringr')\nlibrary('tidytuesdayR')\n\n\n\nData loading\nApparently there is now a tidytuesdayR package that makes data loading very easy!\n\n#tuesdata <- tidytuesdayR::tt_load('2021-09-28')\n\nWell, that command above failed, claiming that date didn’t exist. So loading the data manually after all.\n\npapers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')\n\nRows: 29434 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, title\ndbl (2): year, month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nauthors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/authors.csv')\n\nRows: 15437 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): author, name, user_nber, user_repec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprograms <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')\n\nRows: 21 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): program, program_desc, program_category\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_authors <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')\n\nRows: 67090 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, author\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_programs <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')\n\nRows: 53996 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, program\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nUnderstanding the data\nThe explanations on the Tidy Tuesday website were a bit confusing. Looks like each CSV file only has a few variables, and papers.csv does not contain all the variables listed in the readme.\n\ncolnames(papers)\n\n[1] \"paper\" \"year\"  \"month\" \"title\"\n\ncolnames(authors)\n\n[1] \"author\"     \"name\"       \"user_nber\"  \"user_repec\"\n\ncolnames(programs)\n\n[1] \"program\"          \"program_desc\"     \"program_category\"\n\ncolnames(paper_authors)\n\n[1] \"paper\"  \"author\"\n\ncolnames(paper_programs)\n\n[1] \"paper\"   \"program\"\n\n\nPapers seem to be linked to authors by the paper_authors file and to programs (areas of work) by the paper_programs file. Probably best to combine all into one data frame. The Tidy Tuesday website has some code for that already, let’s see if it works\n\ndf <- left_join(papers, paper_authors) %>% \n  left_join(authors) %>% \n  left_join(paper_programs) %>% \n  left_join(programs)%>% \n  mutate(\n    catalogue_group = str_sub(paper, 1, 1),\n    catalogue_group = case_when(\n      catalogue_group == \"h\" ~ \"Historical\",\n      catalogue_group == \"t\" ~ \"Technical\",\n      catalogue_group == \"w\" ~ \"General\"\n    ),\n    .after = paper\n  ) \n\nJoining, by = \"paper\"\nJoining, by = \"author\"\nJoining, by = \"paper\"\nJoining, by = \"program\"\n\nglimpse(df)\n\nRows: 130,081\nColumns: 12\n$ paper            <chr> \"w0001\", \"w0002\", \"w0003\", \"w0004\", \"w0005\", \"w0006\",…\n$ catalogue_group  <chr> \"General\", \"General\", \"General\", \"General\", \"General\"…\n$ year             <dbl> 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973,…\n$ month            <dbl> 6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 1…\n$ title            <chr> \"Education, Information, and Efficiency\", \"Hospital U…\n$ author           <chr> \"w0001.1\", \"w0002.1\", \"w0003.1\", \"w0004.1\", \"w0005.1\"…\n$ name             <chr> \"Finis Welch\", \"Barry R Chiswick\", \"Swarnjit S Arora\"…\n$ user_nber        <chr> \"finis_welch\", \"barry_chiswick\", \"swarnjit_arora\", NA…\n$ user_repec       <chr> NA, \"pch425\", NA, \"pli669\", \"psm28\", NA, NA, NA, \"pli…\n$ program          <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ program_desc     <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ program_category <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThat seems to have worked, now all data is in one data frame.\n\n\nExploration 1\nMy first idea is very idiosyncratic. A friend of mine is an Econ/Finance professor at Bocconi University. I’m going to see if he has any papers in that dataset.\n\nsmalldf <- df %>% filter(grepl(\"Wagner\",name))\nprint(unique(smalldf$name))\n\n [1] \"Joachim Wagner\"       \"Alexander F Wagner\"   \"Todd H Wagner\"       \n [4] \"Stefan Wagner\"        \"Mathis Wagner\"        \"Ulrich J Wagner\"     \n [7] \"Wolf Wagner\"          \"Gernot Wagner\"        \"Zachary Wagner\"      \n[10] \"Katherine R H Wagner\" \"Myles Wagner\"         \"Kathryn L Wagner\"    \n\n\nNot seeing him in there. Just to make sure, a check on first name.\n\nsmalldf <- df %>% filter(grepl(\"Hannes\",name))\nprint(unique(smalldf$name))\n\n[1] \"Hannes Schwandt\"\n\n\nOk, nothing. Might be that his area, finance, is not indexed by NBER. I don’t know enough about the econ/business/finance fields enough to know what is and isn’t part of NBER. So I guess moving on.\n\n\nExploration 2\nIn most areas of science and when looking at publication records, one finds that most people publish very little (e.g. a student who is a co-author on a paper, then goes into the “real world” and never publishes again) and a few people publish a lot (super-star and/or old faculty). One usually sees an 80/20 pattern or a distribution that follows a power law. Let’s see what we find here.\nFirst, I’m doing a few more checks.\n\n#looking at missing data for each variable\nnas <- colSums(is.na(df))\nprint(nas)\n\n           paper  catalogue_group             year            month \n               0                0                0                0 \n           title           author             name        user_nber \n               0                0                0             2112 \n      user_repec          program     program_desc program_category \n           47158              530              530             1516 \n\n\nMissing data pattern seems ok. To be expected that some users don’t have those NBER or REPEC IDs.\nLet’s look at number of unique papers and unique authors.\n\nn_authors = length(unique(df$author))\nn_papers = length(unique(df$title))\nprint(n_authors)\n\n[1] 15437\n\nprint(n_papers)\n\n[1] 29419\n\n\nComparing those numbers to the original data frames, we see that the number of authors is same as in original authors data frame, that’s good. Number of papers (or at least unique titles) is less. Seems like some papers have the same titles? Non-unique titles is confirmed by checking the ID for each paper, which is the same as the one in the original papers CSV file.\nLet’s look at those titles that show up more than once. Note that we need to do that with the original papers data frame, since the merged one contains many duplicates since each author gets their own row.\n\n#using base R here, can of course also do that with tidyverse syntax\ndfdup <- papers[duplicated(papers$title),]\nprint(dfdup$title)\n\n [1] \"The Wealth of Cohorts: Retirement Saving and the Changing Assets of Older Americans\"        \n [2] \"Empirical Patterns of Firm Growth and R&D Investment: A Quality Ladder Model Interpretation\"\n [3] \"The Market for Catastrophe Risk: A Clinical Examination\"                                    \n [4] \"Taxation and Corporate Financial Policy\"                                                    \n [5] \"Asset Pricing with Heterogeneous Consumers and Limited Participation: Empirical Evidence\"   \n [6] \"Tax Incidence\"                                                                              \n [7] \"Liquidity Shortages and Banking Crises\"                                                     \n [8] \"Legal Institutions and Financial Development\"                                               \n [9] \"Inequality\"                                                                                 \n[10] \"Predictive Systems: Living with Imperfect Predictors\"                                       \n[11] \"Corruption\"                                                                                 \n[12] \"The Dynamic Properties of Financial-Market Equilibrium with Trading Fees\"                   \n[13] \"Forward Guidance\"                                                                           \n[14] \"Labor Market Integration Before the Civil War\"                                              \n[15] \"The Impact of Globalization on Pre-Industrial, Technologically Quiescent Economies\"         \n\n\nSome titles I can easily seen being used more than once, e.g. a paper called “Corruption”. Others sound very unique, so not sure why they show up as duplicates. If this were a serious analysis, I would look more closely into that. But for this exercise, and since it’s just a few titles, I’ll ignore and move on.\nI want to look at publications per author. Since names might not be unique but NBER ID should be, I’m just going to remove those authors that don’t have an NBER ID (likely most of them have co-authored very few papers) and focus on the remaining authors. For each, I’ll count their total papers by counting how often they show up.\n\ndfnew <- df %>% filter(!is.na(user_nber)) %>% \n                group_by(user_nber) %>% \n                summarise(n_papers = n() ) %>%\n                arrange(desc(n_papers)) %>%\n                mutate(allpapers = cumsum(n_papers)) \ndfnew$id = 1:nrow(dfnew) #add an ID variable for plotting\n\nLooking at the histogram of number of papers for each author.\n\np1 <- dfnew %>% ggplot(aes(n_papers)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLooks like expected, most authors have published only a few papers, a few have a lot.\nAnother way to look at this is with a violin plot\n\np1a <- dfnew %>% ggplot(aes(x=1, y=n_papers)) + geom_violin() \nplot(p1a)\n\n\n\n\nThat’s a very flat violin plot, with almost all the density close to 1.\nDoing a quick numerical summary\n\nsummary(dfnew$n_papers)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   8.983   7.000 359.000 \n\n\nOf course one needs to have at least 1 paper published to be in there, so that’s the minimum. The median is 3, so half of individuals have published 3 or less. The mean is higher, as expected for skewed data, and the max is at 359 papers, confirming the histogram which shows a few individuals wrote a lot of papers.\nJust because, let’s look at the 10 most published authors\n\nhead(dfnew,n=10)\n\n# A tibble: 10 × 4\n   user_nber        n_papers allpapers    id\n   <chr>               <int>     <int> <int>\n 1 jonathan_gruber       359       359     1\n 2 james_heckman         331       690     2\n 3 daron_acemoglu        308       998     3\n 4 janet_currie          306      1304     4\n 5 michael_bordo         297      1601     5\n 6 edward_glaeser        291      1892     6\n 7 joshua_aizenman       284      2176     7\n 8 martin_feldstein      272      2448     8\n 9 andrei_shleifer       242      2690     9\n10 alan_taylor           239      2929    10\n\n\nSince this is not my field, I don’t know any of those individuals. But looks like the 1st one, Gruber, is somewhat famous and also worked at NBER, so maybe not surprising that his papers are in there. Not sure, I don’t know how exactly NBER works, but it’s a quick consistency check and no red flag.\nThis looks at the accumulation of papers for the first N authors, with number of authors on the x-axis and total papers on the y axis. If every author were to contribute the same number of papers, we’d see a straight line up the diagonal. The fact that some authors write more papers, and most just a few, pushes the curve to the upper left corner. I’m also plotting a few lines that show the 80/20 idea, i.e. the vertical line indicates 20% of authors, the horizontal indicates 80% of all published papers.\n\nnobs = nrow(dfnew)\ntotpapers = max(dfnew$allpapers)\np2 <- dfnew %>% ggplot(aes(x=id, y=allpapers)) + \n                geom_point() + \n                theme(legend.position=\"none\") + \n               geom_segment(aes(x = floor(nobs*0.2), y = 1, xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) +\n geom_segment(aes(x = 1, y = floor(totpapers*0.8), xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) \nplot(p2)\n\n\n\n\nLooks like the NBER papers are fairly close to that 80/20 distribution, with few authors contributing the bulk, and most authors contributing little.\nNote that this does not account for co-authorship, just doing a per-author count.\n\n\nFurther explorations\nI’m going to leave it at this for now. In contrast to my 2019 Tidy Tuesday exploration I won’t try a fake statistical analysis here.\nBut I can think of a few other ideas and things to explore Here are a few:\n\nThe Tidy Tuesday website had a link to this article which looks at gender representation among the papers/authors. We could do that here too, e.g. follow their approach to try and guess gender for authors, then could stratify number of papers by gender of author.\nAnother possible exploration would be to look at the numbers of papers per author based on the area of research, i.e. the programs variable.\nYet another analysis one could do is to look at the pattern of publication for those that publish a good bit (say over 50 papers) and see how numbers of papers per year changes, or how number of co-authors changes over the years.\n\n\n\nSummary\nPatterns of authorship have been explored often. Sometimes they lead to useful information. At other times, one just needs more or less meaningful numbers for career advancement purposes. See e.g. an analysis of my own papers I did and wrote up in this post covering Google scholar data and this post using the bibliometrix package.\nMy exploration here was not too thorough, but some expected patterns showed up, namely the 80/20 skewed distribution in authorship.\n\n\n\n\nCitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Analysis of Economic Papers},\n  date = {2021-09-30},\n  url = {https://www.andreashandel.com/posts/2021-09-30-tidy-tuesday-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Analysis of Economic Papers.”\nSeptember 30, 2021. https://www.andreashandel.com/posts/2021-09-30-tidy-tuesday-exploration."
  },
  {
    "objectID": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "href": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "title": "Exploring the flowdiagramr R package",
    "section": "",
    "text": "Who this is (not) for\nIf you are interested in making diagrams using R and are looking for an option that might do things that current packages such as diagrammR or others can’t do, you might want to follow along.\nThe flowdiagramr package has (in my biased opinion) fairly decent documentation and vignettes. I’m not going to repeat all that here. I basically assume that if you want to try this out, you go to the package website and skim through the first few vignettes to get an idea of how things work. Then come back here. Or first skim through this post, then if you want, go and explore the package vignettes more.\n\n\nGeneral setup\nThe following shows code and my comments mixed together. It’s often useful to go slow and type your own code, or copy and paste the code chunks below and execute one at a time. However, if you are in a hurry, you can find all the code shown below in this file.\nFirst we load all needed packages.\n\nlibrary('ggplot2')\nlibrary('flowdiagramr')\nlibrary('sysfonts') #for extra fonts\nlibrary('showtext') #for extra fonts\n\n\n\nDiagram setup\nNow we specify the model. Generally, a model consists of variables/compartments/boxes and flows/arrows connecting the boxes. This code chunk defines the variables/boxes, gives them names (used later), and provides the box grid layout as a matrix.\n\n# the unique internal labels, just abbreviations of the full names\nvariables = c(\"GL\",\"P\",\"PTH\",\"PTC\",\"PJR\",\n              \"C\",\"CTB\",\"CTS\",\"CJR\",\n              \"B\",\"BTH\",\"BTS\",\"BJR\",\n              \"BE\")\n# the eventual names for all boxes\n# we'll use that later to label the boxes\nvarnames = c(\"Goldilocks\",\n             \"Porridge\",\n             \"Too hot\",\"Too cold\",\"Just right\",\n             \"Chairs\",\n             \"Too big\", \"Too small\", \"Just right\",\n             \"Beds\",\n             \"Too hard\",\"Too soft\",\"Just right\",\n             \"Bears!\")\n# assigning the varnames the variable labels\n# needed later\nnames(varnames) <- variables\n# a matrix specifying the locations on a matrix grid layout\n# this mimics the look of the original blog post\nvarlocations = matrix(data = c(\"\",\"GL\", \"\", \"\", \"\",\n                               \"\",\"P\", \"\", \"\", \"\",\n                               \"PTH\",\"PTC\", \"PJR\", \"\", \"\",\n                               \"\",   \"\",    \"C\", \"\", \"\",\n                               \"\",\"CTB\",  \"CTS\", \"CJR\", \"\",\n                               \"\",   \"\",   \"\",   \"B\", \"\",\n                               \"\",   \"\", \"BTH\", \"BTS\", \"BJR\",\n                               \"\",   \"\", \"\",     \"\", \"BE\"\n                      ), ncol = 5, byrow = TRUE)\n\nThe second component of every model are the flows between compartments/variables/boxes. Since flowdiagramr has as underlying logic the idea that flows occur between compartments, one needs to set up things as processes in such a way.\n\n# setting up the inflows and outflows (arrows) for each box\nflows = list( GL_flows = c(\"-k1*GL\"),\n              P_flows = c(\"k1*GL\",\"-k2*P\",\"-k3*P\",\"-k4*P\"),\n              PTH_flows = c(\"k2*P\"),\n              PTC_flows = c(\"k3*P\"),\n              PJR_flows = c(\"k4*P\",\"-k5*PJR\"),\n              C_flows = c(\"k5*PJR\",\"-k6*C\",\"-k7*C\",\"-k8*C\"),\n              CTB_flows = c(\"k6*C\"),\n              CTS_flows = c(\"k7*C\"),\n              CJR_flows = c(\"k8*C\",\"-k9*CJR\"),\n              B_flows = c(\"k9*CJR\",\"-k10*B\",\"-k11*B\",\"-k12*B\"),\n              BTH_flows = c(\"k10*B\"),\n              BTS_flows = c(\"k11*B\"),\n              BJR_flows = c(\"k12*B\",\"-k13*BJR\"),\n              BE_flows = c(\"k13*BJR\")\n)\n\n\n\nDiagram preparation\nThe first step for each flowdiagramr diagram is the preparation stage using the prepare_diagram() function. For that, one needs to supply the model as a list of variables and flows (boxes and arrows) and optional layout specifications.\n\n# model object\ngl_model = list(variables = variables, flows = flows)\n# model layout\nmodel_settings = list(varlocations=varlocations,\n                      varbox_x_size = 3)\n# prepare model\ngl_list = flowdiagramr::prepare_diagram(gl_model, model_settings)\n\n\n\nDiagram styling\nThe return from prepare_diagram is a list of data frames containing information about the variables and flows needed for plotting. One could go straight to making the diagram with make_diagram. But we already know the default doesn’t look like the blog post, therefore we apply some styling, which is done with the update_diagram function.\n\n#set colors that are similar to original blog post\nvarcolors = c(\"#b59dac\", rep(c(\"#D9AF6B\", \"#ACACAC\",\"#ACACAC\",\"#ACACAC\"),3), \"#b59dac\")\n# make them a named vector since that's required by update_diagram\nnames(varcolors) = variables\n# list of all style updates we want\ndiagram_settings = list(var_fill_color = varcolors,\n                        var_label_text = varnames,\n                        var_label_color = c(all =  \"#585c45\"),\n                        flow_show_label = c(all = FALSE),\n                        var_label_size = c(all = 4))\n\n# update the look\ngl_list2 <- flowdiagramr::update_diagram(gl_list,diagram_settings)\n\n\n\nDiagram generation\nNow we can make and plot the diagram\n\n# create and plot diagram\ngl_diag <- flowdiagramr::make_diagram(gl_list2)\nplot(gl_diag)\n\n\n\n\nThe result looks somewhat similar to the original, but not quite yet.\n\n\nMore diagram styling\nThe above is as far as we can get with flowdiagramr. The good news is that the created object is a a regular ggplot object and thus we can modify it further using ggplot2 code. A lot of this follows the original blog post, see there for details.\n\n# get different fonts\nsysfonts::font_add_google(name = \"Henny Penny\", family = \"henny\")\nshowtext::showtext_auto()\n\n# update the plot by adding ggplot2 commands\ngl_diag2 <- gl_diag  +\n       labs(title = \"The Goldilocks Decision Tree\",\n            caption = \"Made with flowdiagramr:\\n https://andreashandel.github.io/flowdiagramr/\") +\n       theme_void() +\n        theme(plot.margin = unit(c(1, 1, 0.5, 1), \"cm\"),\n        legend.position = \"none\",\n        plot.background = element_rect(colour = \"#f2e4c1\", fill = \"#f2e4c1\"),\n        panel.background = element_rect(colour = \"#f2e4c1\", fill = \"#f2e4c1\"),\n        plot.title = element_text(family = \"henny\", hjust = 0, face = \"bold\",\n                                  size = 45, color = \"#585c45\",\n                                  margin = margin(t = 10, r = 0, b = 10, l = 0)),\n        plot.caption = element_text(family = \"henny\", hjust = 0,\n                                    size = 16, color = \"#585c45\",\n                                    margin = margin(t = 10)),\n       text = element_text(family = \"henny\")\n       )\n\nNow we can plot it again\n\nplot(gl_diag2)\n\n\n\n\nThis plot is fairly close. I’m skipping the addition of the image since that was done manually.\nOne aspect that isn’t working is having the font in the boxes be the the henry style. I did try to supply it by setting text = element_text(family = \"henny\"), but it seems that doesn’t work after one has already written the text. I’m not aware of a way to update text in an ggplot once it’s already placed (I wouldn’t be surprised if that’s possible, I just don’t know how.) Fortunately, we can fix that in a different way.\n\n\nEven more diagram styling\nLet’s see if we can fix the font issue. While setting the font through update_diagram is on the to-do list, as of this writing, this feature does not yet exist (contributions welcome 😁).\nWe can however ask flowdiagramr to write all the ggplot2 code that produces the diagram into a file, then modify ourselves.\nThis writes the full ggplot code to an R script file:\n\nwrite_diagram(gl_list2,filename = \"gl_diag.R\", always_overwrite = TRUE)\n\nNow we can open that file, find the part that creates the text for the boxes (that part starts with “add label text”) and simply add this part family = \"henny\" into the geom_text() statement. We’ll also copy the commands from above to the end of the script to update the diagram_plot object. Then we’ll save the updated file, source it, and thus have an updated diagram. Here is the updated script if it’s not clear what I’m doing.\n\nsource(\"gl_diag_mod.R\")\n# plot new diagram\nplot(diagram_plot)\n\n\n\n\nSo that worked, very close to the original!\n\n\nA bit more exploration\nFor this specific diagram, using flowdiagramr is maybe not that much better than the original version. However, once one wants to include further features, including loops, flowdiagramr shows its strength. To illustrate this, let’s add a flows that shows that after Goldilock sits in the right chair, it induces hunger and she eats more porridge. (Yes, it’s silly, but I want to show how that can easily be implemented with flowdiagramr).\nTo show that, we update the flows as follows\n\n# need to define again since the above file overwrote it\nvariables = c(\"GL\",\"P\",\"PTH\",\"PTC\",\"PJR\",\n              \"C\",\"CTB\",\"CTS\",\"CJR\",\n              \"B\",\"BTH\",\"BTS\",\"BJR\",\n              \"BE\")\n\n# more complex flows\nflowsnew = list( GL_flows = c(\"-k1*GL\"),\n              P_flows = c(\"k1*GL\",\"-k2*P\",\"-k3*P\",\"-k4*P\",\"-kk1*P*CJR\"),\n              PTH_flows = c(\"k2*P\",\"k3a*PTC\"),\n              PTC_flows = c(\"k3*P\",\"-k3a*PTC\"),\n              PJR_flows = c(\"k4*P\",\"-k5*PJR\",\"kk1*P*CJR\"),\n              C_flows = c(\"k5*PJR\",\"-k6*C\",\"-k7*C\",\"-k8*C\"),\n              CTB_flows = c(\"k6*C\"),\n              CTS_flows = c(\"k7*C\"),\n              CJR_flows = c(\"k8*C\",\"-k9*CJR\"),\n              B_flows = c(\"k9*CJR\",\"-k10*B\",\"-k11*B\",\"-k12*B\"),\n              BTH_flows = c(\"k10*B\"),\n              BTS_flows = c(\"k11*B\"),\n              BJR_flows = c(\"k12*B\",\"-k13*BJR\"),\n              BE_flows = c(\"k13*BJR\")\n)\n\nThen we do the above steps to create the diagram:\n\n# model object\ngl_model_new = list(variables = variables, flows = flowsnew)\n# model layout\nmodel_settings = list(varlocations=varlocations,\n                      varbox_x_size = 3)\n# prepare model\ngl_list_new = flowdiagramr::prepare_diagram(gl_model_new, model_settings)\n# update the look\ngl_list_new2 <- flowdiagramr::update_diagram(gl_list_new,diagram_settings)\n# create and plot diagram\ngl_diag_new <- flowdiagramr::make_diagram(gl_list_new2)\nplot(gl_diag_new)\n\n\n\nggsave('featured.png',diagram_plot)\n\nSaving 7 x 5 in image\n\n\nI’m skipping the font adjustment and other parts, but of course you can apply that again. This ability to create feedback loops - which are very common in scientific process/simulation/mechanistic models - is the reason we built flowdiagramr. But as you saw, it can be used for all kinds of diagrams.\n\n\nFurther resources\nThe flowdiagramr website has a bunch of vignettes/tutorials with lots of examples and further use cases. If you are intrigued by this post, go check it out 😄.\n\n\nAcknowledgments\nDevelopment of flowdiagramr is a joint effort between myself and Andrew Tredennick, who did the majority of the actual coding work.\n\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Exploring the `Flowdiagramr` {R} Package},\n  date = {2022-06-11},\n  url = {https://www.andreashandel.com/posts/2022-06-11-flowdiagramr-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Exploring the `Flowdiagramr` R\nPackage.” June 11, 2022. https://www.andreashandel.com/posts/2022-06-11-flowdiagramr-exploration."
  },
  {
    "objectID": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "href": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "title": "Migration from Hugo/blogdown/Wowchemy to Quarto",
    "section": "",
    "text": "Who this is (not) for\nThis most directly targets folks who want to switch from blogdown/Hugo to Quarto. I’m hosting things on Netlify, but it should work for other hosting platforms too. Some of the tips might also be useful for folks who plan to build a Quarto website from scratch.\nThis is not a detailed walk-through. For that, see the Quarto documentation or for instance this blog post. I’ll describe a lot of steps only briefly, and make comments on some topics that might be not yet commonly known.\n\n\nSetting up the website\nI started by creating a new Quarto website. Either the Quarto documentation or these blog posts by Danielle Navarro and Albert Rapp worked well.\nFor the main page, I simply used the about page template that is built into Quarto. (While my about.qmd page is just a regular page.)\nI structured the new website to be as similar to my old as possible. For me, that meant folders and subfolders for posts, presentations and projects, and all other files (e.g. about.qmd) in the top level.\nNote that while I have posts and presentations separately, and under the old setup those pages were somewhat different, with Quarto there is (currently) no separate styling for presentations, thus it is basically another collection of posts. Only in this case, each post just contains the basic information of the presentation and a link to the slides and other relevant material. The same is true for the projects, they follow again the same structure as the posts, just somewhat differently formatted.\nIt would be possible to tune more, and make the presentations entries display differently. But I wanted to keep it simple, I learned that too much customization is just a time suck for me 😁.\nFor the projects, I made a change. They are also set up like posts, but I wanted the image that’s shown as thumbnail to also show on each page explaining a project. That was the way it was on my old site. I was thinking of messing with the project pages, but realized I could just use one of the ‘about’ page layout templates and that would do the trick. So I just added the solona template into the YAML of each file and got a layout that looked good enough for what I wanted.\n\n\nConfiguring the site\nThe main file for setting configurations is the _quarto.yml file. Additional configurations can be done in _metadata.yml inside the posts and presentations folders. I followed a mix of the Quarto documentation and the blog posts mentioned above and below to configure those files. You can check out my setup on the GitHub repo of this page.\n\n\nConverting YAML sections\nI had to do a bunch of manual conversions of all the index.qmd files that go with each post and presentation. It was a bit tedious but not too bad. Basically, I had to remove anything from the YAML that was specific for Hugo/Wowchemy and format it to have the fields supported by Quarto. I found Danielle’s post to be helpful for a quick orientation. (Note that she converts from Distill, so her starting point is slightly different, but the new Quarto entries are the same.)\n\n\nSetting up a 404/not found page\nThis is not really required, but I liked this approach from Lucy D’Agostino McGowan and decided to mostly copy it and adjust a bit for my purposes.\n\n\nTurning on comments\nI used utterances before to allow folks to comment on certain sites. This can be done easily as explained in this Quarto documentation section. To prevent comments on certain pages, one can turn those off in the YAML, also also described on the Quarto documentation page. An alternative is to place the utterances information into the _metadata.yml files, which then means comments are only on for those specific files/folders, in my case the blog posts and the presentations.\n\n\nHTML form embedding\nUpdate 2023-01-09: At the end of 2022, I decided to stop doing my newsletter for the time being and turned off the newsletter subscription section. Leaving this here since it might still be of general interest to figure out how to embed HTML.\nFor my newsletter subscription page, I embed HTML code produced by Mailchimp. It took me a little bit to figure out how to do it. This tip explained it, basically the setup looks like this:\n```{=html}\nALL THE HTML STUFF FROM MAILCHIMP GOES HERE\n```\n\n\nRedirects 1\nMy previous website used slug: labels in the YAML to provide shortened URLs. I wanted to keep those old URLs so breaking links are minimized. First I was thinking of following this example by Danielle Navarro and adjusting the code such that it parses the YAML of all my index.qmd files, pulls out the slug part and builds the paths and redirect table. It should have worked for me since I’m also hosting this website on Netlify, like Danielle.\nBut then I found a simpler option, using the aliases: setting, here is a an explanation/example. I tried to do an automated search and replace, but it didn’t quite work, so I ended up doing it manually. I basically replaced this:\nslug: publications-analysis-1\nwith this:\naliases: \n  - ../publications-analysis-1/\nin the YAML of all my posts (my presentations didn’t use slugs).\nNote on this: I ran into some problems initially. Since I was mapping into the same file structure, just a different file name, I wrote this:\naliases: \n  - publications-analysis-1\nThat didn’t work. On contemplation, the alias is referenced to the current document. So if my file is https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1/ I need to move up a folder and place the alias folder/file publications-analysis-1 there. That was confusing on first try. I only figured it out once I looked into the _site folder to see what Quarto was producing, and from that deduced the right setup.\n\n\nRedirects 2\nAfter I got the aliases bit to work, I realized that I needd further redirects. On my old blog, I had andreashandel.com/talk redirect to andreashandel.com/presentations. The first URL is on many of my presentation slides and I didn’t want to change them all. So I figured I should use Danielle’s approach after all, and make a small _redirects file that contained these. Basically it looks like this:\n/talk /presentations\n/talks /presentations\n/post /posts\nI followed her instructions of placing this bit of code into the main index.qmd file.\n\n\nWidgets\nOn my previous website, I had several sections (widgets) on the main page under the static welcome text. Those showed my Twitter feed and most recent posts and presentations. I was trying to see if I can reproduce that with Quarto. Based on this example it seems one could do something like that. I contemplated giving it a try. But then I decided to keep it simple, and let interested readers klick on my Twitter/Posts/Presentations sections if they want. No need to complicate things 😁.\n\n\nCustom footer\nI had a footer with copyright text that I wanted to keep. Albert Rapp’s post has an example of using an html file. I had that setup on my previous site, but I didn’t quite like the inclusion of html. I instead added the footer using the page-footer section in _quarto.yml as described here. I basically copied this statement from one of Andrew Heiss’ courses. To get the alignment right, I also had to grab a bit of code out of his custom.scss file and stick it int my custom.css file (I don’t really know the difference between scss and css, seemed easier to place it into the css file.)\n\n\nExtensions\nTo get the nice icons in the footer that Andrew has, one needs to install an extension. In this case, it’s the fontawsome extension. This is easily done, as described here. or here. I expect many more cool extensions to show up soon.\n\n\nOther settings\nI’m using Google Analytics, it is easy to include that in a Quarto website. I also turned on the Cookies setting notification (I don’t use any cookies directly, but Google Analytics likely does, and probably Mailchimp that I use for my newsletter too?).\n\n\nPublishing\nI host my website on Netlify. I followed the Quarto documentation. First I did a quarto publish netlify. That placed my website onto Netlify and gave it a temporary URL. That was great for testing it online (as opposed to testing locally with Quarto’s preview option, which of course I used a lot as well). Once online, I ran a link checker (I like using Dr Link Check but there are many others.) Of course there were broken internal links, so I went ahead and fixed them. I decided that I like the somewhat manual publishing of the site instead of doing it automatically with GitHub integration (also an option described in the Quarto documentation, and what I was using). The manual approach means I can mess around with new blog posts and sync with GitHub and don’t have to worry about using the main branch or not and then only once things look good do I publish with Quarto. So I changed my setup to that. That was done easily by setting the information in the _publish.yml file to point to my actual website URL, and changing the settings on the Netlify side as described in the Quarto documentation.\n\n\nSummary\nOverall, it wasn’t too hard. The one item that got me stuck for a bit was the aliases issue as described above. The conversion did require some manual changing, which I’m sure I could have written R scripts for it, but since my webpage isn’t that large, it seemed easier to just do things by hand. The new site is slightly different, some parts are simplified, but I got pretty much the same functionality and content back. And it is a much simpler setup compared to the - in my opinion - fairly convoluted setup of Hugo/blogdown/Wowchemy. Overall I’m happy with the results. My research group website is still using blogdown/Hugo/Wowchemy. That one has a few more custom layout features, which would likely require some fiddling before they work in Quarto. However, I have high hopes for those extensions and I’m pretty sure soon someone will have made new layouts, and then I might just be able to use some of them. So research group website conversion will happen, though probably not this year 😄.\n\n\nFurther resources\nIn addition to the main Quarto website and the blogs mentioned above, the a Quarto tip a day by Mine Çetinkaya-Rundel and Thomas Mock’s materials (look for his Quarto training repositories) are great resources. This repo has materials for one of his workshops, here is another one. This tutorial by Isabella Velásquez is another nice resource describing specifically Quarto blogs.\n\n\nAcknowledgments\nIt should be obvious that I owe a lot of these ideas to the blog posts and other resources I cite above. So thanks to those who tried it before me and wrote about it!\n\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Migration from {Hugo/blogdown/Wowchemy} to {Quarto}},\n  date = {2022-10-01},\n  url = {https://www.andreashandel.com/posts/2022-10-01-hugo-to-quarto-migration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Migration from Hugo/Blogdown/Wowchemy to\nQuarto.” October 1, 2022. https://www.andreashandel.com/posts/2022-10-01-hugo-to-quarto-migration."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nR Packages\n\n\nTools\n\n\nModeling\n\n\n\n\nA presentation covering some of the modeling tools and resources we previously developed.\n\n\n\n\n\n\nMay 17, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccine\n\n\n\n\nA research presentation covering some of our recent influenza vaccine work.\n\n\n\n\n\n\nMay 12, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for and discussion with our departmental PhD and MS students\n\n\n\n\n\n\nJan 17, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students to kick off a semester-long IDP development\n\n\n\n\n\n\nJan 10, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\n\n\nA presentation given at the 12th European Conference on Mathematical and Theoretical Biology.\n\n\n\n\n\n\nSep 22, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 14th time, and 3rd time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 20, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInoculum Dose\n\n\nNorovirus\n\n\nInfluenza\n\n\n\n\nA (virtual) research presentation given at York University.\n\n\n\n\n\n\nApr 21, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\nResearch\n\n\n\n\nA presentation for our departmental PhD/MS students on publishing (peer reviewed) papers.\n\n\n\n\n\n\nJan 15, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nCOVID-19\n\n\n\n\nA presentation of some recent projects, given at GA Southern University.\n\n\n\n\n\n\nNov 1, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nData Science\n\n\n\n\nA presentation given to team members of Metrum RG\n\n\n\n\n\n\nOct 26, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nModeling\n\n\nR package\n\n\n\n\nAn introduction to infectious disease modeling and software to learn it.\n\n\n\n\n\n\nOct 25, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nData Science\n\n\n\n\nA presentation given at a Data Science & Business Intelligence Society of Atlanta (virtual) meeting\n\n\n\n\n\n\nSep 24, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 13th time, and 2nd time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 14, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR package\n\n\nR package\n\n\n\n\nA pre-recorded presentation given at useR! 2021 introducing one of our new R packages\n\n\n\n\n\n\nJul 7, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\nInoculum Dose\n\n\n\n\nA (virtual) presentation discussing the role of dose for Influenza vaccines.\n\n\n\n\n\n\nApr 28, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students on mentorship\n\n\n\n\n\n\nApr 22, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\nResearch\n\n\n\n\nA presentation for our departmental PhD and MS students on how to find and implement good (academic) projects\n\n\n\n\n\n\nApr 1, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\nInoculum Dose\n\n\n\n\nA (virtual) presentation of some work on understanding the impact of dose for infection and vaccination.\n\n\n\n\n\n\nFeb 22, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\nAcademia\n\n\n\n\nA presentation for our departmental PhD and MS students on how to create CVs and resumes\n\n\n\n\n\n\nFeb 18, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\nWebsite\n\n\n\n\nSlides for a discussion with our departmental PhD and MS students on how to build an online brand and presence\n\n\n\n\n\n\nJan 21, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students to kick off a semester-long IDP development\n\n\n\n\n\n\nJan 14, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\n\n\nA (virtual) presentation of some work on understanding the impact of dose for Influenza vaccines.\n\n\n\n\n\n\nDec 2, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nModeling\n\n\nR package\n\n\n\n\nAn introduction ot infectious disease modeling and software to learn it.\n\n\n\n\n\n\nNov 23, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation of some recent COVID-19 modeling work.\n\n\n\n\n\n\nNov 18, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\nVaccines\n\n\n\n\nA (virtual) presentation and discussion covering COVID-19 vaccines and modeling.\n\n\n\n\n\n\nOct 22, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at Virginia Tech discussing some recent projects related to COVID-19 modeling and analysis.\n\n\n\n\n\n\nOct 7, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at the University of British Columbia discussing some recent COVID-19 modeling projects.\n\n\n\n\n\n\nSep 30, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA short talk given as part of an webinar on Battling Dual Threats: Flu and COVID-19 Converge hosted by MJH life sciences.\n\n\n\n\n\n\nSep 15, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at UGA’s Global Health Institute discussing some recent COVID-19 work.\n\n\n\n\n\n\nAug 27, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 12th time, and the first time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at GA Southern discussing some recent COVID-19 projects.\n\n\n\n\n\n\nApr 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nTuberculosis\n\n\n\n\nA presentation on some TB superspreading work I did for the EIA research group at UGA.\n\n\n\n\n\n\nMar 4, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD students on various topics related to (peer reviewed) papers.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD students on how to build and manage their brand (aka online presence).\n\n\n\n\n\n\nJan 23, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfectious Disease\n\n\n\n\nSome recent research exploring the role of dose for infection and vaccination.\n\n\n\n\n\n\nOct 23, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nImmunology\n\n\n\n\nAn introductory workshop on Infectious Diseases, Immunology and Within-Host Models.\n\n\n\n\n\n\nJul 17, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nInfectious Disease\n\n\nResearch\n\n\n\n\nAn introductory workshop on infectious disease modeling.\n\n\n\n\n\n\nJul 10, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/2019-07-hangzhou/index.html",
    "href": "presentations/2019-07-hangzhou/index.html",
    "title": "Introduction to Infectious Disease Modeling",
    "section": "",
    "text": "Outline\nThe following topics are covered in this workshop:\n\nIntroduction to infectious disease modeling\nSome example models\nHow to use simulation models\nTypes of models\nSources of uncertainty\nHow to build (good) models\nHow to assess modeling studies\nActive learning of infectious disease epidemiology\n\n\n\nPresentation Slides\nAll pdf slides as zip file\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Infectious} {Disease} {Modeling}},\n  date = {2019-07-10},\n  url = {https://www.andreashandel.com/presentations/2019-07-hangzhou},\n  langid = {en},\n  abstract = {An introductory workshop on infectious disease modeling.}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Introduction to Infectious Disease\nModeling.” July 10, 2019. https://www.andreashandel.com/presentations/2019-07-hangzhou."
  },
  {
    "objectID": "presentations/2019-07-sismid/index.html",
    "href": "presentations/2019-07-sismid/index.html",
    "title": "Introduction to within-host modeling",
    "section": "",
    "text": "Outline\nI cover the following topics in this workshop (my co-teacher Paul Thomas covers the Immunology part):\n\nIntroduction to modeling\nSome example models\nHow to use simulation models\nSources of uncertainty\nTypes of models\nHow to build (good) models\nHow to assess modeling studies\n\n\n\nPresentation Slides\nAll pdf slides as zip file\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to Within-Host Modeling},\n  date = {2019-07-17},\n  url = {https://www.andreashandel.com/presentations/2019-07-sismid},\n  langid = {en},\n  abstract = {An introductory workshop on Infectious Diseases,\n    Immunology and Within-Host Models.}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Introduction to Within-Host\nModeling.” July 17, 2019. https://www.andreashandel.com/presentations/2019-07-sismid."
  },
  {
    "objectID": "presentations/2019-10-paris/index.html",
    "href": "presentations/2019-10-paris/index.html",
    "title": "Model-based optimization of vaccine inoculum dose",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Model-Based Optimization of Vaccine Inoculum Dose},\n  date = {2019-10-23},\n  url = {https://www.andreashandel.com/presentations/2019-10-paris},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Model-Based Optimization of Vaccine\nInoculum Dose.” October 23, 2019. https://www.andreashandel.com/presentations/2019-10-paris."
  },
  {
    "objectID": "presentations/2020-01-your-brand/index.html",
    "href": "presentations/2020-01-your-brand/index.html",
    "title": "Building and curating your brand",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Building and Curating Your Brand},\n  date = {2020-01-23},\n  url = {https://www.andreashandel.com/presentations/2020-01-your-brand},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Building and Curating Your Brand.”\nJanuary 23, 2020. https://www.andreashandel.com/presentations/2020-01-your-brand."
  },
  {
    "objectID": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "href": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "title": "Reading, managing and publishing papers",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Reading, Managing and Publishing Papers},\n  date = {2020-02-20},\n  url = {https://www.andreashandel.com/presentations/2020-02-reading-managing-publishing-papers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Reading, Managing and Publishing\nPapers.” February 20, 2020. https://www.andreashandel.com/presentations/2020-02-reading-managing-publishing-papers."
  },
  {
    "objectID": "presentations/2020-03-tb-superspreaders/index.html",
    "href": "presentations/2020-03-tb-superspreaders/index.html",
    "title": "Tuberculosis Superspreading",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tuberculosis {Superspreading}},\n  date = {2020-03-04},\n  url = {https://www.andreashandel.com/presentations/2020-03-tb-superspreaders},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Tuberculosis Superspreading.” March\n4, 2020. https://www.andreashandel.com/presentations/2020-03-tb-superspreaders."
  },
  {
    "objectID": "presentations/2020-04-gasouthern-covid/index.html",
    "href": "presentations/2020-04-gasouthern-covid/index.html",
    "title": "Some recent analysis and modeling applied to COVID-19",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Some Recent Analysis and Modeling Applied to {COVID-19}},\n  date = {2020-04-20},\n  url = {https://www.andreashandel.com/presentations/2020-04-gasouthern-covid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Some Recent Analysis and Modeling Applied\nto COVID-19.” April 20, 2020. https://www.andreashandel.com/presentations/2020-04-gasouthern-covid."
  },
  {
    "objectID": "presentations/2020-07-sismid/index.html",
    "href": "presentations/2020-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2020-07-20},\n  url = {https://www.andreashandel.com/presentations/2020-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 20, 2020. https://www.andreashandel.com/presentations/2020-07-sismid."
  },
  {
    "objectID": "presentations/2020-08-uga-ghi/index.html",
    "href": "presentations/2020-08-uga-ghi/index.html",
    "title": "Studying COVID-19 Spread and Control",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Studying {COVID-19} {Spread} and {Control}},\n  date = {2020-08-27},\n  url = {https://www.andreashandel.com/presentations/2020-08-uga-ghi},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Studying COVID-19 Spread and\nControl.” August 27, 2020. https://www.andreashandel.com/presentations/2020-08-uga-ghi."
  },
  {
    "objectID": "presentations/2020-09-mjh/index.html",
    "href": "presentations/2020-09-mjh/index.html",
    "title": "Population-level patterns of COVID-19 and Flu: What should we expect?",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Population-Level Patterns of {COVID-19} and {Flu:} {What}\n    Should We Expect?},\n  date = {2020-09-15},\n  url = {https://www.andreashandel.com/presentations/2020-09-mjh},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Population-Level Patterns of COVID-19 and\nFlu: What Should We Expect?” September 15, 2020. https://www.andreashandel.com/presentations/2020-09-mjh."
  },
  {
    "objectID": "presentations/2020-09-ubc/index.html",
    "href": "presentations/2020-09-ubc/index.html",
    "title": "Modeling COVID-19",
    "section": "",
    "text": "The presentation slides are here.\nThe presentation slides are here.\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Modeling {COVID-19}},\n  date = {2020-09-30},\n  url = {https://www.andreashandel.com/presentations/2020-09-ubc},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Modeling COVID-19.” September 30,\n2020. https://www.andreashandel.com/presentations/2020-09-ubc."
  },
  {
    "objectID": "presentations/2020-10-uga-pha/index.html",
    "href": "presentations/2020-10-uga-pha/index.html",
    "title": "An overview of COVID-19 vaccines and modeling",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {An Overview of {COVID-19} Vaccines and Modeling},\n  date = {2020-10-22},\n  url = {https://www.andreashandel.com/presentations/2020-10-uga-pha},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “An Overview of COVID-19 Vaccines and\nModeling.” October 22, 2020. https://www.andreashandel.com/presentations/2020-10-uga-pha."
  },
  {
    "objectID": "presentations/2020-10-vt/index.html",
    "href": "presentations/2020-10-vt/index.html",
    "title": "COVID-19: Modeling, Visualization and Data Analysis",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {COVID-19: {Modeling,} {Visualization} and {Data} {Analysis}},\n  date = {2020-10-07},\n  url = {https://www.andreashandel.com/presentations/2020-10-vt},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “COVID-19: Modeling, Visualization and Data\nAnalysis.” October 7, 2020. https://www.andreashandel.com/presentations/2020-10-vt."
  },
  {
    "objectID": "presentations/2020-11-fyos/index.html",
    "href": "presentations/2020-11-fyos/index.html",
    "title": "User-friendly software for simulation modeling of infectious diseases",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {User-Friendly Software for Simulation Modeling of Infectious\n    Diseases},\n  date = {2020-11-23},\n  url = {https://www.andreashandel.com/presentations/2020-11-fyos},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “User-Friendly Software for Simulation\nModeling of Infectious Diseases.” November 23, 2020. https://www.andreashandel.com/presentations/2020-11-fyos."
  },
  {
    "objectID": "presentations/2020-11-pudong/index.html",
    "href": "presentations/2020-11-pudong/index.html",
    "title": "Simulation modeling to inform COVID-19 control and vaccination strategies",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Simulation Modeling to Inform {COVID-19} Control and\n    Vaccination Strategies},\n  date = {2020-11-18},\n  url = {https://www.andreashandel.com/presentations/2020-11-pudong},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Simulation Modeling to Inform COVID-19\nControl and Vaccination Strategies.” November 18, 2020. https://www.andreashandel.com/presentations/2020-11-pudong."
  },
  {
    "objectID": "presentations/2020-12-isv/index.html",
    "href": "presentations/2020-12-isv/index.html",
    "title": "The Role of Influenza Vaccine Dose Towards Homologous and Heterologous Protection",
    "section": "",
    "text": "The organizers also recorded the event, the video recording can be found here.\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The {Role} of {Influenza} {Vaccine} {Dose} {Towards}\n    {Homologous} and {Heterologous} {Protection}},\n  date = {2020-12-02},\n  url = {https://www.andreashandel.com/presentations/2020-12-isv},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “The Role of Influenza Vaccine Dose Towards\nHomologous and Heterologous Protection.” December 2, 2020. https://www.andreashandel.com/presentations/2020-12-isv."
  },
  {
    "objectID": "presentations/2021-01-building-your-brand/index.html",
    "href": "presentations/2021-01-building-your-brand/index.html",
    "title": "Building and curating your brand (online presence)",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Building and Curating Your Brand (Online Presence)},\n  date = {2021-01-21},\n  url = {https://www.andreashandel.com/presentations/2021-01-building-your-brand},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Building and Curating Your Brand (Online\nPresence).” January 21, 2021. https://www.andreashandel.com/presentations/2021-01-building-your-brand."
  },
  {
    "objectID": "presentations/2021-01-idp/index.html",
    "href": "presentations/2021-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Individual} {Development} {Plans} and the\n    {AAAS} {myIDP}},\n  date = {2021-01-14},\n  url = {https://www.andreashandel.com/presentations/2021-01-idp},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Introduction to Individual Development\nPlans and the AAAS myIDP.” January 14, 2021. https://www.andreashandel.com/presentations/2021-01-idp."
  },
  {
    "objectID": "presentations/2021-02-U01/index.html",
    "href": "presentations/2021-02-U01/index.html",
    "title": "The Role of Inoculum Dose Following Infection or Vaccination",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The {Role} of {Inoculum} {Dose} {Following} {Infection} or\n    {Vaccination}},\n  date = {2021-02-22},\n  url = {https://www.andreashandel.com/presentations/2021-02-U01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “The Role of Inoculum Dose Following\nInfection or Vaccination.” February 22, 2021. https://www.andreashandel.com/presentations/2021-02-U01."
  },
  {
    "objectID": "presentations/2021-02-cv-resume/index.html",
    "href": "presentations/2021-02-cv-resume/index.html",
    "title": "Tips for writing a (hopefully good) CV or resume",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tips for Writing a (Hopefully Good) {CV} or Resume},\n  date = {2021-02-18},\n  url = {https://www.andreashandel.com/presentations/2021-02-cv-resume},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Tips for Writing a (Hopefully Good) CV or\nResume.” February 18, 2021. https://www.andreashandel.com/presentations/2021-02-cv-resume."
  },
  {
    "objectID": "presentations/2021-04-CIVIC-meeting/index.html",
    "href": "presentations/2021-04-CIVIC-meeting/index.html",
    "title": "A Comparison of High-Dose and Regular-Dose Seasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous Immunity",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {A {Comparison} of {High-Dose} and {Regular-Dose} {Seasonal}\n    {Influenza} {Vaccines} {Toward} {Eliciting} {Homologous} and\n    {Heterologous} {Immunity}},\n  date = {2021-04-28},\n  url = {https://www.andreashandel.com/presentations/2021-04-CIVIC-meeting},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “A Comparison of High-Dose and Regular-Dose\nSeasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous\nImmunity.” April 28, 2021. https://www.andreashandel.com/presentations/2021-04-CIVIC-meeting."
  },
  {
    "objectID": "presentations/2021-04-good-projects/index.html",
    "href": "presentations/2021-04-good-projects/index.html",
    "title": "Coming up with good (research) projects",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Coming up with Good (Research) Projects},\n  date = {2021-04-01},\n  url = {https://www.andreashandel.com/presentations/2021-04-good-projects},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Coming up with Good (Research)\nProjects.” April 1, 2021. https://www.andreashandel.com/presentations/2021-04-good-projects."
  },
  {
    "objectID": "presentations/2021-04-mentorship/index.html",
    "href": "presentations/2021-04-mentorship/index.html",
    "title": "How to be a good mentee and mentor",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {How to Be a Good Mentee and Mentor},\n  date = {2021-04-22},\n  url = {https://www.andreashandel.com/presentations/2021-04-mentorship},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “How to Be a Good Mentee and Mentor.”\nApril 22, 2021. https://www.andreashandel.com/presentations/2021-04-mentorship."
  },
  {
    "objectID": "presentations/2021-07-sismid/index.html",
    "href": "presentations/2021-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2021-07-14},\n  url = {https://www.andreashandel.com/presentations/2021-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 14, 2021. https://www.andreashandel.com/presentations/2021-07-sismid."
  },
  {
    "objectID": "presentations/2021-07-useR/index.html",
    "href": "presentations/2021-07-useR/index.html",
    "title": "An R package to flexibly generate simulation model flow diagrams",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {An {R} Package to Flexibly Generate Simulation Model Flow\n    Diagrams},\n  date = {2021-07-07},\n  url = {https://www.andreashandel.com/presentations/2021-07-useR},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “An R Package to Flexibly Generate\nSimulation Model Flow Diagrams.” July 7, 2021. https://www.andreashandel.com/presentations/2021-07-useR."
  },
  {
    "objectID": "presentations/2021-09-DSATL/index.html",
    "href": "presentations/2021-09-DSATL/index.html",
    "title": "Adventures in Public Health Data Analytics - COVID-19 and beyond",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Adventures in {Public} {Health} {Data} {Analytics} -\n    {COVID-19} and Beyond},\n  date = {2021-09-24},\n  url = {https://www.andreashandel.com/presentations/2021-09-DSATL},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Adventures in Public Health Data Analytics\n- COVID-19 and Beyond.” September 24, 2021. https://www.andreashandel.com/presentations/2021-09-DSATL."
  },
  {
    "objectID": "presentations/2021-10-Metrum/index.html",
    "href": "presentations/2021-10-Metrum/index.html",
    "title": "Modeling the role of dose for vaccines & some other projects",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Modeling the Role of Dose for Vaccines \\& Some Other\n    Projects},\n  date = {2021-10-26},\n  url = {https://www.andreashandel.com/presentations/2021-10-Metrum},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Modeling the Role of Dose for Vaccines\n& Some Other Projects.” October 26, 2021. https://www.andreashandel.com/presentations/2021-10-Metrum."
  },
  {
    "objectID": "presentations/2021-10-fyos/index.html",
    "href": "presentations/2021-10-fyos/index.html",
    "title": "Introduction to infectious disease modeling",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to Infectious Disease Modeling},\n  date = {2021-10-25},\n  url = {https://www.andreashandel.com/presentations/2021-10-fyos},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Introduction to Infectious Disease\nModeling.” October 25, 2021. https://www.andreashandel.com/presentations/2021-10-fyos."
  },
  {
    "objectID": "presentations/2021-11-GASouthern/index.html",
    "href": "presentations/2021-11-GASouthern/index.html",
    "title": "Adventures in Data Analytics and Modeling - COVID-19 and Influenza",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Adventures in {Data} {Analytics} and {Modeling} - {COVID-19}\n    and {Influenza}},\n  date = {2021-11-01},\n  url = {https://www.andreashandel.com/presentations/2021-11-GASouthern},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Adventures in Data Analytics and Modeling -\nCOVID-19 and Influenza.” November 1, 2021. https://www.andreashandel.com/presentations/2021-11-GASouthern."
  },
  {
    "objectID": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "href": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "title": "Tips for publishing academic papers",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tips for Publishing Academic Papers},\n  date = {2022-01-15},\n  url = {https://www.andreashandel.com/presentations/2022-01-tips-for-publishing-papers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Tips for Publishing Academic\nPapers.” January 15, 2022. https://www.andreashandel.com/presentations/2022-01-tips-for-publishing-papers."
  },
  {
    "objectID": "presentations/2022-04-York-University/index.html",
    "href": "presentations/2022-04-York-University/index.html",
    "title": "Assessing the impact of dose on infection and vaccination outcomes",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Assessing the Impact of Dose on Infection and Vaccination\n    Outcomes},\n  date = {2022-04-21},\n  url = {https://www.andreashandel.com/presentations/2022-04-York-University},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Assessing the Impact of Dose on Infection\nand Vaccination Outcomes.” April 21, 2022. https://www.andreashandel.com/presentations/2022-04-York-University."
  },
  {
    "objectID": "presentations/2022-07-sismid/index.html",
    "href": "presentations/2022-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2022-07-20},\n  url = {https://www.andreashandel.com/presentations/2022-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 20, 2022. https://www.andreashandel.com/presentations/2022-07-sismid."
  },
  {
    "objectID": "presentations/2022-09-ECMTB/index.html",
    "href": "presentations/2022-09-ECMTB/index.html",
    "title": "The impact of seasonal Influenza vaccine dose on homologous and heterologous immunity",
    "section": "",
    "text": "Note: I had to cancel the trip at the last minute and thus was not able to deliver the presentation. I decide to leave the slides here anyway.\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The Impact of Seasonal {Influenza} Vaccine Dose on Homologous\n    and Heterologous Immunity},\n  date = {2022-09-22},\n  url = {https://www.andreashandel.com/presentations/2022-09-ECMTB},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “The Impact of Seasonal Influenza Vaccine\nDose on Homologous and Heterologous Immunity.” September 22,\n2022. https://www.andreashandel.com/presentations/2022-09-ECMTB."
  },
  {
    "objectID": "presentations/2023-01-academia-vs-industry/index.html",
    "href": "presentations/2023-01-academia-vs-industry/index.html",
    "title": "Some thoughts on academic versus industry positions",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Some Thoughts on Academic Versus Industry Positions},\n  date = {2023-01-17},\n  url = {https://www.andreashandel.com/presentations/2023-01-academia-vs-industry},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Some Thoughts on Academic Versus Industry\nPositions.” January 17, 2023. https://www.andreashandel.com/presentations/2023-01-academia-vs-industry."
  },
  {
    "objectID": "presentations/2023-01-idp/index.html",
    "href": "presentations/2023-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "This is a slightly updated version of a similar presentation from a few years ago.\n\n\n\nCitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Individual} {Development} {Plans} and the\n    {AAAS} {myIDP}},\n  date = {2023-01-10},\n  url = {https://www.andreashandel.com/presentations/2023-01-idp},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Introduction to Individual Development\nPlans and the AAAS myIDP.” January 10, 2023. https://www.andreashandel.com/presentations/2023-01-idp."
  },
  {
    "objectID": "presentations/2023-05-CEIRR/index.html",
    "href": "presentations/2023-05-CEIRR/index.html",
    "title": "Tools and resources for mechanistic within-host modeling",
    "section": "",
    "text": "Handel-CEIRR-CompCore-2023-05.html\n\n\n\nCitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tools and Resources for Mechanistic Within-Host Modeling},\n  date = {2023-05-17},\n  url = {https://www.andreashandel.com/presentations/2023-05-CEIRR},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Tools and Resources for Mechanistic\nWithin-Host Modeling.” May 17, 2023. https://www.andreashandel.com/presentations/2023-05-CEIRR."
  },
  {
    "objectID": "presentations/2023-05-CIVR/index.html",
    "href": "presentations/2023-05-CIVR/index.html",
    "title": "Drivers of heterologous influenza vaccine antibody responses",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Drivers of Heterologous Influenza Vaccine Antibody Responses},\n  date = {2023-05-12},\n  url = {https://www.andreashandel.com/presentations/2023-05-CIVR},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Drivers of Heterologous Influenza Vaccine\nAntibody Responses.” May 12, 2023. https://www.andreashandel.com/presentations/2023-05-CIVR."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nEpidemiology\n\n\nModeling\n\n\nR Package\n\n\n\nR package that teaches model-based infectious disease epidemiology in a user-friendly way.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nImmunology\n\n\nModeling\n\n\nR Package\n\n\n\nR package that teaches modeling for within-host infection and immunology.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nVisualization\n\n\nR Package\n\n\n\nR package that allows easy creation of high-quality flow diagrams.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nModeling\n\n\nTextbook\n\n\n\nAn online book (perpetually under construction) covering infectious disease epidemiology from a model-based perspective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nModeling\n\n\nCourse\n\n\n\nComplete materials of an online course on infectious disease epidemiology from a model-based perspective.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nData Analysis\n\n\nR Package\n\n\n\nR package containing a collection of online, interactive shiny/learnr based labs that teach basics of biostatistics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nR Package\n\n\n\nR package that allows building and analysis of simulation models without the need to write code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nData Analysis\n\n\nCourse\n\n\n\nComplete materials of an online course on Modern Applied Data Analysis\n\n\n\nAndreas Handel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nR Package\n\n\n\nR package that allows easy administration, submission and grading of online quizzes without the need of a learning management system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nTeaching\n\n\nInfectious Disease\n\n\nData Analysis\n\n\n\nA simple website containing lists with links to teaching and research resources.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nImmunology\n\n\nModeling\n\n\nCourse\n\n\n\nMaterials of an online course on simulation modeling in immunology.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/IDEMAbook/index.html",
    "href": "projects/IDEMAbook/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "An online book (perpetually under construction) convering infectious disease epidemiology using a model-based approach. Some parts of the book are fairly readable and complete enough that I use it when I teach a course on that topic. Other sections are currently only templates/outlines. While I try to ensure that what I write is correct, the whole book is not thoroughly fact-checked, error-corrected, properly referenced, etc. While I have been considering the idea of turning this into a full/real book, in my current thinking the cost-benefit analysis doesn’t pan out. I thus plan to leave it online for free as is, with occasional updates and fixes, but without an attempt to make it polished and complete enough for a printed book."
  },
  {
    "objectID": "projects/IDEMAcourse/index.html",
    "href": "projects/IDEMAcourse/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "The course also makes heavy use of my IDEMA online book and the DSAIDE R package."
  },
  {
    "objectID": "projects/MADAcourse/index.html",
    "href": "projects/MADAcourse/index.html",
    "title": "Modern Applied Data Analysis",
    "section": "",
    "text": "Modern Applied Data Analysis (MADA) is a course I regularly teach online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site."
  },
  {
    "objectID": "projects/SMIcourse/index.html",
    "href": "projects/SMIcourse/index.html",
    "title": "Simulation Modeling in Immunology",
    "section": "",
    "text": "The course makes heavy use of my DSAIRM R package."
  },
  {
    "objectID": "projects/dsaide/index.html",
    "href": "projects/dsaide/index.html",
    "title": "DSAIDE - Dynamical Systems Approach to Infectious Disease Epidemiology",
    "section": "",
    "text": "We developed an R package that teaches a modern, model-based approach to infectious disease epidemiology, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/dsairm/index.html",
    "href": "projects/dsairm/index.html",
    "title": "DSAIRM - Dynamical Systems Approach to Immune Response Modeling",
    "section": "",
    "text": "We developed an R package that provides immunologists and other bench scientists a user-friendly introduction to simulation modeling of within-host infection, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/flowdiagramr/index.html",
    "href": "projects/flowdiagramr/index.html",
    "title": "Flowdiagramr",
    "section": "",
    "text": "We developed the R package flowdiagramr to allow anyone to easily create high-quality ggplot2 based flow diagrams of simulation models (and other flowcharts) with just a few lines of R code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/iblir/index.html",
    "href": "projects/iblir/index.html",
    "title": "Introduction to Biostatistics Labs in R (iblir)",
    "section": "",
    "text": "iblir is an R package that contains several Shiny/learnr based tutorials that teach introductory aspects of biostatistics in an interactive way. Learn more about it on the package website."
  },
  {
    "objectID": "projects/modelbuilder/index.html",
    "href": "projects/modelbuilder/index.html",
    "title": "Modelbuilder",
    "section": "",
    "text": "We are developing an R package called modelbuilder that allows users to build and analyze compartmental, dynamical, mechanistic models (implemented as differential equations, discrete-time or stochastic), without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/quizgrader/index.html",
    "href": "projects/quizgrader/index.html",
    "title": "Quizgrader",
    "section": "",
    "text": "quizgrader is an R package allows teachers to administer and auto-grade quizzes that students submit online. It replaces the functionality often found in learning management systems (LMS). Student submissions can be fully analyzed to gain insights into problem areas. It is functional, but not yet fully tested. Learn more about it on the package website."
  },
  {
    "objectID": "projects/resourcelist/index.html",
    "href": "projects/resourcelist/index.html",
    "title": "Resource list website",
    "section": "",
    "text": "A simple website with a collection of lists with links to various resources that are related to my research and teaching, as well as some general (academic) career content."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "",
    "text": "This is a tutorial and worked example, to illustrate how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data in a multilevel/hierarchical/mixed-effects setup.\nI wrote it mainly for my own benefit/learning (nothing forces learning a concept like trying to explain it.) Hopefully, others find it useful too.\nIt started out as a single post, then became too large and is now a multi-part series. It currently has the following parts:\nI generally place further resources and acknowledgments sections at the end. However, since this series seems to be expanding and there is no clear order, I decided to get it out of the way and place these items right here at the beginning, before starting with the tutorial sequence."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for outcome (the likelihood)",
    "text": "Model for outcome (the likelihood)\nFor our scenario, the outcome of interest (the log of the virus load) is continuous, which we assume to be normally distributed. Note that this is technically never fully correct, since there is some lower limit of detection for the virus load, which would lead to a truncation at low values. (A similar upper limit of detection does at times also occur.) If you have such censored data, you have to decide what to do about them. Here, we assume for simplicity that all values are far away from any limits, such that a normal distribution is a good approximation.\nMaking this normal distribution assumption, the equation describing the outcome (the likelihood model) is\n\\[\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right)\n\\]\nThe \\(Y_{i,t}\\) are the measured outcomes (log virus load here) for each individual \\(i\\) at time \\(t\\). This is shown as symbols in the (simulated) data you can see in the figure above. The deterministic time-series trajectory for each individual is given by \\(\\mu_{i,t}\\) (shown as lines in the figure above). \\(\\sigma\\) captures variation in the data that is not accounted for by the deterministic trajectories.\nNote that you could assume a different distribution, based on the specifics of your data. For instance, if you had a time-series of counts, you could use a Poisson distribution. Some of the details would then change, e.g., you wouldn’t have a mean and standard deviation in your model, but instead a rate. But the overall setup described here will still work the same way."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for deterministic trajectories",
    "text": "Model for deterministic trajectories\nNext, we need to describe the underlying deterministic time-series model for the outcome.\nThere are different ways for choosing this part of the model. If you have enough information about your system to propose a mechanistic/process model, it is generally the best idea to go with such a model. Unfortunately, this is rare. Further, process models for time-series data are often implemented as differential equations, and those can take a very long time to fit.\nA more common approach is to model the overall pattern in the data with some type of phenomenological/heuristic function, chosen to match the data reasonably well. Generalized linear models (GLM), such as linear or logistic models, fall into that category. Here, we use such a phenomenological function, but a GLM won’t describe the pattern in our data (rapid virus growth, followed by decline). Therefore, we use an equation that gets us the shape we are looking for. For our simple example here, I choose a function that grows polynomially and declines exponentially with time. To be clear, this function doesn’t try to represent any real processes or mechanisms, it is simply chosen as an easy way to capture the general pattern seen in the virus load time-series. This is very similar to the use of GLMs or other standard models, which often work well at describing the overall pattern, without assuming a mechanistic process leading to the relation between predictor and outcome assumed by the GLM.\nThe equation for our model is given by\n\\[\n\\mu_{i,t} = \\log\\left( t_i^{\\alpha_i} e^{-\\beta_i t_i} \\right)  \n\\] In the model, \\(t_i\\) are the times for each individual \\(i\\) at which their outcome \\(Y_{i,t}\\) was measured. Those could be the same for each individual, which we’ll do here for simplicity, but they could also be all at different times and things won’t change. The model parameters are \\(\\alpha_i\\) and \\(\\beta_i\\), and their values describe the trajectory for each individual.\nYou can convince yourself with the following bit of code that this function, for the right values of \\(\\alpha\\) and \\(\\beta\\), gives you “up, then down” curves as a function of time. Note that since we are modeling the log of the virus load, I already log-transformed the equation.\n\nt = seq(0.1,30,length=100) #simulating 30 days, don't start at 0 to avoid 0/inf in plot\nalpha = 20; beta = 2; #just some values to show shape\nmu = log(t^alpha*exp(-beta*t)) #log virus load\nplot(t,mu, type = \"l\",ylim=c(0,30)) #looks somewhat like virus load in acute infections\n\n\n\n\n(Log) virus load time series for the heuristic model we will use.\n\n\n\n\nThe simple function I use here is in fact not that great for most real data, and better functions exist. See part 4 of this tutorial, where I show a more complex 4-parameter function, the one we actually used for our research problem. But to keep things somewhat simple here, I’m sticking with the 2-parameter function. It is fully sufficient to illustrate all the conceptual ideas I want to discuss. It can give us time-series which look somewhat similar to real virus load data often seen in acute infections. Of course, you need to pick whatever function describes your data reasonably well."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Numerical trickeries",
    "text": "Numerical trickeries\nLet’s go on a brief detour and discuss an important topic that comes up often.\nIn the equation for \\(\\mu_{i,t}\\) I just introduced, only positive values of \\(\\alpha\\) and \\(\\beta\\) produce reasonable trajectories. It is common to have parameters that can only take on certain values (e.g., positive, between 0-1). The problem is that by default, most fitting routines assume that the parameters that need to be estimated can take on any value. It turns out that the fitting software we will use (Stan through rethinking and brms) can be told that some parameters are only positive. You’ll see that in action later. But with different software, that might not be possible. Further, as you’ll also see below, we don’t actually fit \\(\\alpha\\) and \\(\\beta\\) directly, and it is tricky to enforce them to be positive using the built-in parameter constraint functionality of Stan.\nA general trick is to redefine parameters and rewrite the model to ensure positivity. Here, we can do that by exponentiating the parameters \\(\\alpha\\) and \\(\\beta\\) like this\n\\[\n\\mu_{i,t}  = \\log\\left( t_i^{\\exp(\\alpha_{i})} e^{-\\exp(\\beta_{i}) t_i} \\right)\n\\] Now, \\(\\alpha_i\\) and \\(\\beta_i\\) themselves can take any value without the danger of getting a nonsensical shape for \\(\\mu_{i,t}\\). It is likely possible to fit the model without taking those exponents and hoping that during the fitting process, the fitting routine “notices” that only positive values make sense. However, it might make the numerical procedures less robust.\nAnother alternative would be to enforce positive \\(\\alpha_i\\) and \\(\\beta_i\\) by setting up the rest of the model such that they can only take positive values. I’ll show a version for that in part 4.\nOne issue with that exponentiation approach is that it can sometimes produce very large or very small numbers and lead to numerical problems. For instance, if during the fitting the solver tries \\(\\beta_i = 10\\) and time is 10, then the exponent in the second term becomes \\(e^{-10 exp(10)}\\), and that number is so small that R sets it to 0. Similarly, if the solver happens to explore \\(\\alpha_i = 10\\) at time 10, we would end up with \\(10^{exp(10)}\\) in the first term, which R can’t handle and sets to Infinity. (Try by typing exp(-10 * exp(10)) or 10^exp(10) into the R console). In both cases, the result will not make sense and can lead to the numerical routine either completely failing and aborting with an error, or at a minimum wasting computational time by having to ignore those values. (Stan is good at usually not completely breaking and instead ignoring such nonsensical results, but one can waste a lot of computational time.)\nNumerical issues like this one are not uncommon and something to always be aware of. To minimize such problems with very large/small numbers, one can often use algebraic (logarithm, exponent, etc.) rules and rewrite the equations. In our case, we can rewrite as\n\\[\n\\mu_{i,t}  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i}\n\\] Using \\(\\mu_{i,t}\\) in this form in the code seems to work fine, as you’ll see. Note that this is exactly the same equation as the one above, just rewritten for numerical convenience. Nothing else has changed."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Modeling the main model parameters",
    "text": "Modeling the main model parameters\nOk, now let’s get back to building the rest of our model. So far, we specified an equation for the virus load trajectory \\(\\mu_{i,t}\\). We assume that every individual has their own virus-load trajectory, specified by parameters \\(\\alpha_i\\) and \\(\\beta_i\\). We need to define those. We allow each individual to have their own individual-level contribution to the parameters, and also assume there is a potential population-level effect of dose.\nThe latter assumption is in fact our main scientific question. We want to know if the dose someone receives has a systematic impact on the virus load trajectories. At the same time, we want to allow for variation between individuals. We could also consider a model that allows the impact of dose to be different for every individual. With enough data, that might seem feasible. But here, we assume we have limited data. (Of course, this is just simulated data, so it is as large as we want it to be. But for the real research project which motivates this tutorial, we only have data on 20 individuals.) We also really want to focus on the overall, population-level, effect of dose, and are less interested to see if there is variation of dose effect among individuals.\nIt is not clear how to best model the potential impact of inoculum dose. We really don’t have much biological/scientific intuition. Without such additional insight, a linear assumption is generally a reasonable choice. We thus model the main parameters \\(\\alpha_i\\) and \\(\\beta_i\\) as being linearly related to the (log of) the dose. This assumption relating the parameter to the log of the dose is mostly heuristic. But it does make some biological sense as often in systems like this, outcomes change in response to the logarithm of some input.\nIn addition to the dose component, every individual can have their unique contribution to \\(\\alpha_i\\) and \\(\\beta_i\\).\nWriting this in equation form gives\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\log (D_i) \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\log (D_i)\n\\end{aligned}\n\\] Here, \\(a_{0,i}\\) and \\(b_{0,i}\\) are the individual-level contributions of each person to the main parameters, and \\(a_1\\) and \\(b_1\\) quantify how the dose each individual receives, \\(D_i\\), impacts the overall time-series trajectories. \\(a_1\\) and \\(b_1\\) do not vary between individuals, thus we are only estimating the overall/mean/population-level impact of dose, and won’t try to see if different individuals respond differently to dose. If, after fitting the data, we find that the distributions for \\(a_1\\) and \\(b_1\\) are mostly around zero, we could conclude that dose does not have an impact. In contrast, if the distributions for those parameters are away from zero, we conclude that dose seems to impact the time-series trajectories.\nNote that if we were to fit this model in a frequentist framework, we would overfit (trying to estimate too many parameters). That is because if every individual has their own \\(a_{0,i}\\) and \\(b_{0,i}\\), the model can take any shape without needing the dose-related parameters to play a role. Thus we would have non-identifiability of parameters. As you’ll see in the next post of this series, this feature of potential overfitting/non-identifiability can also be seen in the Bayesian approach, but we are still able to obtain reasonable fits and parameter estimates. We’ll discuss that topic in more detail in the next post."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Some rescaling",
    "text": "Some rescaling\nAlright, time for another brief detour.\nThe model we have is ok. But as it is written right now, the parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the virus-load trajectory for an individual with a dose of 1 (log(1)=0). In our made-up example, individuals receive doses of strength 10/100/1000 but not 1. If we didn’t work with dose on a log scale, the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters would represent trajectories for individuals who received a dose of 0. That doesn’t make sense, since anyone not being challenged with virus will not have any virus trajectory.\nIt doesn’t mean the model is wrong, one can still fit it and get reasonable estimates. But interpretation of parameters, and thus choices for priors, might get trickier. In such cases, some transformation of the data/model can be useful.\nA common approach is to adjust predictor variables by standardizing (subtracting the mean and dividing by the standard deviation). Here we do something slightly different. We subtract the log of the middle dose. We call that dose \\(D_m\\). In our example it takes on the value of 100. In general, you can do any transformation that you think makes the setup and problem easier to interpret.\nThe equations then become\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right)\n\\end{aligned}\n\\] Now the intercept parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the main model parameters \\(\\alpha_i\\) and \\(\\beta_i\\), and thus the trajectory for the virus, if the dose is at the intermediate level. Thus, these parameters are now easy to interpret."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Quick summary",
    "text": "Quick summary\nAt this stage in the model building process, our model as the following parts\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\end{aligned}\n\\]\nThe model parameters are \\(\\sigma\\), \\(a_1\\), \\(b_1\\), \\(a_{i,0}\\) and \\(b_{i,0}\\). The latter two consist of as many parameters as there are individuals in the data. So if we have \\(N\\) individuals, we have a total of \\(3+2N\\) parameters.\nAt this point, we could fit the model in either a Bayesian or frequentist framework. For either approach, we need to determine what (if any) additional structure we want to impose on the model parameters.\nOne approach is to not impose any further structure. We could make the assumption that every individual is completely different from each other, such that their outcomes do not inform each other. That would mean we allow values of \\(a_{i,0}\\) and \\(b_{i,0}\\) to be different for each individual, and let them be completely independent from each other. Such a model is (in McElreath’s terminology) a no pooling model. Such a model is expected to fit the data for each individual well. But it would lead to overfitting, trying to estimate too many parameters given the data. That means the estimates for the parameters will be uncertain, and thus our question of interest, the possible impact of dose, will be hard to answer. Also, it won’t be very good at predicting future individuals.\nOn the other extreme, we could instead assume that all individuals share the same parameter values, i.e. \\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\). This is called a full pooling model. You see this model often in data analyses, when investigators take the mean of some measurements and then just model the means. For our example, it we would be modeling the mean virus load time-series trajectory for all individuals in a given dose group. This type of model can extract the population level (in our case dose) effect, but by ignoring the variation among individuals for the same dose, the model is likely overly confident in its estimates, and it leads to underfitting of the data. By not allowing differences between individuals, the model is likely too restrictive and thus is not that great at capturing the patterns seen in the data. We’ll explore that when we fit the models.\nAn intermediate model - and usually the best approach - is one that neither allows the \\(a_{i,0}\\) and \\(b_{i,0}\\) to be completely independent or forces them to be exactly the same. Instead, it imposes some correlation between the parameters. This leads to the mixed/hierarchical/multilevel modeling approach. Such an approach can be implemented in both a frequentist or Bayesian framework. Here, we focus on the Bayesian approach, which I personally find more intuitive since everything is explicit and spelled out."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Specifying priors",
    "text": "Specifying priors\nSince we are working in a Bayesian framework, our parameters need priors. We assume that for all models we discuss below, the parameters \\(a_{0,i}\\), \\(b_{0,i}\\), \\(a_1\\) and \\(b_1\\) have priors described by Normal distributions. The standard deviation \\(\\sigma\\) is modeled by a Half-Cauchy distribution (a Cauchy distribution that’s only defined for positive values, since standard deviations need to be positive). Those choices are a mix of convention, numerical usefulness and first principles. See for instance the Statistical Rethinking book for more details. You can likely choose other prior distributions and results might not change much. If they do, it means you don’t have a lot of data to inform your results and need to be careful about drawing conclusions.\nThe equations for our priors are\n\\[\n\\begin{aligned}\n\\sigma  \\sim \\mathrm{HalfCauchy}(0, 1)  \\\\\na_1  \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1  \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i}  \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\] I gave the prior distributions for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) fixed values. I chose those values to get reasonable simulation results (as you’ll see below). We will use those same values for all models. The priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) are more interesting. They depend on parameters themselves. In the next sections, we will explore different choices for those parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\), based on the different modeling approaches described in the previous section."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 1",
    "text": "Model 1\nOur first model is one that replicates the no pooling approach. In a Bayesian framework, such a no-pooling model can be implemented by making the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) very wide, which essentially assumes that any values are allowed, and there is (almost) no shared commonality/information among the parameters for each individual.\nIn our example, we can accomplish this by ensuring \\(\\sigma_a\\) and \\(\\sigma_b\\) are large, such that the normal distributions for \\(a_{i,0}\\) and \\(b_{i,0}\\) become very wide. In that case, the values for the mean, \\(\\mu_a\\) and \\(\\mu_b\\) don’t matter much since we allow the model to take on any values, even those far away from the mean. Therefore, we can just set \\(\\mu_a\\) and \\(\\mu_b\\) to some reasonable values, without paying too much attention.\nThis choice for the priors leads to a Bayesian model similar to a frequentist no-pooling model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 2",
    "text": "Model 2\nNow we’ll try to reproduce the full pooling model in a Bayesian framework. We could remove the individual-level variation by setting \\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\) (and we’ll do that below). But if we want to keep the structure we have above, what we need to do is to ensure the priors for those parameters are very narrow, such that every individual is forced to have more or less the same value. We can accomplish that by setting values for \\(\\sigma_a\\) and \\(\\sigma_b\\) very close to zero.\nIf we set the \\(\\mu_a\\) and \\(\\mu_b\\) parameters to some fixed values, we would enforce \\(a_{i,0}\\) and \\(b_{i,0}\\) to take specific values too. We don’t want that, we want them to be estimated, we just want to make sure all individuals get pretty much the same estimate. To do so, we can give \\(\\mu_a\\) and \\(\\mu_b\\) their own distributions and make those wide/flat. A normal distribution for each parameter with a wide variance should work.\nWith these choices, we have a model that can find the mean for \\(a_{i,0}\\) and \\(b_{i,0}\\), but the spread in those parameters is minimal."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 3",
    "text": "Model 3\nOk, so we discussed that the no pooling model 1 is too flexible and thus likely overfits, the full pooling model 2 is too rigid and likely underfits. Why not build a model that has reasonable priors in between those two? That’s a good idea and it leads us to a partial pooling model.\nWe want priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) that are not too flat/wide (model 1) or too narrow (model 2). They should allow some variation, but still ensure that there is shared information among the parameters. With that, we might be able to find a happy medium between underfitting and overfitting. Such priors are known as regularizing priors. They allow some variability for the parameters among individuals, while implementing the notion that the individuals share some commonality, and therefore their parameters should also share some common features, as indicated by belonging to the same prior distributions.\nThe question is, how to set the priors? One option is to pick some values for \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) (either by specifying their distributions or by directly setting values), then do prior predictive simulations, see if results look reasonable (no crazy outcomes, but still a good bit of variability) and then iterate until one found good priors. One can also explore the impact of the priors on the posterior. If they have a strong impact, it suggests there is not enough data to fully determine the posterior.\nThis approach of trial and error is reasonable, and we’ll use it here for our model 3. But it also feels a bit like ad-hoc. One might want to ask the question if there is another way to pick the priors. The answer is yes, which brings us to our next model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 4",
    "text": "Model 4\nInstead of trying to pick values for the priors manually (again, nothing wrong with that, but maybe not always optimal), one can let the data determine the priors. That approach involves estimating each of the parameters that specify the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\).\nThe parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) now get their own distributions, with their own priors (often called hyper-priors). The values for the hyper-priors are picked such that resulting simulations from the model produce (somewhat) reasonable trajectories, as you’ll see below. In principle, one can further specify them as functions of other priors (turtles, I mean priors, all the way down!). But in most cases, including our example, not much is gained from that.\nWhat now happens is that as we fit the model, our priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) share some information, and the amount of sharing is controlled by the hyper-prior parameters, which are determined by the data fitting process itself. It sounds a bit like magic, and I admit that on some deep level, I still don’t fully understand this magic, even though I can follow the steps. Maybe at some point in the future I will fully get it. For now I’m content with the level of understanding I have, and knowing that it works 😄.\nThis model is a partial pooling model like model 3, but now the pooling is determined adaptively by the data. This leads to a happy intermediate between the too-rigid full-pooling model and the too-flexible no-pooling model. This kind of model is often the best choice."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Recap",
    "text": "Recap\nOk, those were a lot of steps, so to have it all in one place, here are the models again, now shown with equations and with all components in one place.\nAll models have these parts:\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\na_1 \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1 \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\]\nFor model 1, we set the parameters describing the distribution for \\(a_{0,i}\\) and \\(b_{0,i}\\) to produce un-informative/flat priors. For our example, these values work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 10  \\\\\n\\sigma_b & = 10   \n\\end{aligned}\n\\]\nFor model 2, we set the standard deviations to a very small value and give the mean parameters somewhat flexible distributions. These work:\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & = 0.001  \\\\\n\\sigma_b & = 0.001  \n\\end{aligned}\n\\]\nAs mentioned, an alternative for model 2, which I’ll call model 2a, is to reduce the parameters from 2N to 2 and specify these priors for what are now population-level only parameters, like this:\n\\[\n\\begin{aligned}\na_{0} &  \\sim \\mathrm{Normal}(3, 1) \\\\\nb_{0} & \\sim \\mathrm{Normal}(1, 1)\n\\end{aligned}\n\\]\nFor model 3, we set values that lead to priors that are reasonably intermediate between the model 1 too flat and model 2 too narrow priors. These work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 1  \\\\\n\\sigma_b & = 1   \n\\end{aligned}\n\\]\nFinally, model 4 has distributions for all 4 parameters. These work for our example\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Comment on terminology",
    "text": "Comment on terminology\nI have been talking about 4 different models (or 5 if you count model 2a). As I’m sure you realized, some models are structurally the same, just with different choices for the priors. In a Bayesian framework, the priors (which includes choices for both the distribution and values) are part of the model, thus in that sense, model 1 and 3 can be considered different models, even if we only change the values for the variance priors. For the purpose of this tutorial I’ll take that perspective and consider them separate models. It also makes it easier to talk about them by giving each their own label/number."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "General settings",
    "text": "General settings\n\n## General settings\nset.seed(123) #for reproducibility\n# days at which we assume outcome is measured\ntimevec <- c(0.1,1,3,5,7,10,14,21,28,35,42)\n\n#different number of individuals per dose to make it clearer which is which\n#also, that's the structure of the data which motivated the tutorial\nNlow = 7; Nmed = 8; Nhigh = 9; filename = \"simdat.Rds\"\n#if you want to explore how model fitting changes if you increase sample size\n#turn on this line of code\n#this is used in part 4 of the tutorial\n#Nlow = 70; Nmed = 80; Nhigh = 90; filename = \"simdat_big.Rds\"\n\nNtot = Nlow + Nmed + Nhigh; #total number of individuals\n\n# Set values for dose\n# since we only consider dose on a log scale\n# we'll log transform right here and then always use it in those log units\nhigh_dose = log(1000)\nmed_dose = log(100)\nlow_dose = log(10)\ndosevec = c(rep(low_dose,Nlow),rep(med_dose,Nmed),rep(high_dose,Nhigh))\n# we are also creating a version of the dose variable\n# that consists of ordered categories instead of numeric values\n# we'll use that mostly for plotting\ndosevec_cat = ordered(c(rep(\"low\", Nlow),\n                        rep(\"medium\",Nmed),\n                        rep(\"high\",Nhigh)),\n                      levels=c(\"low\",\"medium\",\"high\"))\n\n\n## Setting parameter values\n\nI chose fairly low sample sizes, with less than 10 individuals for each dose group. This is motivated by the real data I have in mind, which has similar sample sizes. Of course, more data is generally better. In part 4 of the tutorial I play around a bit with fitting larger samples."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Setting parameter values",
    "text": "Setting parameter values\nThe parameters \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) show up in all models. For easy comparison between models, I’m setting them to the same value for all models.\nFor the estimation procedure (see part 2), we assume that those parameters follow the distributions shown above. We could sample a single value for each of them from such a distribution. But to reduce variability and to more easily compare estimated parameters to those used to simulate the data, I’m setting them to specific values, which you can conceptually think of as being a single sample from the distributions we discussed above. It makes sense to chose their means as the value to use.\n\nsigma = 1\na1 = 0.1\nb1 = -0.1\n\nNow well get values for the other parameters. For model 1, we have \\(N\\) parameters for \\(a_{i,0}\\) and \\(b_{i,0}\\), with priors that are very wide. We set them as follows\n\nm1_mua = 3\nm1_mub = 1\nm1_sigmaa = 1\nm1_sigmab = 1\nm1_a0 = rnorm(n=Ntot, m1_mua, m1_sigmaa)\nm1_b0 = rnorm(n=Ntot, m1_mub, m1_sigmaa)\n# saving main parameters\nm1pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m1_mua, b0_mu = m1_mub)\n\nNote a few things here. First, the priors are narrower than I specified above. As you will see in the figures below, even with these less wide priors, results for model 1 still look way too variable. We can use the wider priors when we fit the model, to allow the data to dominate the fits. But for data generation, going too wild/wide seems pointless.\nSecond, you noticed that I did sample from distributions for the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters. That’s not necessary, I could have also specified values for each of the parameters, like I did for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\), as long as the values can be thought of as coming from the underlying distribution. If I sample, I need to make sure to set a random seed (which I did above) to ensure reproducibility.\nLastly, I’m saving the parameters in a vector which will be added to the generated data so we can keep track of the parameters that were used to generate the data, and compare later with the estimates from the models.\nOk, now for model 2. We have 2 versions, model 2a collapses the individual-level parameters into a single population one. We’ll explore that model when doing the fitting, for simulating the data I’m just going with model 2.\n\nm2_mua = 3\nm2_mub = 1\nm2_sigmaa = 0.0001\nm2_sigmab = 0.0001\nm2_a0 = rnorm(n=Ntot, m2_mua, m2_sigmaa)\nm2_b0 = rnorm(n=Ntot, m2_mub, m2_sigmab)\nm2pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m2_mua, b0_mu = m2_mub)\n\nFinally, model 3 with priors that have a width between those of model 1 and model 2.\n\nm3_mua = 3\nm3_mub = 1\nm3_sigmaa = 0.1\nm3_sigmab = 0.1\nm3_a0 = rnorm(n=Ntot, m3_mua, m3_sigmaa)\nm3_b0 = rnorm(n=Ntot, m3_mub, m3_sigmaa)\nm3pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m3_mua, b0_mu = m3_mub)\n\nNote that for the purpose of simulating data, model 4 is basically the same as model 3. We would need to sample (or pick) values for the parameters \\(\\mu_a\\), \\(\\sigma_a\\), \\(\\mu_b\\), and \\(\\sigma_b\\) and then use them to sample (or set) values for \\(a_{i,0}\\) and \\(b_{i,0}\\). This setup makes sense during fitting, but for generating data, it isn’t really different than what er already did. You can conceptually assume we did sample parameters and happen to get the values shown for model 3. Thus, model 4 collapses to the models we already specified.\nOverall, when generating data, we can go through all steps of sampling from each specified distribution to get values. Nothing wrong with that. But if we change the random seed, values change. And it is harder to compare the parameters used to generate the data with those that are estimated. Thus, it is generally easier during the data generation process to assume conceptually that values correspond to samples from distributions, but then set the values manually. Above, we used that approach for most parameters. We did sample the \\(a_{0,i}\\) and \\(b_{0,i}\\) to show explicitly the sampling steps involved in generating simulated data from our Bayesian models."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Creating simulated data",
    "text": "Creating simulated data\nNow we can combine the parameters as specified in the equations above to get simulated trajectories for each individual, for each of the models. We just need to add the parameters together in the way prescribed by the model to get to the outcome. This is a nice feature of Bayesian models, that you can run them both “forward” to generate data given parameter values, and “backward” to estimate parameter values, given the data. Because of that feature, Bayesian models are generative models.\nHere is the code that computes the mean and the outcome for model 1, the wide model.\n\nm1_alpha = m1_a0 + a1*(dosevec - med_dose)\nm1_beta = m1_b0 + b1*(dosevec - med_dose)\n#doing matrix multiplication to get time-series for each individual\n#for that to work, the timevec vector needs to be transposed\nm1_mu =  exp(m1_alpha) %*% t(log(timevec)) - exp(m1_beta) %*% t(timevec)\n# apply variation following a normal distribution to each data point\nm1_y = rnorm(length(m1_mu),m1_mu, sigma)\n# in a final step, we reorganize the data into a long data frame with\n# columns id, time, dose, model,\n# the deterministic mean mu, and the normally distributed outcome.\n# We store dose in 3 versions, the original (log transformed one),\n# the one that has the middle value subtracted, and a categorical one.\n# Note that trick using sort to get time in the right order.\n# Not a robust way of doing things, but works here\nm1_dat <- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m1_mu),\n                     outcome = as.vector(m1_y),\n                     model = \"m1\")\n\nNow we just repeat the same code again for the other models.\n\n#model 2\nm2_alpha = m2_a0 + a1*(dosevec - med_dose)\nm2_beta = m2_b0 + b1*(dosevec - med_dose)\nm2_mu =  exp(m2_alpha) %*% t(log(timevec)) - exp(m2_beta) %*% t(timevec)\nm2_y = rnorm(length(m2_mu),m2_mu, sigma)\nm2_dat <- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m2_mu),\n                     outcome = as.vector(m2_y),\n                     model = \"m2\")\n\n#model 3\nm3_alpha = m3_a0 + a1*(dosevec - med_dose)\nm3_beta = m3_b0 + b1*(dosevec - med_dose)\nm3_mu =  exp(m3_alpha) %*% t(log(timevec)) - exp(m3_beta) %*% t(timevec)\nm3_y = rnorm(length(m3_mu),m3_mu, sigma)\nm3_dat <- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m3_mu),\n                     outcome = as.vector(m3_y),\n                     model = \"m3\")"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Plotting the simulated data",
    "text": "Plotting the simulated data\nTo ensure our simulated data makes sense, let’s plot what we produced. We’ll use ggplot2, so let’s load it first.\n\nlibrary('ggplot2')\n\nThese lines of code create plots for each model/simulated dataset.\n\np1 <- ggplot(m1_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,200)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\n\np2 <- ggplot(m2_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\np3 <- ggplot(m3_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\nNow, let’s plot the simulated data. For each plot, the lines show the deterministic mean trajectory, and the symbols show the outcomes, which have some extra variation on top, determined by the value of \\(\\sigma\\).\n\nplot(p1)\n\n\n\nplot(p2)\n\n\n\nplot(p3)\n\n\n\n\nAs you can see, the priors for model 1 are so wide that some of the resulting trajectories are not reasonable. The variation between individuals is so strong that the dose effect - which we programmed into the simulated data to exist - is swamped out. That could certainly be true for real data, sometimes there is just too much noise/variability to detect a pattern, even if it exists. But some of the trajectories produce virus load that’s just biologically unreasonable (note how high the y-values go.)\nOn the other extreme, the priors for model 2 allow so little variation that the individual-level variation is minimal and the only effect that is noticable is the dose dependence we assumed in our model (by setting \\(a_1\\) and \\(b_1\\) to non-zero values).\nModel 3 produces the most reasonable trajectories, with both the dose-effect showing, and some variation between individuals."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Saving the simulated data",
    "text": "Saving the simulated data\nFinally, let’s combine all the simulated data into a single list containing all data frames, and save it. I’m also saving the parameters for each model, and sample sizes, so we can quickly retrieve them when we compare with the model estimates.\nWe’ll also save one of the plots (this is mainly so I can show it at the beginning of the tutorial).\n\n#save a plot so we can use it in the blog post\nsimdat <- list(m1 = m1_dat, m2 = m2_dat, m3 = m3_dat, m1pars = m1pars, m2pars = m2pars, m3pars = m3pars, Nlow = Nlow, Nmed = Nmed, Nhigh = Nhigh)\nsaveRDS(simdat, file = filename)\nggsave(file = paste0(\"featured.png\"), p3, dpi = 300, units = \"in\", width = 6, height = 6)\n\nWarning: Removed 53 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 53 rows containing missing values (geom_point).\n\n\nWe’ll load and use the simdat file in the next parts of the tutorial."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "",
    "text": "This is part two of a tutorial illustrating how to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup. In this part, we’ll fit the simulated data using the rethinking package.\nI assume you’ve read part 1, otherwise this post won’t make much sense. You might even want to open that first part in a separate tab for quick comparison."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 1",
    "text": "Model 1\nThese lines of code specify the full set of equations for our model 1. Note how closely the R code resembles the mathematical notation. That close match between math and code is one of the nice features of rethinking/ulam. Also note the indexing of the parameters a0 and b0 by id, which indicates that each individual has their own values.\n\n#wide-prior, no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1 <- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nYou might have noticed that I chose some of the values in the priors to be different than the values we used to generate the simulated data. I don’t want to make things too easy for the fitting routine 😁. We want to have the fitting routine “find” the right answer (parameter estimates). Hopefully, even if we don’t start at the right values, we’ll end up there."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 2",
    "text": "Model 2\nNow we’ll set up model 2 exactly as for model 1 but with some of the priors changed as discussed previously. Specifically, the priors now force the individual-level parameters to be essentially all the same. Note that - as you will see below - this model is not a good model, and if one wanted to not allow the \\(a_0\\) and \\(b_0\\) parameters to have any individual level variation, one should just implement and run the model 2 alternative I describe below. We’ll run this model anyway, to just illustration and to see what happens.\n\n#narrow-prior, full-pooling model\n#2x(N+2)+1 parameters\nm2 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  0.0001),\n  b0[id] ~ dnorm(mu_b, 0.0001),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors. These priors are somewhat regularizing and more reasonable. As we’ll see, the results are similar to those from model 1, but the model runs more efficiently and thus faster.\n\n#regularizing prior, partial-pooling model\nm3 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(2,  1),\n  b0[id] ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 4",
    "text": "Model 4\nThis is our adaptive pooling model. For this model, we specify a few extra distributions.\n\n#adaptive priors, partial-pooling model\n#2x(N+2)+1 parameters\nm4 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  sigma_a),\n  b0[id] ~ dnorm(mu_b, sigma_b),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "A few model alternatives",
    "text": "A few model alternatives\nThere are a few model alternatives I also want to consider. The first one is a version of model 2 that gets rid of individual-level parameters and instead has only population-level parameters. I discussed this model in part 1 of the tutorial and called it 2a there. Here is the model definition\n\nModel 2a\n\n#full-pooling model, population-level parameters only\n#2+2+1 parameters\nm2a <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0 + a1*dose_adj,\n  beta <-  b0 + b1*dose_adj,\n  a0 ~ dnorm(2,  0.1),\n  b0 ~ dnorm(0.5, 0.1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nNote that a0 and b0 are not indexed by id anymore and are now single numbers, instead of \\(N\\) values as before.\n\n\nModel 4a\nAnother model I want to look at is a variant of model 4. This is in fact the same model as model 4, but written in a different way. A potential problem with model 4 and similar models is that parameters inside parameters can lead to inefficient or unreliable numerical results when running your Monte Carlo routine (in our case, this is Stan-powered Hamilton Monte Carlo). It is possible to rewrite the model such that it is the same model, but it looks different in a way that makes the numerics often run better. It turns out for our example, model 4 above runs ok. But it’s a good idea to be aware of the fact that one can re-write models if needed, therefore I decided to include this model alternative here.\nThe above model 4 is called a centered model and the re-write for model 4a is called a non-centered model. The trick is to pull out the parameters from inside the distributions for \\(a_{0,i}\\) and \\(b_{0,i}\\). The non-centered model looks like this:\n\n#adaptive priors, partial-pooling model\n#non-centered\nm4a <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  #rewritten to non-centered\n  alpha <-  mu_a + az[id]*sigma_a + a1*dose_adj,\n  beta  <-  mu_b + bz[id]*sigma_b + b1*dose_adj,\n  #rewritten to non-centered\n  az[id] ~ dnorm(0, 1),\n  bz[id] ~ dnorm(0, 1),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n\n  )\n\nAgain, this model is mathematically the same as the original model 4. If this is confusing and doesn’t make sense (it sure wouldn’t to me if I just saw that for the first time 😁), check the Statistical Rethinking book. (And no, I do not get a commission for continuing to point you to the book, and I wish there was a free online version (or a cheap paperback). But it is a great book and if you want to learn this kind of modeling for real, I think it’s worth the investment.)\n\n\nModel 5\nAnother model, which I’m calling model 5 here, is one that does not include the dose effect. That means, parameters \\(a_1\\) and \\(b_1\\) are gone. Otherwise I’m following the setup of model 1. The reason I’m doing this is because on initial fitting of the above models, I could not obtain estimates for the dose parameters I used for the simulation. I noticed strong correlations between posterior distributions of the model parameters. I suspected an issue with non-identifiable parameters (i.e, trying to estimate more parameters than the data supports). To figure out what was going on, I wanted to see how a model without the dose component would perform. It turned out that the main reason things didn’t look right was because I had a typo in the code that generated the data, so what I thought was the generating model actually wasn’t 🤦. A helpful colleague and reader pointed this out. Once I fixed it, things made more sense. But I figured it’s instructive to keep this model anyway.\n\n#no dose effect\n#separate intercept for each individual/id\n#2xN+1 parameters\nm5 <- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha <-  a0[id],\n  beta <-  b0[id],\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Setting starting values",
    "text": "Setting starting values\nAny fitting routine needs to start with some parameter values and then from there tries to improve. Stan uses a heuristic way of picking some starting values. Often that works, sometimes it fails initially but then the routine fixes itself, and sometimes it fails all the way. In either case, I find it a good idea to specify starting values, even if they are not strictly needed. And it’s good to know that this is possible and how to do it, just in case you need it at some point. Setting starting values gives you more control, and you also know exactly what should happen when you look at for instance the traceplots of the chains.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 2\nstartm2 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), mu_a = 2, mu_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4 and 4a\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 1, sigma_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\nstartm4a = startm4\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.3, b1 = -0.3, sigma = 1)\n#starting values for model 5\nstartm5 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), sigma = 1)\n\n#put different starting values in list\n#need to be in same order as models below\nstartlist = list(startm1,startm2,startm3,startm4,startm2a,startm4,startm5)\n\nNote that we only specify values for the parameters that are directly estimated. Parameters that are built from other parameters (e.g. \\(\\alpha\\) and \\(\\beta\\)) are computed and don’t need starting values.\nFor some more detailed discussion on starting values, see for instance this post by Solomon Kurz. He uses brms in his example, but the same idea applies with any package/fitting routine. He also explains that it is a good idea to set different starting values for each chain. I am not sure if/how this could be done with rethinking, it seems ulam does not support this? But it can be done for brms (and I’m doing it there)."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model fitting",
    "text": "Model fitting\nNow that we specified all models, we can loop through all models and fit them. First, some setup before the actual fitting loop.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 6000\niter = warmup + floor(warmup/2)\nmax_td = 18 #tree depth\nadapt_delta = 0.9999\nchains = 5\ncores  = chains\nseed = 4321\n\n#stick all models into a list\nmodellist = list(m1=m1,m2=m2,m3=m3,m4=m4,m2a=m2a,m4a=m4a,m5=m5)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))\n\n\n#setting for parameter constraints\nconstraints = list(sigma=\"lower=0\",sigma_a=\"lower=0\",sigma_b=\"lower=0\")\n\nThe first code block defines various settings for the ulam function. Look at the help file for details. Then we place all models into a list, set up an empty list for our fit results, and specify the data needed for fitting. The final command enforces some constraints on parameters. For our model, we want Half-Cauchy distributions for all variance parameters to ensure they are positive. Above, I specified them as Cauchy. There is no direct Half-Cauchy implementation. The way one achieves one is to tell ulam/Stan that the values for those parameters need to be positive. That’s what the constraints line in the code below does.\nLooping over each model and fitting it. In addition to the actual fitting call to ulam, I’m also printing a few messages and storing the model name and the time it took to run. That’s useful for diagnostic. It’s generally a good idea to do short runs/chains until things work, then do a large run to get the actual result. Recording the running time helps decide how long a real run can be and how long it might take.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  #run model fit\n  fl[[n]]$fit <- ulam(flist = modellist[[n]],\n                          data = fitdat,\n                          start=startlist[[n]],\n                          constraints=constraints,\n                          log_lik=TRUE, cmdstan=TRUE,\n                          control=list(adapt_delta=adapt_delta,\n                                       max_treedepth = max_td),\n                          chains=chains, cores = cores,\n                          warmup = warmup, iter = iter,\n                          seed = seed\n  )# end ulam\n\n  #capture time taken for fit\n  tdiff=proc.time()-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n\n} #end fitting of all models\n\n# saving the list of results so we can use them later\n# the file is too large for GitHub\n# thus I am saving here to a local folder\n# adjust accordingly for your setup\nfilepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"ulamfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nNow I’ll look at bit more carefully at the different models. We start by comparing fits for models 1 and 3. Those two are essentially the same model, with the only difference being wider priors for the individual-level parameters in model 1. It is worth mentioning that when running the fitting routine, model 1 takes much longer to fit than model 3. With the settings I used, runtimes were 331 versus 57 minutes. The wide priors made the fitting efficiency poor. But let’s see how it impacts the results.\nFirst, we explore priors and posteriors. They are easy to extract from the models using the extract.prior() and extract.samples() functions from rethinking.\n\n#get priors and posteriors for models 1 and 3\nm1prior <- extract.prior(fl[[1]]$fit, n = 1e4)\nm1post <- extract.samples(fl[[1]]$fit, n = 1e4)\n\nm3prior <- extract.prior(fl[[3]]$fit, n = 1e4)\nm3post <- extract.samples(fl[[3]]$fit, n = 1e4)\n\nNow we can plot the distributions. Note that for the individual-level parameters \\(a_0\\) and \\(b_0\\), the plots show the distribution across all individuals. The dashed lines show the priors, the solid the posteriors. Black is model 1, blue is model 3.\n\n#showing density plots for a0\nplot(density(m1prior$a0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$a0), lty=1)\nlines(density(m3prior$a0), col = \"blue\", lty=2)\nlines(density(m3post$a0), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b0\nplot(density(m1prior$b0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$b0), lty=1)\nlines(density(m3prior$b0), col = \"blue\", lty=2)\nlines(density(m3post$b0), col = \"blue\", lty=1)\n\n\n\n#showing density plots for a1\nplot(density(m1prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$a1), lty=1)\nlines(density(m3prior$a1), col = \"blue\", lty=2)\nlines(density(m3post$a1), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b1\nplot(density(m1prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$b1), lty=1)\nlines(density(m3prior$b1), col = \"blue\", lty=2)\nlines(density(m3post$b1), col = \"blue\", lty=1)\n\n\n\n\nWe set up the models to have wider \\(a_0\\) and \\(b_0\\) priors for model 1, and the same priors for the \\(a_1\\) and \\(b_1\\) parameters. The dashed lines in the figures show that. Looking at the posteriors, we find that changing the priors has an impact, especially for \\(a_1\\) and \\(b_1\\). Not only does model 3 lead to more peaked posteriors, they are also not centered at the same values, especially for \\(b_1\\). I don’t think that’s a good sign. We want the data to dominate the results, the priors should just be there to ensure the models explore the right parameter space efficiently and don’t do anything crazy. The fact that the same model, started with different priors, leads to different posterior distributions is in my opinion concerning. It could be that with more sampling, the posteriors might get closer. Or it might suggest that we are overfitting and have non-identifiability problems here.\nOne way to check that further is to look at potential correlations between parameter posterior distributions, e.g., using a pairs() plot as shown above. Here are such plots for the parameters associated with \\(\\alpha\\) for model 1. I only plot a few for each dose, otherwise the plots won’t be legible inside this html document. But you can try for yourself, if you make the plot large enough you can fit them all. You can also make plots for model 3 and for the \\(b\\) parameters, those look very similar.\n\n# all \"a\" parameters - too big to show\n#pairs(fl[[1]]$fit, pars = c(\"a0\",\"a1\"))\n# a few parameters for each dose\n#low dose\npairs(fl[[1]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[1]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[1]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\nRecall that we set up the model such that dose is non-zero for low and high dose, while for the intermediate dose it drops out of the model. What seems to happen is that if the dose effect, i.e., \\(a_1\\), is present, there is a strong correlation among that parameter and the individual-level parameters for that dose. That part makes some sense to me. Both \\(a_{0,i}\\) or \\(a_1\\) can change \\(\\alpha\\) and thus the trajectory. If one is low, the other might be high, and the reverse, leading to similarly good fits.\nBecause every \\(a_{0,i}\\) is correlated with \\(a_1\\) in this way, this also leads to correlations among the \\(a_{0,i}\\) values. I am surprised that this is an essentially perfect correlation. Maybe, if I thought about it harder and/or did the math, it would be clear that it needs to be that way. But I haven’t yet, so for now I’m just taking it as given 😁. Overall, this is another sign of that we might be overfitting and have non-identifiability problems, i.e. combinations for different values of \\(a_{0,i}\\) and \\(a_1\\) can lead to more or less the same results (everything I write here of course also holds for the \\(b_{0,i}\\) and \\(b_1\\) parameters).\nLet’s move on and now look at the posterior distributions in numerical form. For that, I use the precis function from rethinking. Instead of printing all the \\(N\\) different values of \\(a_{0,i}\\) and \\(b_{0,i}\\), I compute their means. If you want to see them all, change to depth=2 in the precis function.\n\n# Model 1\na0mean = mean(precis(fl[[1]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[1]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[1]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n       mean    sd  5.5% 94.5% n_eff Rhat4\na1     0.22 0.736 -0.96  1.40  2600     1\nb1    -0.21 0.745 -1.40  0.98  2384     1\nsigma  1.06 0.052  0.98  1.15 16450     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.961711 1.005527\n\n# Model 3\na0mean = mean(precis(fl[[3]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[3]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[3]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n        mean    sd   5.5% 94.5% n_eff Rhat4\na1     0.141 0.110 -0.033 0.316  3005     1\nb1    -0.082 0.109 -0.256 0.093  2776     1\nsigma  1.062 0.052  0.985 1.148 15663     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.9757761 0.9810916\n\n\nThe models seem to have converged ok, based on Rhat values of 1. Some parameters sampled better than others, as can be seen by the varying n_eff values. I used 5 chains of 3000 post-warmup samples for each chain, so the actual samples are 15000. If n_eff is lower than that, it means the sampling was not efficient, more means it worked very well (see e.g. Statistical Rethinking why it’s possible to get more effective samples than actual samples.)\nWe find that estimates for \\(a_{0}\\), \\(b_0\\) and \\(\\sigma\\) are similar, \\(a_1\\) and \\(b_1\\) differ more.\nAgain, note that the only thing we changed between models 1 and 3 are to make the priors for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters tighter. It didn’t seem to impact estimates for those parameters, but it did impact the estimates for the posterior distributions of parameters \\(a_1\\) and \\(b_1\\). The numbers are consistent with the posterior distribution figures above."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Comparing model estimates with the truth",
    "text": "Comparing model estimates with the truth\nWe know the “truth” here, i.e., the actual values of the parameters which we used to created the simulated data. To generate the data, we used these parameter values: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1. We also said that our main scientific question is if there is a dose effect, i.e. non-zero \\(a_1\\) and \\(b_1\\).\nThe models find estimates of \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) that are close to what we used. The estimates for \\(a_1\\) and \\(b_1\\) are not that great. That’s especially true for model 1. With these models, we aren’t able to convincingly recover the parameters used to generate the data. I’m not sure if increasing the sampling (longer or more chains) would help. Both models, especially model 1, already took quite a while to run. Thus I’m not too keen to try it with even more samples. As we’ll see below, alternative models do better."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nNext, let’s look at models 2 and 2a. The estimates should be similar since the two models are conceptually pretty much the same.\n\n# Compare models 2 and 2a\n# first we compute the mean across individuals for model 2\na0mean = mean(precis(fl[[2]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[2]]$fit,depth=2,\"b0\")$mean)\n\n#rest of model 2\nprint(precis(fl[[2]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\nmu_a   2.992 0.0271  2.94  3.026    16   1.4\nmu_b   0.999 0.0236  0.96  1.029    12   1.4\na1     0.095 0.0097  0.08  0.111   101   1.1\nb1    -0.097 0.0082 -0.11 -0.083   147   1.1\nsigma  6.867 0.2838  6.46  7.334   244   1.0\n\nprint(c(a0mean,b0mean))\n\n[1] 2.991509 0.998633\n\n#model 2a\nprint(precis(fl[[5]]$fit,depth=1),digits = 2)\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\na0     2.920 0.0236  2.88  2.957  5343     1\nb0     0.938 0.0204  0.90  0.971  5651     1\na1     0.107 0.0109  0.09  0.125  6123     1\nb1    -0.098 0.0094 -0.11 -0.084  6787     1\nsigma  7.003 0.3207  6.51  7.536  7308     1\n\n\nThe first thing to note is that model 2 performs awfully, with Rhat values >1 and very low effective sample size n_eff. This indicates that this model doesn’t work well for the data. Whenever you see diagnostics like that, you should not take the estimated values seriously. However, let’s pretend for a moment that we can take them seriously. Here is what we find.\nFirst, both models produce similar estimates. Since model 2a is simpler and doesn’t have that strange feature of us enforcing a very tight distribution for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters, it actually samples much better, see the higher n_eff numbers. It also runs much faster, 1.3 minutes compared to 30 minutes for model 2.\nBoth models do a very poor job estimating \\(\\sigma\\). That’s because we don’t allow the models to have the flexibility needed to fit the data, so it has to account for any variation between its estimated mean trajectory and the real data by making \\(\\sigma\\) large.\nSince the models are more constrained compared to models 1 and 3, they produce estimates for \\(a_1\\) and \\(b_1\\) that are tighter. However, these estimates are over-confident. overall these models underfitting and are not good. We can for instance look at this using the compare function:\n\ncompare(fl[[1]]$fit,fl[[3]]$fit,fl[[2]]$fit,fl[[5]]$fit)\n\n                 WAIC       SE        dWAIC        dSE     pWAIC        weight\nfl[[1]]$fit  832.9455 23.39942   0.00000000         NA 43.718111  5.067138e-01\nfl[[3]]$fit  832.9992 23.42829   0.05371325  0.4274202 43.737196  4.932862e-01\nfl[[2]]$fit 1778.3776 44.18971 945.43210978 45.5782040  9.698475 2.551459e-206\nfl[[5]]$fit 1788.1450 45.28523 955.19949836 47.0533571 10.466546 1.931199e-208\n\n\nI’m not going to discuss things in detail (see Statistical Rethinking), but a lower WAIC means a model that fits best in the sense that it strikes a good balance between fitting the data while not overfitting. As you can see, models 1 and 3 perform very similarly and models 2 and 2a are much worse.\nThe larger WAIC indicates either strong overfitting or underfitting. In this case, it’s underfitting. The models are not flexible enough to capture the individual-level variation. You’ll see that clearly in the plots shown further below. If we did indeed not want to account for individual-level variation, we should go with a model that simply doesn’t include it, i.e. model 2a. The contrived model 2 with very narrow priors is just a bad model, and I’m really only exploring it here for demonstration purposes."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nNow we get to the models we really care about. When I set up the models, I suggested that model 4 was similar to models 1-3, but with priors adaptively chosen. That didn’t apply during data generation/simulation since in that step, we always need to manually choose values. But during the fitting/estimation, we should expect that model 4 chooses priors in a smart way, such that it is better than the models where we fixed the priors. Let’s see what model 4 produces. We also look at model 4a, which is exactly the same model, just rewritten to potentially make the numerical fitting routine more efficient.\nLet’s start with prior and posterior plots.\n\n#get priors and posteriors for models 4 and 4a\nm4prior <- extract.prior(fl[[4]]$fit, n = 1e4)\nm4post <- extract.samples(fl[[4]]$fit, n = 1e4)\n\nm4aprior <- extract.prior(fl[[6]]$fit, n = 1e4)\nm4apost <- extract.samples(fl[[6]]$fit, n = 1e4)\n\nAs before, the dashed lines show the priors, the solid the posteriors. Black is model 4, blue is model 4a.\n\n#showing density plots for a0\nplot(density(m4prior$mu_a), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_a), lty=1)\nlines(density(m4aprior$mu_a), col = \"blue\", lty=2)\nlines(density(m4apost$mu_a), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b0\nplot(density(m4prior$mu_b), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_b), lty=1)\nlines(density(m4aprior$mu_b), col = \"blue\", lty=2)\nlines(density(m4apost$mu_b), col = \"blue\", lty=1)\n\n\n\n#showing density plots for a1\nplot(density(m4prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$a1), lty=1)\nlines(density(m4aprior$a1), col = \"blue\", lty=2)\nlines(density(m4apost$a1), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b1\nplot(density(m4prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$b1), lty=1)\nlines(density(m4aprior$b1), col = \"blue\", lty=2)\nlines(density(m4apost$b1), col = \"blue\", lty=1)\n\n\n\n\nAs you can see, up to numerical sampling variability, the results for models 4 and 4a are pretty much the same. That should be expected, since they are the same model, just reformulated for potential efficiency. Also, the posterior distributions are much narrower than the priors. I think that’s a good sign as well, it indicates the data mostly informed the posterior distributions, the priors just helped to keep things efficient.\nWe can also explore pair plots again, showing them here for model 4.\n\n# a few parameters for each dose\n#low dose\npairs(fl[[4]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[4]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[4]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n# mean of a0 prior\npairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#saving one plot so I can use as featured image\npng(filename = \"featured.png\", width = 6, height = 6, units = \"in\", res = 300)\npairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\ndev.off()\n\npng \n  2 \n\n\nWe still see the same issue with correlations among the parameters for dose levels where \\(a_1\\) is acting, though the correlations are not as extreme. They are also minor between the overall estimates for the mean of the \\(a_0\\) and \\(b_0\\) parameters and \\(a_1\\) and \\(b_1\\). I interpret this to mean that the adaptive sampling helped somewhat with the identifiability and overfitting problem, though it seems to not fully resolve it. The fact that we gave each individual their own \\(a_{0,i}\\) and \\(b_{0,i}\\) values allows those parameters to still “absorb” some of the dose-dependent signal in \\(a_1\\) and \\(b_1\\).\nWe can also again look at the numerical outputs from the precis function.\n\n# model 4\nprint(precis(fl[[4]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.020  2.956  3.018 14437     1\nmu_b     0.986 0.025  0.946  1.026 14777     1\nsigma_a  0.093 0.016  0.071  0.121 10722     1\nsigma_b  0.119 0.020  0.092  0.153 12199     1\na1       0.086 0.010  0.069  0.102  2170     1\nb1      -0.107 0.013 -0.128 -0.085  2230     1\nsigma    1.062 0.052  0.983  1.148 13477     1\n\n# model 4a\nprint(precis(fl[[6]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.019  2.956  3.017  3803     1\nmu_b     0.986 0.025  0.946  1.026  3259     1\nsigma_a  0.093 0.016  0.072  0.121  4376     1\nsigma_b  0.119 0.020  0.092  0.153  4285     1\na1       0.086 0.010  0.069  0.103  4267     1\nb1      -0.106 0.013 -0.126 -0.084  3861     1\nsigma    1.062 0.051  0.985  1.148 10624     1\n\n\nThe numerics confirm that the two models lead to essentially the same results. The values for n_eff differ between models, though neither model is consistently larger. This suggests that each model formulation had advantages in sampling for some of the parameters.\nIn terms of run times, there wasn’t much difference, with 7 minutes for model 4 versus 10 minutes for model 4a (much better than models 1-3).\nIf we compare the parameter estimates with the true values and those found for models 1 and 3 above, we find that again the true \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) are estimated fairly well. Estimates for \\(a_1\\) and \\(b_1\\) are now also pretty good, and the credible intervals are less wide.\nNow let’s briefly run the compare function too and include model 3 as well. In addition to using WAIC for comparison, I’m also including PSIS. Read about it in the Statistical Rethinking book. One advantage of PSIS is that it gives warnings if the estimates might not be reliable. You see that this happens here.\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = WAIC)\n\n                WAIC       SE     dWAIC       dSE    pWAIC    weight\nfl[[4]]$fit 831.0751 23.40197 0.0000000        NA 42.58290 0.4659494\nfl[[6]]$fit 831.6133 23.39119 0.5382072 0.3058545 42.76359 0.3560152\nfl[[3]]$fit 832.9992 23.42829 1.9241902 2.5641934 43.73720 0.1780353\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = PSIS)\n\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n\n\n                PSIS       SE     dPSIS       dSE    pPSIS    weight\nfl[[4]]$fit 839.5114 24.33096 0.0000000        NA 46.80107 0.4616132\nfl[[6]]$fit 839.8745 24.30905 0.3630563 0.5782962 46.89419 0.3849830\nfl[[3]]$fit 841.7147 24.33321 2.2033083 2.7044971 48.09493 0.1534037\n\n\nWe do find that model 4/4a performs a bit better, but not by much. Note that model 4/4a has more actual parameters, but the effective parameters (which is described by pWAIC) is a bit smaller.\nOverall, this suggests that the adaptive pooling approach helped to estimate results more precisely and efficiently and is the best of the models."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nAs stated above, due to a typo in my code, the above models, including 4/4a, initially produced estimates for \\(a_1\\) and \\(b_1\\) that were not close to those (I thought I) used to generate the data. Based on the pair plots, I suspected non-identifiability issues and wanted to explore what would happen if I removed the dose.\nIf we now look at the pairs plots, maybe not surprisingly, the correlations between individual \\(a_0\\) parameters are gone.\n\n# a few parameters for each dose\n#low dose\npairs(fl[[7]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a0[5]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[7]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a0[12]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[7]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a0[20]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\nThe model estimates the parameters reasonably well\n\na0mean = mean(precis(fl[[7]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[7]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[7]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n      mean    sd 5.5% 94.5% n_eff Rhat4\nsigma  1.1 0.052 0.98   1.1 16614     1\n\nprint(c(a0mean,b0mean))\n\n[1] 3.0031331 0.9655931\n\n\nIt doesn’t seem quite as good as the previous models.\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[7]]$fit)\n\n                WAIC       SE    dWAIC      dSE    pWAIC    weight\nfl[[4]]$fit 831.0751 23.40197 0.000000       NA 42.58290 0.5705560\nfl[[3]]$fit 832.9992 23.42829 1.924190 2.564193 43.73720 0.2180046\nfl[[7]]$fit 833.0604 23.39781 1.985346 2.445539 43.82551 0.2114394\n\n\nAnd of course the main problem with this model: It can’t answer any question about the role of dose, since we removed that component from the model! So while ok to explore, scientifically not useful since it can’t help us address the question we want to answer regarding the potential impact of dose."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\n\nplot(plotlist[[1]])\n\n\n\nplot(plotlist[[3]])\n\n\n\n\nDespite differences in estimates for the dose related parameters, the predicted outcomes of the models are very similar. In fact, it’s hard to tell any difference by just looking at the plots (but they are slightly different, I checked). Thus, despite the inability of these models to provide precises estimates of all the parameter values, the predictions/outcomes are fine, they fit the data well."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nFor models 2 and 2a, recall that the only variation is for dose, we didn’t allow variation among individuals. That’s reflected in the plots. The credible intervals based on parameters are tight, but because the variability, \\(\\sigma\\), had to account for all the differences, the prediction intervals are very wide.\n\nplot(plotlist[[2]])\n\n\n\nplot(plotlist[[5]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nThese models look good again, and very similar to models 1 and 3.\n\nplot(plotlist[[4]])\n\n\n\nplot(plotlist[[6]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nThe model fits look fine, suggesting that one parameter for each individual is enough to capture the data. That’s not surprising. However, this of course does not allow us to ask and answer any scientific questions about the role of dose.\n\nplot(plotlist[[7]])\n\n\n\n\nSo overall, the figures make sense It seems that if we want to do prediction, all models that include individual variability are fine, models 2/2a are not great. If we wanted to estimate the model parameters, specifically \\(a_1\\) and \\(b_1\\), models 1 and 3 and of course 5 don’t work. In that case, model 2/2a works ok. I consider model 4/4a the best one overall.\nFor another example and more discussion of estimation versus prediction, see e.g. Section 6.1. in Statistical Rethinking, as well as 9.5.4 (all referring to the 2nd edition of the book)."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "",
    "text": "This is part 3 of a tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.\nI assume you’ve read both part 1, and part 2 otherwise this post won’t make much sense."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 1",
    "text": "Model 1\nThis is one of the models with individual-level and dose-level effects, all priors fixed. This model has \\(2N+2+1\\) parameters. \\(N\\) each for the individual-level intercepts for \\(\\alpha\\) and \\(\\beta\\) (the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters), the two dose-level parameters \\(a_1\\) and \\(b_1\\), and 1 overall deviation, \\(\\sigma\\) for the outcome distribution.\n\n#no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1eqs <- bf(  #main equation for time-series trajectory\n          outcome ~  exp(alpha)*log(time) - exp(beta)*time,\n          #equations for alpha and beta\n          alpha ~ 0 + id + dose_adj,\n          beta  ~ 0 + id + dose_adj,\n          nl = TRUE)\n\nm1priors <- c(#assign priors to all coefficients related to both id and dose_adj for alpha and beta\n              prior(normal(2, 10),  class = \"b\",  nlpar = \"alpha\"),\n              prior(normal(0.5, 10),  class = \"b\",  nlpar = \"beta\"),\n              #change the dose_adj priors to something different than the id priors\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\") )\n\nNotice how this notation in brms looks quite a bit different from the mathematical equations or the ulam implementation. That’s a part I don’t particularly like about brms, the very condensed formula notation. It takes time getting used to and it always requires extra checking to ensure the model implemented in code corresponds to the mathematical model. One can check by looking at the priors and make sure they look as expected. We’ll do that below after we fit."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the easiest model, with only population level effects for intercept and dose, so only 2+2+1 parameters.\n\n#full-pooling model\n#2+2+1 parameters\nm2aeqs <- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\n\nm2apriors <- c(prior(normal(2, 2),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 2),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors.\n\n#same as model 1 but regularizing priors\nm3eqs <- m1eqs\n\nm3priors <- c(#assign priors to all coefficients related to id and dose_adj for alpha and beta\n  prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\"),\n  prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\"),\n  #change the dose_adj priors to something different than the id priors\n  prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n  prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n  prior(cauchy(0,1), class = \"sigma\") )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is the adaptive-pooling multi-level model where priors are estimated. Here we have for each main parameter (\\(\\alpha\\) and \\(\\beta\\)) an overall mean and standard deviation, and N individual intercepts, so 2 times 1+1+N. And of course we still have the 2 dose-related parameters and the overall standard deviation, so a total of 2*(1+1+N)+2+1 parameters.\n\n#adaptive prior, partial-pooling model\nm4eqs <- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~  (1|id) + dose_adj,\n  beta  ~  (1|id) + dose_adj,\n  nl = TRUE)\n\nm4priors <- c(prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"alpha\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"beta\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Combine models",
    "text": "Combine models\nTo make our lives easier below, we combine all models and priors into lists.\n\n#stick all models into a list\nmodellist = list(m1=m1eqs,m2a=m2aeqs,m3=m3eqs,m4=m4eqs)\n#also make list for priors\npriorlist = list(m1priors=m1priors,m2apriors=m2apriors,m3priors=m3priors,m4priors=m4priors)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Fitting setup",
    "text": "Fitting setup\nWe define some general values for the fitting. Since the starting values depend on number of chains, we need to do this setup first.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 6000\niter = warmup + floor(warmup/2)\nmax_td = 18 #tree depth\nadapt_delta = 0.9999\nchains = 5\ncores  = chains\nseed = 1234"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Setting starting values",
    "text": "Setting starting values\nWe’ll again set starting values, as we did for ulam/rethinking. Note that brms needs them in a somewhat different form, namely as list of lists for each model, one list for each chain.\nI set different values for each chain, so I can check that each chain ends up at the same posterior. This is inspired by this post by Solomon Kurz, though I keep it simpler and just use the jitter function.\nNote that this approach not only jitters (adds noise/variation) between chains, but also between the individual-level parameters for each chain. That’s fine for our purpose, it might even be beneficial.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.5 , b1 = -0.5, sigma = 1)\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.5 , b1 = 0.5, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 0, sigma_b = 1, a1 = 0.5 , b1 = -0.5, sigma = 1)\n#put different starting values in list\n#need to be in same order as models below\n#one list for each chain, thus a 3-leveled list structure\n#for each chain, we add jitter so they start at different values\nstartlist = list( rep(list(lapply(startm1,jitter,10)),chains),\n                  rep(list(lapply(startm2a,jitter,10)),chains),\n                  rep(list(lapply(startm3,jitter,10)),chains),\n                  rep(list(lapply(startm4,jitter,10)),chains)\n                  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll use the same strategy to loop though all models and fit them. The fitting code looks very similar to the previous one for rethinking/ulam, only now the fitting is done calling the brm function.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  fl[[n]]$fit <- brm(formula = modellist[[n]],\n                   data = fitdat,\n                   family = gaussian(),\n                   prior = priorlist[[n]],\n                   init = startlist[[n]],\n                   control=list(adapt_delta=adapt_delta, max_treedepth = max_td),\n                   sample_prior = TRUE,\n                   chains=chains, cores = cores,\n                   warmup = warmup, iter = iter,\n                   seed = seed,\n                   backend = \"cmdstanr\"\n  )# end brm statement\n\n  tend=proc.time(); #capture current time\n  tdiff=tend-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n}\n# saving the results so we can use them later\nfilepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"brmsfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)\n\nYou’ll likely find that model 1 takes the longest, the other ones run faster. You can check the runtime for each model by looking at fl[[n]]$runtime. It’s useful to first run with few iterations (100s instead of 1000s), make sure everything works in principle, then do a “final” long run with longer chains."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nLet’s explore those two models first. Recall that they are the same, apart from the prior definitions. As previously, the wider priors for model 1 make it less efficient. With the settings I used, run times were 417 minutes for model 1 versus 61 minutes for model 3.\nLet’s see if the priors impact the results, i.e. the posterior distributions. We can actually do that by looking briefly at the summaries for both fits.\n\n#save some typing\nfit1 <- fl[[1]]$fit\nfit3 <- fl[[3]]$fit\nsummary(fit1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.50      1.67     0.26     6.80 1.00     2840     4734\nalpha_id2          3.46      1.67     0.21     6.75 1.00     2839     4794\nalpha_id3          3.26      1.67     0.02     6.56 1.00     2839     4760\nalpha_id4          3.19      1.67    -0.05     6.49 1.00     2841     4760\nalpha_id5          3.24      1.67    -0.01     6.53 1.00     2839     4760\nalpha_id6          3.33      1.67     0.08     6.62 1.00     2839     4760\nalpha_id7          3.28      1.67     0.03     6.58 1.00     2841     4796\nalpha_id8          2.98      0.02     2.95     3.01 1.00    17885    10342\nalpha_id9          2.91      0.02     2.88     2.94 1.00    17315    10942\nalpha_id10         2.98      0.02     2.95     3.01 1.00    17656    10966\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18085    10133\nalpha_id12         2.84      0.02     2.81     2.88 1.00    17692    11631\nalpha_id13         2.97      0.02     2.94     3.00 1.00    18451    10345\nalpha_id14         3.09      0.01     3.06     3.12 1.00    18387    10003\nalpha_id15         2.95      0.02     2.91     2.98 1.00    17682    10963\nalpha_id16         2.77      1.67    -0.52     6.01 1.00     2839     4781\nalpha_id17         2.54      1.67    -0.76     5.79 1.00     2840     4750\nalpha_id18         2.73      1.67    -0.57     5.97 1.00     2839     4798\nalpha_id19         2.76      1.67    -0.53     6.01 1.00     2839     4820\nalpha_id20         2.73      1.67    -0.56     5.98 1.00     2840     4771\nalpha_id21         2.71      1.67    -0.59     5.96 1.00     2840     4751\nalpha_id22         2.66      1.67    -0.64     5.91 1.00     2839     4807\nalpha_id23         2.65      1.67    -0.64     5.90 1.00     2840     4764\nalpha_id24         2.59      1.67    -0.70     5.84 1.00     2838     4762\nalpha_dose_adj     0.22      0.73    -1.19     1.65 1.00     2839     4785\nbeta_id1           0.75      1.71    -2.66     4.10 1.00     2420     4179\nbeta_id2           0.65      1.71    -2.76     4.01 1.00     2420     4181\nbeta_id3           0.70      1.71    -2.72     4.04 1.00     2419     4158\nbeta_id4           0.71      1.71    -2.70     4.06 1.00     2419     4155\nbeta_id5           0.93      1.71    -2.48     4.28 1.00     2418     4167\nbeta_id6           0.68      1.71    -2.73     4.03 1.00     2419     4175\nbeta_id7           0.77      1.71    -2.64     4.13 1.00     2419     4155\nbeta_id8           1.01      0.01     0.99     1.04 1.00    16977    10323\nbeta_id9           0.91      0.02     0.88     0.94 1.00    17374    11382\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18009    10155\nbeta_id11          1.15      0.01     1.13     1.18 1.00    18260    10293\nbeta_id12          1.05      0.01     1.02     1.07 1.00    17891    11580\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18998    10824\nbeta_id14          0.95      0.01     0.92     0.98 1.00    18321    10396\nbeta_id15          0.79      0.02     0.75     0.82 1.00    17550    11046\nbeta_id16          1.36      1.71    -1.99     4.77 1.00     2418     4208\nbeta_id17          1.08      1.71    -2.27     4.49 1.00     2419     4159\nbeta_id18          1.36      1.71    -2.00     4.77 1.00     2421     4150\nbeta_id19          1.44      1.71    -1.92     4.85 1.00     2417     4173\nbeta_id20          1.09      1.71    -2.25     4.50 1.00     2420     4083\nbeta_id21          1.31      1.71    -2.04     4.73 1.00     2420     4118\nbeta_id22          1.24      1.71    -2.10     4.65 1.00     2421     4122\nbeta_id23          1.12      1.71    -2.23     4.53 1.00     2419     4157\nbeta_id24          1.09      1.71    -2.26     4.51 1.00     2419     4209\nbeta_dose_adj     -0.21      0.74    -1.69     1.24 1.00     2419     4163\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    16342    11307\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.31      0.25     2.83     3.80 1.00     2443     4189\nalpha_id2          3.27      0.25     2.79     3.75 1.00     2418     4115\nalpha_id3          3.07      0.25     2.59     3.55 1.00     2447     4120\nalpha_id4          3.00      0.25     2.52     3.48 1.00     2433     4196\nalpha_id5          3.05      0.25     2.56     3.53 1.00     2437     4206\nalpha_id6          3.14      0.25     2.66     3.62 1.00     2465     4179\nalpha_id7          3.09      0.25     2.61     3.57 1.00     2438     4311\nalpha_id8          2.98      0.02     2.95     3.01 1.00    18493    10858\nalpha_id9          2.91      0.02     2.87     2.94 1.00    18973    11242\nalpha_id10         2.98      0.02     2.95     3.01 1.00    18479    10529\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18804    10454\nalpha_id12         2.84      0.02     2.81     2.87 1.00    18873    11236\nalpha_id13         2.97      0.02     2.94     3.00 1.00    19043    11438\nalpha_id14         3.09      0.01     3.06     3.12 1.00    19247    10690\nalpha_id15         2.95      0.02     2.91     2.98 1.00    19114    12099\nalpha_id16         2.96      0.25     2.48     3.44 1.00     2432     4256\nalpha_id17         2.73      0.25     2.25     3.21 1.00     2418     4214\nalpha_id18         2.92      0.25     2.43     3.39 1.00     2433     4335\nalpha_id19         2.95      0.25     2.47     3.43 1.00     2438     4308\nalpha_id20         2.92      0.25     2.43     3.40 1.00     2427     4161\nalpha_id21         2.90      0.25     2.41     3.37 1.00     2418     4311\nalpha_id22         2.85      0.25     2.36     3.33 1.00     2439     4182\nalpha_id23         2.84      0.25     2.36     3.32 1.00     2431     4213\nalpha_id24         2.78      0.25     2.30     3.26 1.00     2438     4170\nalpha_dose_adj     0.14      0.11    -0.07     0.35 1.00     2426     4307\nbeta_id1           1.05      0.24     0.58     1.53 1.00     2953     5165\nbeta_id2           0.96      0.24     0.49     1.43 1.00     2937     5242\nbeta_id3           1.00      0.24     0.53     1.47 1.00     2944     5168\nbeta_id4           1.01      0.24     0.54     1.49 1.00     2937     5142\nbeta_id5           1.24      0.24     0.76     1.71 1.00     2939     5218\nbeta_id6           0.99      0.24     0.52     1.46 1.00     2946     5117\nbeta_id7           1.08      0.24     0.60     1.55 1.00     2941     5223\nbeta_id8           1.01      0.01     0.99     1.04 1.00    18029    10844\nbeta_id9           0.91      0.02     0.88     0.93 1.00    18953    11005\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18500    10509\nbeta_id11          1.15      0.01     1.13     1.17 1.00    18599    10418\nbeta_id12          1.05      0.01     1.02     1.07 1.00    19002    10853\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18714    11040\nbeta_id14          0.95      0.01     0.92     0.98 1.00    19168    10286\nbeta_id15          0.79      0.02     0.75     0.82 1.00    18771    11344\nbeta_id16          1.06      0.24     0.58     1.53 1.00     2943     5165\nbeta_id17          0.78      0.25     0.30     1.25 1.00     2941     5155\nbeta_id18          1.05      0.24     0.58     1.53 1.00     2939     5207\nbeta_id19          1.14      0.24     0.66     1.61 1.00     2951     5251\nbeta_id20          0.79      0.25     0.31     1.26 1.00     2962     5253\nbeta_id21          1.00      0.24     0.52     1.47 1.00     2944     5222\nbeta_id22          0.94      0.24     0.46     1.41 1.00     2951     5207\nbeta_id23          0.82      0.25     0.34     1.29 1.00     2943     5263\nbeta_id24          0.79      0.25     0.31     1.26 1.00     2957     5311\nbeta_dose_adj     -0.08      0.11    -0.29     0.13 1.00     2939     5258\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    15961    10880\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote the different naming of the parameters in brms. It’s unfortunately not possible (as far as I know) to get the names match the mathematical model. The parameters that have dose in their names are the ones we called \\(a_1\\) and \\(b_1\\) in our models. The many _id parameters are our previous \\(a_0\\) and \\(b_0\\) parameters. Conceptually, the latter are on the individual level. But we don’t have a nested/multi-level structure here, which seems to lead brms to consider every parameter on the same level, and thus labeling them all population level.\nNow, let’s look at priors and posteriors somewhat more. First, we extract priors and posteriors.\n\n#get priors and posteriors for models 1 and 3\nm1prior <- prior_draws(fit1)\nm1post <- as_draws_df(fit1)\nm3prior <- prior_draws(fit3)\nm3post <- as_draws_df(fit3)\n\nNow we can plot the distributions. I’m focusing on the \\(a_1\\) and \\(b_1\\) parameters since those are of more interest, and because I couldn’t figure out quickly how to get out and process all the individual level \\(a_0\\) and \\(b_0\\) parameters from brms 😁.\n\n#showing density plots for a1\n\n#make a data frame and get it in shape for ggplot\na1df <- data.frame(m1_prior = m1prior$b_alpha_dose_adj,\n                   m1_post = m1post$b_alpha_dose_adj,\n                   m3_prior = m3prior$b_alpha_dose_adj,\n                   m3_post =  m3post$b_alpha_dose_adj) %>%\n        pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 <- a1df %>%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot(p1)\n\n\n\n#save for display on post\nggsave(file = paste0(\"featured.png\"), p1, dpi = 300, units = \"in\", width = 6, height = 6)\n\n\n#showing density plots for b1\nb1df <- data.frame(m1_prior = m1prior$b_beta_dose_adj,\n                   m1_post = m1post$b_beta_dose_adj,\n                   m3_prior = m3prior$b_beta_dose_adj,\n                   m3_post =  m3post$b_beta_dose_adj) %>%\n  pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n\np2 <- b1df %>%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\nplot(p2)\n\n\n\n\nAs before, the priors for the \\(a_1\\) and \\(b_1\\) parameters are the same. We only changed the \\(a_0\\) and \\(b_0\\) priors, but that change leads to different posteriors for \\(a_1\\) and \\(b_1\\). It’s basically the same result we found with ulam/rethinking.\nIt would be surprising if we did NOT find the same correlation structure again in the parameters, let’s check it.\n\n# a few parameters for each dose\n#low dose\npairs(fit1, variable = variables(fit1)[c(1:4,25)])\n\n\n\n#medium dose\npairs(fit1, variable = variables(fit1)[c(8:11,25)])\n\n\n\n#high dose\npairs(fit1, variable = variables(fit1)[c(16:19,25)])\n\n\n\n\nApart from the unfortunate naming of parameters in brms, these are the same plots as we made for the ulam fits and show the same patterns.\nLet’s look at the posteriors in numerical form.\n\n# model 1 first\nfit1pars = posterior::summarize_draws(m1post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n\n#only entries for the a0 parameters\na0post <- m1post %>% dplyr::select(starts_with('b_alpha_id'))\nfit1a0mean <- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post <- m1post %>% dplyr::select(starts_with('b_beta_id'))\nfit1b0mean <- mean(colMeans(b0post))\nfit1otherpars <- fit1pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit1otherpars)\n\n# A tibble: 4 × 8\n  variable             mean     sd       q5     q95  rhat ess_bulk ess_tail\n  <chr>               <dbl>  <dbl>    <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_dose_adj    0.224 0.726    -0.972    1.44  1.00    2839.    4785.\n2 b_beta_dose_adj    -0.212 0.744    -1.44     1.01  1.00    2419.    4163.\n3 sigma               1.06  0.0514    0.981    1.15  1.00   16342.   11307.\n4 lp__             -549.    5.58   -559.    -540.    1.00    4974.    8286.\n\nprint(c(fit1a0mean,fit1b0mean))\n\n[1] 2.960140 1.006334\n\n# repeat for model 3\nfit3pars = posterior::summarize_draws(m3post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n#only entries for the a0 parameters\na0post <- m3post %>% dplyr::select(starts_with('b_alpha_id'))\nfit3a0mean <- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post <- m3post %>% dplyr::select(starts_with('b_beta_id'))\nfit3b0mean <- mean(colMeans(b0post))\nfit3otherpars <- fit3pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit3otherpars)\n\n# A tibble: 4 × 8\n  variable              mean     sd        q5       q95  rhat ess_bulk ess_tail\n  <chr>                <dbl>  <dbl>     <dbl>     <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_dose_adj    0.142  0.107    -0.0334    0.316   1.00    2426.    4307.\n2 b_beta_dose_adj    -0.0811 0.106    -0.254     0.0921  1.00    2939.    5258.\n3 sigma               1.06   0.0515    0.982     1.15    1.00   15961.   10880.\n4 lp__             -453.     5.60   -463.     -444.      1.00    4605.    7829.\n\nprint(c(fit3a0mean,fit3b0mean))\n\n[1] 2.9756367 0.9808696\n\n\nAgain, model 1 seems worse, with higher uncertainty intervals for the \\(a_1\\) and \\(b_1\\) parameters and the mean further away from the true value.\nWe can also compare the models as we did for rethinking using these lines of code:\n\nfit13comp <- loo_compare(add_criterion(fit1,\"waic\"),\n            add_criterion(fit3,\"waic\"),\n            criterion = \"waic\")\n\nWarning: \n30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: \n29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nprint(fit13comp, simplify = FALSE)\n\n                            elpd_diff se_diff elpd_waic se_elpd_waic p_waic\nadd_criterion(fit1, \"waic\")    0.0       0.0  -416.3      11.7         43.5\nadd_criterion(fit3, \"waic\")    0.0       0.2  -416.3      11.7         43.5\n                            se_p_waic waic   se_waic\nadd_criterion(fit1, \"waic\")    4.3     832.5   23.4 \nadd_criterion(fit3, \"waic\")    4.3     832.6   23.4 \n\n\nModel performance is similar between models. The WAIC values are also close to those reported by rethinking."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparison with the truth and ulam",
    "text": "Comparison with the truth and ulam\nThe values used to generate the data are: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1.\nSince the models are the same as those we previously fit with ulam, only a different R package is used to run them, we should expect very similar results. This is the case. We find that as for the ulam fits, the estimates for \\(a_0\\), \\(b_0\\) and \\(\\sigma\\) are similar to the values used the generate the data, but estimates for \\(a_1\\) and \\(b_1\\) are not that great. The agreement with ulam is good, because we should expect that if we fit the same models, results should - up to numerical/sampling differences - be the same, no matter what software implementation we use. It also suggests that we did things right - or made the same mistake in both implementations! 😁.\nWhy the WAIC estimates are different is currently not clear to me. It could be that the 2 packages use different definitions/ways to compute it. Or something more fundamental is still different. I’m not sure."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the model with only population-level estimates. We already explored it somewhat above when we looked at traceplots and trankplots and the like. Here is just another quick table for the posteriors.\n\nm2post <- as_draws_df(fit2)\nfit2pars = posterior::summarize_draws(m2post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit2otherpars <- fit2pars %>% dplyr::filter(!grepl('prior',variable))\nprint(fit2otherpars)\n\n# A tibble: 6 × 8\n  variable               mean      sd        q5       q95  rhat ess_bulk ess_t…¹\n  <chr>                 <dbl>   <dbl>     <dbl>     <dbl> <dbl>    <dbl>   <dbl>\n1 b_alpha_Intercept    2.98   0.0211     2.95      3.02    1.00    6244.   6691.\n2 b_alpha_dose_adj     0.0960 0.00967    0.0802    0.112   1.00    6569.   7301.\n3 b_beta_Intercept     0.992  0.0188     0.961     1.02    1.00    6387.   6724.\n4 b_beta_dose_adj     -0.0971 0.00862   -0.111    -0.0829  1.00    6947.   7786.\n5 sigma                6.88   0.302      6.39      7.39    1.00    8391.   7850.\n6 lp__              -892.     1.59    -895.     -890.      1.00    4964.   7039.\n# … with abbreviated variable name ¹​ess_tail\n\n\nThe parameters that have _Intercept in their name are what we called \\(\\mu_a\\) and \\(\\mu_b\\), the ones containing _dose are our \\(a_1\\) and \\(b_1\\). We find pretty much the same results we found using ulam. Specifically, the main parameters are estimated well, but because the model is not very flexible, the estimate for \\(\\sigma\\) is much larger, since it needs to account for all the individual-level variation we ommitted from the model itself."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is what I consider the most interesting and conceptually best model. It performed best in the ulam fits. Let’s see how it looks here. It is worth pointing out that this model ran much faster compared to models 1 and 3, it only took 10.5518333 minutes.\nWe’ll start with the summary for the model.\n\nfit4 <- fl[[4]]$fit\nm4prior <- prior_draws(fit4)\nm4post <- as_draws_df(fit4)\nsummary(fit4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ (1 | id) + dose_adj\n         beta ~ (1 | id) + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nGroup-Level Effects: \n~id (Number of levels: 24) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(alpha_Intercept)     0.09      0.02     0.07     0.13 1.00     3685     6514\nsd(beta_Intercept)      0.12      0.02     0.09     0.16 1.00     4048     5853\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept     2.99      0.02     2.95     3.03 1.00     3771     5404\nalpha_dose_adj      0.09      0.01     0.07     0.11 1.00     3979     5040\nbeta_Intercept      0.99      0.02     0.94     1.03 1.00     3486     5134\nbeta_dose_adj      -0.11      0.01    -0.13    -0.08 1.00     3855     5732\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    10136    10314\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNext, the prior/posterior plots. To ensure one can see the priors, I’m cutting off the y-axis at 10, that’s why the posteriors look a bit weird. They do infected extend and peak like the distributions shown for models 1 and 3.\n\n#showing density plots for a1 and b1\n#make a data frame and get it in shape for ggplot\nm4df <- data.frame(a1_prior = m4prior$b_alpha_dose_adj,\n                   a1_post = m4post$b_alpha_dose_adj,\n                   b1_prior = m4prior$b_beta_dose_adj,\n                   b1_post = m4post$b_beta_dose_adj) %>%\n  pivot_longer(cols = everything(), names_to = c(\"parameter\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 <- m4df %>%\n  ggplot() +\n  ylim(0, 10) + xlim(-2, 2) +\n  geom_density(aes(x = value, color = parameter, linetype = type), adjust = 10, size = 1) +\n  ggtitle('model 4, parameters a1 and b1') +\n  theme_minimal()\nplot(p1)\n\n\n\n\nNumerical output for the posterior:\n\nfit4pars = posterior::summarize_draws(m4post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit4otherpars <- fit4pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable)) %>%\n  dplyr::filter(!grepl('z_',variable))\n\nprint(fit4otherpars)\n\n# A tibble: 6 × 8\n  variable               mean     sd        q5       q95  rhat ess_bulk ess_tail\n  <chr>                 <dbl>  <dbl>     <dbl>     <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_Intercept    2.99   0.0197    2.95      3.02    1.00    3771.    5404.\n2 b_alpha_dose_adj     0.0861 0.0106    0.0688    0.104   1.00    3979.    5040.\n3 b_beta_Intercept     0.987  0.0247    0.946     1.03    1.00    3486.    5134.\n4 b_beta_dose_adj     -0.106  0.0131   -0.127    -0.0844  1.00    3855.    5732.\n5 sigma                1.06   0.0517    0.981     1.15    1.00   10136.   10314.\n6 lp__              -468.     7.47   -481.     -457.      1.00    2720.    4987.\n\n\nThese estimates look good, close to the truth.\nFinishing with the pairs lots:\n\n# a few parameters for each dose\n#low dose\npairs(fit4, variable = variables(fit4)[c(1:4,25)])\n\n\n\n#medium dose\npairs(fit4, variable = variables(fit4)[c(8:11,25)])\n\n\n\n#high dose\npairs(fit4, variable = variables(fit4)[c(16:19,25)])\n\n\n\n\nThe strong correlations between parameters are reduced, the same we say with the ulam models.\nAs was the case for the ulam fits, model 4 seems to perform overall best."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparing all models",
    "text": "Comparing all models\nWe can repeat the model comparison we did above, now including all 4 models. I’m looking now at both WAIC and LOO (leave one out). Note the various warning messages. We got that as well when we computed PSIS (which is similar to LOO) with rethinking.\n\nfit1a <- add_criterion(fit1,c(\"waic\",\"loo\"))\n\nWarning: \n30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 6 observations with a pareto_k > 0.7 in model 'fit1'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\nfit2a <- add_criterion(fit2,c(\"waic\",\"loo\"))\n\nWarning: \n5 (1.9%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nfit3a <- add_criterion(fit3,c(\"waic\",\"loo\"))\n\nWarning: \n29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 5 observations with a pareto_k > 0.7 in model 'fit3'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\nfit4a <- add_criterion(fit4,c(\"waic\",\"loo\"))\n\nWarning: \n27 (10.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 6 observations with a pareto_k > 0.7 in model 'fit4'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\ncompall1 <- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"waic\")\ncompall2 <- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"loo\")\nprint(compall1, simplify = FALSE)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit4a    0.0       0.0  -415.7      11.7         42.7    4.3     831.4   23.4 \nfit1a   -0.6       1.2  -416.3      11.7         43.5    4.3     832.5   23.4 \nfit3a   -0.6       1.3  -416.3      11.7         43.5    4.3     832.6   23.4 \nfit2a -473.6      23.0  -889.4      22.4         10.3    3.0    1778.7   44.8 \n\nprint(compall2, simplify = FALSE)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit4a    0.0       0.0  -419.8     12.1        46.8    5.0    839.6   24.2  \nfit3a   -0.6       1.4  -420.4     12.1        47.6    5.0    840.7   24.2  \nfit1a   -1.1       1.5  -421.0     12.2        48.2    5.1    841.9   24.4  \nfit2a -469.6      23.0  -889.4     22.4        10.3    3.1   1778.9   44.8  \n\n\nModel 4 is considered best, though not by much. The above results, namely faster runtime and better estimates, speak more convincingly to the fact that model 4 is the best of these. The LOO is close to the PSIS metric reported by rethinking, even though I don’t think it’s defined and computed exactly the same."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Prior exploration",
    "text": "Prior exploration\nSince brms has a way of specifying the model and priors that makes direct mapping to the mathematical model a bit more opaque, it is useful to explore if the models we run are what we think we run. brms has two helpful functions for looking at priors. One can help set priors before fitting, the other shows priors after fitting. To make the output manageable, we look at the simplest model, model 2. This looks as follows\n\n#defining model again\nm2aeqs <- bf(outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\npreprior2 <- get_prior(m2aeqs,data=fitdat,family=gaussian())\npostprior2 <- prior_summary(fit2)\nprint(preprior2)\n\n               prior class      coef group resp dpar nlpar lb ub       source\n student_t(3, 0, 23) sigma                                  0         default\n              (flat)     b                           alpha            default\n              (flat)     b  dose_adj                 alpha       (vectorized)\n              (flat)     b Intercept                 alpha       (vectorized)\n              (flat)     b                            beta            default\n              (flat)     b  dose_adj                  beta       (vectorized)\n              (flat)     b Intercept                  beta       (vectorized)\n\nprint(postprior2)\n\n           prior class      coef group resp dpar nlpar lb ub  source\n          (flat)     b                           alpha       default\n  normal(0.3, 1)     b  dose_adj                 alpha          user\n    normal(2, 2)     b Intercept                 alpha          user\n          (flat)     b                            beta       default\n normal(-0.3, 1)     b  dose_adj                  beta          user\n  normal(0.5, 2)     b Intercept                  beta          user\n    cauchy(0, 1) sigma                                  0       user\n\n\nThe first output shows the priors as the model sees them, before we apply any settings. It uses defaults. The second output shows the actual priors used when fitting the model, which are the ones we set. I find these functions and the information useful, but overall it’s still a bit confusing to me. For instance why are there those flat entries in there? I don’t know what they mean.\nIt gets worse for bigger models, and here things get confusing to me. This is looking at the priors for models 1,3 and 4. Recall that we expect \\(2(N+1)+1\\) priors for models 1 and 3, and \\(2(N+1+1)+1\\) for model 4. Since our data has 24 samples, we should find 51 and 53 priors. Here is what we get:\n\npostprior1 <- prior_summary(fit1)\npostprior3 <- prior_summary(fit3)\npostprior4 <- prior_summary(fit4)\nprint(paste(nrow(postprior1),nrow(postprior3),nrow(postprior4)))\n\n[1] \"53 53 13\"\n\n\nCloser inspection shows that for models 1 and 3, the priors include those strange flat ones that only have a class but no coefficient. My guess is those are not “real”, and thus we actually have the right number of priors/parameters. This can be checked by looking at the names of all the parameters for say model 1. Here they are:\n\nnames(m1post)\n\n  [1] \"b_alpha_id1\"            \"b_alpha_id2\"            \"b_alpha_id3\"           \n  [4] \"b_alpha_id4\"            \"b_alpha_id5\"            \"b_alpha_id6\"           \n  [7] \"b_alpha_id7\"            \"b_alpha_id8\"            \"b_alpha_id9\"           \n [10] \"b_alpha_id10\"           \"b_alpha_id11\"           \"b_alpha_id12\"          \n [13] \"b_alpha_id13\"           \"b_alpha_id14\"           \"b_alpha_id15\"          \n [16] \"b_alpha_id16\"           \"b_alpha_id17\"           \"b_alpha_id18\"          \n [19] \"b_alpha_id19\"           \"b_alpha_id20\"           \"b_alpha_id21\"          \n [22] \"b_alpha_id22\"           \"b_alpha_id23\"           \"b_alpha_id24\"          \n [25] \"b_alpha_dose_adj\"       \"b_beta_id1\"             \"b_beta_id2\"            \n [28] \"b_beta_id3\"             \"b_beta_id4\"             \"b_beta_id5\"            \n [31] \"b_beta_id6\"             \"b_beta_id7\"             \"b_beta_id8\"            \n [34] \"b_beta_id9\"             \"b_beta_id10\"            \"b_beta_id11\"           \n [37] \"b_beta_id12\"            \"b_beta_id13\"            \"b_beta_id14\"           \n [40] \"b_beta_id15\"            \"b_beta_id16\"            \"b_beta_id17\"           \n [43] \"b_beta_id18\"            \"b_beta_id19\"            \"b_beta_id20\"           \n [46] \"b_beta_id21\"            \"b_beta_id22\"            \"b_beta_id23\"           \n [49] \"b_beta_id24\"            \"b_beta_dose_adj\"        \"sigma\"                 \n [52] \"prior_b_alpha_id1\"      \"prior_b_alpha_id2\"      \"prior_b_alpha_id3\"     \n [55] \"prior_b_alpha_id4\"      \"prior_b_alpha_id5\"      \"prior_b_alpha_id6\"     \n [58] \"prior_b_alpha_id7\"      \"prior_b_alpha_id8\"      \"prior_b_alpha_id9\"     \n [61] \"prior_b_alpha_id10\"     \"prior_b_alpha_id11\"     \"prior_b_alpha_id12\"    \n [64] \"prior_b_alpha_id13\"     \"prior_b_alpha_id14\"     \"prior_b_alpha_id15\"    \n [67] \"prior_b_alpha_id16\"     \"prior_b_alpha_id17\"     \"prior_b_alpha_id18\"    \n [70] \"prior_b_alpha_id19\"     \"prior_b_alpha_id20\"     \"prior_b_alpha_id21\"    \n [73] \"prior_b_alpha_id22\"     \"prior_b_alpha_id23\"     \"prior_b_alpha_id24\"    \n [76] \"prior_b_alpha_dose_adj\" \"prior_b_beta_id1\"       \"prior_b_beta_id2\"      \n [79] \"prior_b_beta_id3\"       \"prior_b_beta_id4\"       \"prior_b_beta_id5\"      \n [82] \"prior_b_beta_id6\"       \"prior_b_beta_id7\"       \"prior_b_beta_id8\"      \n [85] \"prior_b_beta_id9\"       \"prior_b_beta_id10\"      \"prior_b_beta_id11\"     \n [88] \"prior_b_beta_id12\"      \"prior_b_beta_id13\"      \"prior_b_beta_id14\"     \n [91] \"prior_b_beta_id15\"      \"prior_b_beta_id16\"      \"prior_b_beta_id17\"     \n [94] \"prior_b_beta_id18\"      \"prior_b_beta_id19\"      \"prior_b_beta_id20\"     \n [97] \"prior_b_beta_id21\"      \"prior_b_beta_id22\"      \"prior_b_beta_id23\"     \n[100] \"prior_b_beta_id24\"      \"prior_b_beta_dose_adj\"  \"prior_sigma\"           \n[103] \"lprior\"                 \"lp__\"                   \".chain\"                \n[106] \".iteration\"             \".draw\"                 \n\n\nWe can see that there are the right number of both priors and posterior parameters, namely 2 times 24 for the individual level parameters, plus 2 dose parameters and \\(\\sigma\\).\nI find model 4 more confusing. Here is the full list of priors:\n\nprint(postprior4)\n\n           prior class      coef group resp dpar nlpar lb ub       source\n          (flat)     b                           alpha            default\n  normal(0.3, 1)     b  dose_adj                 alpha               user\n    normal(2, 1)     b Intercept                 alpha               user\n          (flat)     b                            beta            default\n normal(-0.3, 1)     b  dose_adj                  beta               user\n  normal(0.5, 1)     b Intercept                  beta               user\n    cauchy(0, 1)    sd                           alpha  0            user\n    cauchy(0, 1)    sd                            beta  0            user\n    cauchy(0, 1)    sd              id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd              id            beta  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id            beta  0    (vectorized)\n    cauchy(0, 1) sigma                                  0            user\n\n\nAnd this shows the names of all parameters\n\nnames(m4post)\n\n  [1] \"b_alpha_Intercept\"         \"b_alpha_dose_adj\"         \n  [3] \"b_beta_Intercept\"          \"b_beta_dose_adj\"          \n  [5] \"sd_id__alpha_Intercept\"    \"sd_id__beta_Intercept\"    \n  [7] \"sigma\"                     \"r_id__alpha[1,Intercept]\" \n  [9] \"r_id__alpha[2,Intercept]\"  \"r_id__alpha[3,Intercept]\" \n [11] \"r_id__alpha[4,Intercept]\"  \"r_id__alpha[5,Intercept]\" \n [13] \"r_id__alpha[6,Intercept]\"  \"r_id__alpha[7,Intercept]\" \n [15] \"r_id__alpha[8,Intercept]\"  \"r_id__alpha[9,Intercept]\" \n [17] \"r_id__alpha[10,Intercept]\" \"r_id__alpha[11,Intercept]\"\n [19] \"r_id__alpha[12,Intercept]\" \"r_id__alpha[13,Intercept]\"\n [21] \"r_id__alpha[14,Intercept]\" \"r_id__alpha[15,Intercept]\"\n [23] \"r_id__alpha[16,Intercept]\" \"r_id__alpha[17,Intercept]\"\n [25] \"r_id__alpha[18,Intercept]\" \"r_id__alpha[19,Intercept]\"\n [27] \"r_id__alpha[20,Intercept]\" \"r_id__alpha[21,Intercept]\"\n [29] \"r_id__alpha[22,Intercept]\" \"r_id__alpha[23,Intercept]\"\n [31] \"r_id__alpha[24,Intercept]\" \"r_id__beta[1,Intercept]\"  \n [33] \"r_id__beta[2,Intercept]\"   \"r_id__beta[3,Intercept]\"  \n [35] \"r_id__beta[4,Intercept]\"   \"r_id__beta[5,Intercept]\"  \n [37] \"r_id__beta[6,Intercept]\"   \"r_id__beta[7,Intercept]\"  \n [39] \"r_id__beta[8,Intercept]\"   \"r_id__beta[9,Intercept]\"  \n [41] \"r_id__beta[10,Intercept]\"  \"r_id__beta[11,Intercept]\" \n [43] \"r_id__beta[12,Intercept]\"  \"r_id__beta[13,Intercept]\" \n [45] \"r_id__beta[14,Intercept]\"  \"r_id__beta[15,Intercept]\" \n [47] \"r_id__beta[16,Intercept]\"  \"r_id__beta[17,Intercept]\" \n [49] \"r_id__beta[18,Intercept]\"  \"r_id__beta[19,Intercept]\" \n [51] \"r_id__beta[20,Intercept]\"  \"r_id__beta[21,Intercept]\" \n [53] \"r_id__beta[22,Intercept]\"  \"r_id__beta[23,Intercept]\" \n [55] \"r_id__beta[24,Intercept]\"  \"prior_b_alpha_Intercept\"  \n [57] \"prior_b_alpha_dose_adj\"    \"prior_b_beta_Intercept\"   \n [59] \"prior_b_beta_dose_adj\"     \"prior_sigma\"              \n [61] \"prior_sd_id\"               \"prior_sd_id__1\"           \n [63] \"lprior\"                    \"lp__\"                     \n [65] \"z_1[1,1]\"                  \"z_1[1,2]\"                 \n [67] \"z_1[1,3]\"                  \"z_1[1,4]\"                 \n [69] \"z_1[1,5]\"                  \"z_1[1,6]\"                 \n [71] \"z_1[1,7]\"                  \"z_1[1,8]\"                 \n [73] \"z_1[1,9]\"                  \"z_1[1,10]\"                \n [75] \"z_1[1,11]\"                 \"z_1[1,12]\"                \n [77] \"z_1[1,13]\"                 \"z_1[1,14]\"                \n [79] \"z_1[1,15]\"                 \"z_1[1,16]\"                \n [81] \"z_1[1,17]\"                 \"z_1[1,18]\"                \n [83] \"z_1[1,19]\"                 \"z_1[1,20]\"                \n [85] \"z_1[1,21]\"                 \"z_1[1,22]\"                \n [87] \"z_1[1,23]\"                 \"z_1[1,24]\"                \n [89] \"z_2[1,1]\"                  \"z_2[1,2]\"                 \n [91] \"z_2[1,3]\"                  \"z_2[1,4]\"                 \n [93] \"z_2[1,5]\"                  \"z_2[1,6]\"                 \n [95] \"z_2[1,7]\"                  \"z_2[1,8]\"                 \n [97] \"z_2[1,9]\"                  \"z_2[1,10]\"                \n [99] \"z_2[1,11]\"                 \"z_2[1,12]\"                \n[101] \"z_2[1,13]\"                 \"z_2[1,14]\"                \n[103] \"z_2[1,15]\"                 \"z_2[1,16]\"                \n[105] \"z_2[1,17]\"                 \"z_2[1,18]\"                \n[107] \"z_2[1,19]\"                 \"z_2[1,20]\"                \n[109] \"z_2[1,21]\"                 \"z_2[1,22]\"                \n[111] \"z_2[1,23]\"                 \"z_2[1,24]\"                \n[113] \".chain\"                    \".iteration\"               \n[115] \".draw\"                    \n\n\nTo compare directly, this is the model we want:\n\\[\n\\begin{aligned}\nY_{i,t}  & \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\mu_{i,t} &  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i} &  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\na_{0,i} & \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} & \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\na_1 & \\sim \\mathrm{Normal}(0.3, 1) \\\\\nb_1 & \\sim \\mathrm{Normal}(-0.3, 1) \\\\\n\\mu_a & \\sim \\mathrm{Normal}(2, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(0.5, 1) \\\\\n\\sigma &  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]\nIf understand brms correctly, those z_ parameters are internal adjustments to make things more efficient and can otherwise be ignored. That means we have 2 times 24 parameters for the individual levels that all start with r_id. Those correspond to the \\(a_{0,i}\\) and \\(b_{0,1}\\), and they don’t have pre-defined priors, since they are computed based on other parameters. Then we have 2 dose parameters, which map to \\(a_1\\) and \\(b_1\\), both come with priors. We have 2 _Intercept parameters, which correspond to \\(\\mu_a\\) and \\(\\mu_b\\), again with priors. We have \\(\\sigma\\) with prior, and the two sd_id parameters seem to be those we call \\(\\sigma_a\\) and \\(\\sigma_b\\) in our equations.\nSo it looks like there is a match between our mathematical model we want, and the way we implemented it in brms. Still, I find the brms notation confusing and not that easy to follow. In that respect I much prefer ulam/rethinking.\nIn any case, I somewhat convinced myself that I’m fitting the same models here with brms that I’m fitting with ulam."
  },
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "",
    "text": "This is a continuation with some side analyses of this tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup."
  },
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "A model that “doesn’t work”",
    "text": "A model that “doesn’t work”\nWhen we originally scribbled down models that we might use to fit our data, we came up with this model.\n\\[\n\\begin{align}\n\\textrm{Outcome} \\\\\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t} =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Distribution for main parameters} \\\\\n\\alpha_{i}  \\sim \\mathrm{Normal}\\left(am_{i}, \\sigma_a \\right)  \\\\\n\\beta_{i}  \\sim \\mathrm{Normal}\\left(bm_{i}, \\sigma_b \\right) \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\nam_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\nbm_{i}  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{Distribution for (hyper)parameters} \\\\\na_{0,i}  \\sim \\mathrm{Normal}(2, 0.1) \\\\\na_{1}  \\sim \\mathrm{Normal}(0.5, 0.1) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(0, 0.1) \\\\\nb_{1}  \\sim \\mathrm{Normal}(-0.5, 0.1) \\\\\n\\sigma_a  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{align}\n\\]\nThe model is similar to models 1-3, but with another distribution for parameters \\(\\alpha_i\\) and \\(\\beta_i\\). Fitting this model didn’t work, the fitting routine kept choking. We concluded that with this model we are overfitting. However, I am also not sure if there is something more fundamentally wrong in the way we wrote down this model. I’m not sure if a mix of having parameters defined by equations, then as distributions and equations again is a generally wrong way. This is currently beyond my Bayesian understanding. Feedback appreciated 😄.\nIt is straightforward to translate the model to rethinking or brms code, but since it didn’t fit well, and I’m not even sure if it’s a “proper” model, there’s no point in showing the code. Implement it if you want, and let me know if you have some insights into what exactly might be wrong with the model."
  }
]