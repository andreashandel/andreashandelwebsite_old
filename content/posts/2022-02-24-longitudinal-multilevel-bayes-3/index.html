---
title: Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3  
summary: Part 3 of a tutorial showing how to fit Bayesian models using the `brms` package.
author: Andreas Handel
date: '2022-02-24'
lastMod: "2022-04-18"
slug: longitudinal-multilevel-bayesian-analysis-3
categories: 
- R
- Data Analysis
tags: 
- R
- Data Analysis
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
---



<p><strong>This is work in progress!</strong></p>
<p>This is part 3 of a tutorial illustrating how one can use the <code>brms</code> and <code>rethinking</code> R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.</p>
<p>I assume you’ve read both <a href="/posts/longitudinal-multilevel-bayesian-analysis-1/">part 1</a>, and <a href="/posts/longitudinal-multilevel-bayesian-analysis-2/">part 2</a> otherwise this post won’t make much sense.</p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In the previous post, I showed how to fit the data using the <code>rethinking</code> package. Now I’m re-doing it using <code>brms</code>. The <a href="https://paul-buerkner.github.io/brms/"><code>brms</code> package</a> is a widely used and very powerful tool to interface with Stan. It has overall more capabilities compared to <code>rethinking</code>. In my opinion, the main disadvantage is that it is often not obvious how to go from mathematical model to code, unless one has a good bit of experience jumping between the often terse formula notation of <code>brms</code> and the model equations. I’m not there yet, so I currently prefer to start with <code>rethinking</code>. But since <code>brms</code> can do things that are not as easy (or impossible) with <code>rethinking</code>, it seems good to know how to use both.</p>
<p>Also, comparing results using two different numerical packages is always good (even though both use <code>Stan</code> underneath, so in some sense those are not truly independent software routines).</p>
<p>As was true for <code>ulam/rethinking</code>, fitting the models can take a good bit of time. I therefore wrote separate <code>R</code> scripts for the fitting and the exploring parts. The code chunks from those scripts are shown below. The manual effort and slower pace of copying and pasting the code chunks from this tutorial and re-produce them can help in learning, but if you just want to get all the code from this post you can find it <a href="/posts/longitudinal-multilevel-bayesian-analysis-3/brmsfitmodels.R">here</a> and <a href="/posts/longitudinal-multilevel-bayesian-analysis-3/brmsexploremodels.R">here</a>.</p>
</div>
<div id="r-setup" class="section level1">
<h1>R Setup</h1>
<p>As always, make sure these packages are installed. <code>brms</code> uses the <a href="https://mc-stan.org/">Stan Bayesian modeling engine</a>. If you did the fitting with <code>rethinking</code> tutorial, you’ll have it already installed, otherwise you’ll need to install it. It is in my experience mostly seamless, but at times it seems to be tricky. I generally follow the instructions on the <a href="https://github.com/rmcelreath/rethinking"><code>rethinking</code> website</a> and it has so far always worked for me. It might need some fiddling, but you should be able to get them all to work.</p>
<pre class="r"><code>library(&#39;dplyr&#39;) # for data manipulation
library(&#39;ggplot2&#39;) # for plotting
library(&#39;cmdstanr&#39;) #for model fitting
library(&#39;brms&#39;) # for model fitting
library(&#39;posterior&#39;) #for post-processing
library(&#39;fs&#39;) #for file path</code></pre>
</div>
<div id="data-loading" class="section level1">
<h1>Data loading</h1>
<p>We’ll jump right in and load the data we generated in the previous tutorial.</p>
<pre class="r"><code>simdat &lt;- readRDS(&quot;simdat.Rds&quot;)
#pulling out number of observations
Ntot = length(unique(simdat$m3$id))

#fitting dataset 3
#we need to make sure the id is coded as a factor variable
#also removing anything in the dataframe that&#39;s not used for fitting
#makes the Stan code more robust
fitdat=list(id = as.factor(simdat[[3]]$id),
            outcome = simdat[[3]]$outcome,
            dose_adj = simdat[[3]]$dose_adj,
            time = simdat[[3]]$time)</code></pre>
</div>
<div id="fitting-with-brms" class="section level1">
<h1>Fitting with <code>brms</code></h1>
<p>We’ll fit some of the models we discussed in parts 1 and 2, now using the <code>brms</code> package. The main function in that package, which does the fitting using Stan, is <code>brm</code>.</p>
<p>First, we’ll specify each model. We’ll do that first, then run them all in a single loop.
Since we determined when using <code>ulam</code>/<code>rethinking</code> that our model 2 was a bad model, and model 4 and 4a didn’t lead to much of a difference, I’m skipping those here and only do models 1, 2a, 3 and 4. I’m also skipping model 5 since I only ran that for diagnostics/understanding and it doesn’t encode the right structure, since dose effect is missing.</p>
<div id="model-1" class="section level2">
<h2>Model 1</h2>
<p>This is one of the models with individual-level and dose-level effects, all priors fixed. This model has <span class="math inline">\(2N+2+1\)</span> parameters. <span class="math inline">\(N\)</span> each for the individual-level intercepts for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (the <span class="math inline">\(a_{0,i}\)</span> and <span class="math inline">\(b_{0,i}\)</span> parameters), the two dose-level parameters <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span>, and 1 overall deviation, <span class="math inline">\(\sigma\)</span> for the outcome distribution.</p>
<pre class="r"><code>#no-pooling model
#separate intercept for each individual/id
#2x(N+1)+1 parameters
m1eqs &lt;- bf(  #main equation for time-series trajectory
          outcome ~  exp(alpha)*log(time) - exp(beta)*time,
          #equations for alpha and beta
          alpha ~ 0 + id + dose_adj,
          beta  ~ 0 + id + dose_adj,
          nl = TRUE)

m1priors &lt;- c(#assign priors to all coefficients related to both id and dose_adj for alpha and beta
              prior(normal(2, 10),  class = &quot;b&quot;,  nlpar = &quot;alpha&quot;),
              prior(normal(0.5, 10),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;),
              #change the dose_adj priors to something different than the id priors
              prior(normal(0.3, 1),   class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;dose_adj&quot;),
              prior(normal(-0.3, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;dose_adj&quot;),
              prior(cauchy(0,1), class = &quot;sigma&quot;) )</code></pre>
<p>Notice how this notation in <code>brms</code> looks quite a bit different from the mathematical equations or the <code>ulam</code> implementation. That’s a part I don’t particularly like about <code>brms</code>, the very condensed formula notation. It takes time getting used to and it always requires extra checking to ensure the model implemented in code corresponds to the mathematical model. One can check by looking at the priors and make sure they look as expected. We’ll do that below after we fit.</p>
</div>
<div id="model-2a" class="section level2">
<h2>Model 2a</h2>
<p>This is the easiest model, with only population level effects for intercept and dose, so only 2+2+1 parameters.</p>
<pre class="r"><code>#full-pooling model
#2+2+1 parameters
m2aeqs &lt;- bf(  #main equation for time-series trajectory
  outcome ~ exp(alpha)*log(time) - exp(beta)*time,
  #equations for alpha and beta
  alpha ~ 1 + dose_adj,
  beta  ~  1 + dose_adj,
  nl = TRUE)

m2apriors &lt;- c(prior(normal(2, 2),  class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;Intercept&quot;),
              prior(normal(0.5, 2),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;Intercept&quot;),
              prior(normal(0.3, 1),   class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;dose_adj&quot;),
              prior(normal(-0.3, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;dose_adj&quot;),
              prior(cauchy(0,1), class = &quot;sigma&quot;)  )</code></pre>
</div>
<div id="model-3" class="section level2">
<h2>Model 3</h2>
<p>This is the same as model 1 but with different values for the priors.</p>
<pre class="r"><code>#same as model 1 but regularizing priors
m3eqs &lt;- m1eqs

m3priors &lt;- c(#assign priors to all coefficients related to id and dose_adj for alpha and beta
  prior(normal(2, 1),  class = &quot;b&quot;,  nlpar = &quot;alpha&quot;),
  prior(normal(0.5, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;),
  #change the dose_adj priors to something different than the id priors
  prior(normal(0.3, 1),   class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;dose_adj&quot;),
  prior(normal(-0.3, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;dose_adj&quot;),
  prior(cauchy(0,1), class = &quot;sigma&quot;) )</code></pre>
</div>
<div id="model-4" class="section level2">
<h2>Model 4</h2>
<p>This is the adaptive-pooling multi-level model where priors are estimated.
Here we have for each main parameter (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) an overall mean and standard deviation, and N individual intercepts, so 2 times 1+1+N. And of course we still have the 2 dose-related parameters and the overall standard deviation, so a total of 2*(1+1+N)+2+1 parameters.</p>
<pre class="r"><code>#adaptive prior, partial-pooling model
m4eqs &lt;- bf(  #main equation for time-series trajectory
  outcome ~ exp(alpha)*log(time) - exp(beta)*time,
  #equations for alpha and beta
  alpha ~  (1|id) + dose_adj,
  beta  ~  (1|id) + dose_adj,
  nl = TRUE)

m4priors &lt;- c(prior(normal(2, 1),  class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;Intercept&quot;),
              prior(normal(0.5, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;Intercept&quot;),
              prior(normal(0.3, 1),   class = &quot;b&quot;,  nlpar = &quot;alpha&quot;, coef = &quot;dose_adj&quot;),
              prior(normal(-0.3, 1),  class = &quot;b&quot;,  nlpar = &quot;beta&quot;, coef = &quot;dose_adj&quot;),
              prior(cauchy(0,1), class = &quot;sd&quot;, nlpar = &quot;alpha&quot;),
              prior(cauchy(0,1), class = &quot;sd&quot;, nlpar = &quot;beta&quot;),
              prior(cauchy(0,1), class = &quot;sigma&quot;)  )</code></pre>
</div>
<div id="combine-models" class="section level2">
<h2>Combine models</h2>
<p>To make our lives easier below, we combine all models and priors into lists.</p>
<pre class="r"><code>#stick all models into a list
modellist = list(m1=m1eqs,m2a=m2aeqs,m3=m3eqs,m4=m4eqs)
#also make list for priors
priorlist = list(m1priors=m1priors,m2apriors=m2apriors,m3priors=m3priors,m4priors=m4priors)
# set up a list in which we&#39;ll store our results
fl = vector(mode = &quot;list&quot;, length = length(modellist))</code></pre>
</div>
<div id="fitting-setup" class="section level2">
<h2>Fitting setup</h2>
<p>We define some general values for the fitting. Since the starting values depend on number of chains, we need to do this setup first.</p>
<pre class="r"><code>#general settings for fitting
#you might want to adjust based on your computer
warmup = 6000
iter = warmup + floor(warmup/2)
max_td = 18 #tree depth
adapt_delta = 0.9999
chains = 5
cores  = chains
seed = 1234</code></pre>
</div>
<div id="setting-starting-values" class="section level2">
<h2>Setting starting values</h2>
<p>We’ll again set starting values, as we did for <code>ulam/rethinking</code>.
Note that <code>brms</code> needs them in a somewhat different form, namely as list of lists for each model, one list for each chain.</p>
<p>I set different values for each chain, so I can check that each chain ends up at the same posterior. This is inspired by <a href="https://solomonkurz.netlify.app/post/2021-06-05-don-t-forget-your-inits/">this post by Solomon Kurz</a>, though I keep it simpler and just use the <code>jitter</code> function.</p>
<p>Note that this approach not only jitters (adds noise/variation) between chains, but also between the individual-level parameters for each chain. That’s fine for our purpose, it might even be beneficial.</p>
<pre class="r"><code>## Setting starting values
#starting values for model 1
startm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.5 , b1 = -0.5, sigma = 1)
#starting values for model 2a
startm2a = list(a0 = 2, b0 = 0.5, a1 = 0.5 , b1 = 0.5, sigma = 1)
#starting values for model 3
startm3 = startm1
#starting values for models 4
startm4 = list(mu_a = 2, sigma_a = 1, mu_b = 0, sigma_b = 1, a1 = 0.5 , b1 = -0.5, sigma = 1)
#put different starting values in list
#need to be in same order as models below
#one list for each chain, thus a 3-leveled list structure
#for each chain, we add jitter so they start at different values
startlist = list( rep(list(lapply(startm1,jitter,10)),chains),
                  rep(list(lapply(startm2a,jitter,10)),chains),
                  rep(list(lapply(startm3,jitter,10)),chains),
                  rep(list(lapply(startm4,jitter,10)),chains)
                  )</code></pre>
</div>
<div id="model-fitting" class="section level2">
<h2>Model fitting</h2>
<p>We’ll use the same strategy to loop though all models and fit them.
The fitting code looks very similar to the previous one for <code>rethinking/ulam</code>, only now the fitting is done calling the <code>brm</code> function.</p>
<pre class="r"><code># fitting models
#loop over all models and fit them using ulam
for (n in 1:length(modellist))
{

  cat(&#39;************** \n&#39;)
  cat(&#39;starting model&#39;, names(modellist[n]), &#39;\n&#39;)

  tstart=proc.time(); #capture current time

  fl[[n]]$fit &lt;- brm(formula = modellist[[n]],
                   data = fitdat,
                   family = gaussian(),
                   prior = priorlist[[n]],
                   init = startlist[[n]],
                   control=list(adapt_delta=adapt_delta, max_treedepth = max_td),
                   sample_prior = TRUE,
                   chains=chains, cores = cores,
                   warmup = warmup, iter = iter,
                   seed = seed,
                   backend = &quot;cmdstanr&quot;
  )# end brm statement

  tend=proc.time(); #capture current time
  tdiff=tend-tstart;
  runtime_minutes=tdiff[[3]]/60;

  cat(&#39;model fit took this many minutes:&#39;, runtime_minutes, &#39;\n&#39;)
  cat(&#39;************** \n&#39;)

  #add some more things to the fit object
  fl[[n]]$runtime = runtime_minutes
  fl[[n]]$model = names(modellist)[n]
}
# saving the results so we can use them later
filepath = fs::path(&quot;D:&quot;,&quot;Dropbox&quot;,&quot;datafiles&quot;,&quot;longitudinalbayes&quot;,&quot;brmsfits&quot;, ext=&quot;Rds&quot;)
saveRDS(fl,filepath)</code></pre>
<p>You’ll likely find that model 1 takes the longest, the other ones run faster. You can check the runtime for each model by looking at <code>fl[[n]]$runtime</code>. It’s useful to first run with few iterations (100s instead of 1000s), make sure everything works in principle, then do a “final” long run with longer chains.</p>
</div>
</div>
<div id="explore-model-fits" class="section level1">
<h1>Explore model fits</h1>
<p>As before, fits are in the list called <code>fl</code>. For each model the actual fit is in <code>fit</code>, the model name is in <code>model</code> and the run time is in <code>runtime</code>. Note that the code chunks below come from <a href="/posts/longitudinal-multilevel-bayesian-analysis-3/brmsexploremodels.R">this second R script</a>, thus some things are repeated (e.g., loading of simulated data).</p>
<p>As we did after fitting with <code>ulam/rethinking</code>, let’s briefly inspect some of the models. I’m again only showing a few of those explorations to illustrate what I mean. For any real fitting, it is important to carefully look at all the output and make sure everything worked as expected and makes sense.</p>
<p>I’m again focusing on the simple model 2a, which has no individual-level parameters, thus only a total of 5.</p>
<p>We are using various additional packages here to get plots and output that looks similar to what <code>rethinking</code> produces. I’m getting most of the code snippets from the <a href="https://bookdown.org/content/4857/">Statistical Rethinking using <code>brms</code> book</a> by Solomon Kurz.</p>
<p>Need a few more packages for this part:</p>
<pre class="r"><code>library(&#39;dplyr&#39;) # for data manipulation
library(&#39;tidyr&#39;) # for data manipulation
library(&#39;ggplot2&#39;) # for plotting
library(&#39;cmdstanr&#39;) #for model fitting
library(&#39;brms&#39;) # for model fitting
library(&#39;posterior&#39;) #for post-processing
library(&#39;bayesplot&#39;) #for plots
library(&#39;fs&#39;) #for file path</code></pre>
<p>Loading the data:</p>
<pre class="r"><code># loading list of previously saved fits.
# useful if we don&#39;t want to re-fit
# every time we want to explore the results.
# since the file is too large for GitHub
# it is stored in a local folder
# adjust accordingly for your setup
filepath = fs::path(&quot;D:&quot;,&quot;Dropbox&quot;,&quot;datafiles&quot;,&quot;longitudinalbayes&quot;,&quot;brmsfits&quot;, ext=&quot;Rds&quot;)
fl &lt;- readRDS(filepath)
# also load data file used for fitting
simdat &lt;- readRDS(&quot;simdat.Rds&quot;)
#pull our the data set we used for fitting
#if you fit a different one of the simulated datasets, change accordingly
fitdat &lt;- simdat$m3
#contains parameters used for fitting
pars &lt;- simdat$m3pars</code></pre>
<p>The summary output looks a bit different compared to <code>ulam</code>, but fairly similar.</p>
<pre class="r"><code># Model 2a summary
#saving a bit of typing below
fit2 &lt;- fl[[2]]$fit
summary(fit2)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: outcome ~ exp(alpha) * log(time) - exp(beta) * time 
##          alpha ~ 1 + dose_adj
##          beta ~ 1 + dose_adj
##    Data: fitdat (Number of observations: 264) 
##   Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;
##          total post-warmup draws = 15000
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## alpha_Intercept     2.98      0.02     2.94     3.02 1.00     6244     6691
## alpha_dose_adj      0.10      0.01     0.08     0.12 1.00     6569     7301
## beta_Intercept      0.99      0.02     0.95     1.03 1.00     6387     6724
## beta_dose_adj      -0.10      0.01    -0.11    -0.08 1.00     6947     7786
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     6.88      0.30     6.32     7.49 1.00     8391     7850
## 
## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Here is the default trace plot. Note that <code>brms</code> only plots the post-warmup iterations, and also shows the posterior distributions.</p>
<pre class="r"><code># Model 2a trace plots
plot(fit2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/traceplot-1.png" width="672" /></p>
<p>Since I want to see if the different initial conditions did something useful, I was trying to make a trace plot that shows warmup. Solomon Kurz has <a href="https://bookdown.org/content/4857/markov-chain-monte-carlo.html#visualization.">an example using the <code>ggmcmc</code> package</a>, but his code doesn’t work for me, it always ignores the warmup. I used 6000 warmup samples and 3000 post-warmup samples for each chain. Currently, the figure only shows post-warmup.</p>
<p>For now, it’s another trace plot using the <code>bayesplot</code> package - also which has an example of making the plot I want, but for some reason the <code>stanfit</code> object inside the <code>brms</code> output does not contain the warmups. So for now, what’s shown doesn’t actually include the warmups. Leaving this plot for now and moving on…</p>
<pre class="r"><code># Another trace plot, using the bayesplot package
posterior &lt;- rstan::extract(fit2$fit, inc_warmup = TRUE, permuted = FALSE)
bayesplot::mcmc_trace(posterior, n_warmup = 400, pars = variables(fit2)[c(1,2,3,4,5)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/traceplot-2-1.png" width="672" /></p>
<p>Here is a version of the trank plots. I’m pulling out the first 5 variables since the others are not that interesting for this plot, e.g., they contain prior samples. You can look at them if you want.</p>
<pre class="r"><code># Model 2a trank plots with bayesplot
bayesplot::mcmc_rank_overlay(fit2, pars = variables(fit2)[c(1,2,3,4,5)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/trankplot-1.png" width="672" /></p>
<p>Another nice plot I saw was an autocorrelation plot. One wants little autocorrelation for parameters. This seems to be the case:</p>
<pre class="r"><code>bayesplot::mcmc_acf(fit2, pars = variables(fit2)[c(1,2,3,4,5)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/autocorrelationplot-1.png" width="672" /></p>
<p>And finally a pair plot.</p>
<pre class="r"><code># Model 2a pair plot
# Correlation between posterior samples of parameters
pairs(fit2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/pairplot-1.png" width="672" /></p>
<p>While the layout looks different - and I didn’t bother to try and make things look exactly the same between <code>brms</code> and <code>rethinking</code> - the overall results are similar. That’s encouraging.</p>
<p>Some of the plots already showed posterior distributions, but let’s look at those more carefully.</p>
<div id="models-1-and-3" class="section level2">
<h2>Models 1 and 3</h2>
<p>Let’s explore those two models first. Recall that they are the same, apart from the prior definitions. As previously, the wider priors for model 1 make it less efficient. With the settings I used, run times were 417 minutes for model 1 versus 61 minutes for model 3.</p>
<p>Let’s see if the priors impact the results, i.e. the posterior distributions.
We can actually do that by looking briefly at the summaries for both fits.</p>
<pre class="r"><code>#save some typing
fit1 &lt;- fl[[1]]$fit
fit3 &lt;- fl[[3]]$fit
summary(fit1)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: outcome ~ exp(alpha) * log(time) - exp(beta) * time 
##          alpha ~ 0 + id + dose_adj
##          beta ~ 0 + id + dose_adj
##    Data: fitdat (Number of observations: 264) 
##   Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;
##          total post-warmup draws = 15000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## alpha_id1          3.50      1.67     0.26     6.80 1.00     2840     4734
## alpha_id2          3.46      1.67     0.21     6.75 1.00     2839     4794
## alpha_id3          3.26      1.67     0.02     6.56 1.00     2839     4760
## alpha_id4          3.19      1.67    -0.05     6.49 1.00     2841     4760
## alpha_id5          3.24      1.67    -0.01     6.53 1.00     2839     4760
## alpha_id6          3.33      1.67     0.08     6.62 1.00     2839     4760
## alpha_id7          3.28      1.67     0.03     6.58 1.00     2841     4796
## alpha_id8          2.98      0.02     2.95     3.01 1.00    17885    10342
## alpha_id9          2.91      0.02     2.88     2.94 1.00    17315    10942
## alpha_id10         2.98      0.02     2.95     3.01 1.00    17656    10966
## alpha_id11         2.94      0.02     2.91     2.97 1.00    18085    10133
## alpha_id12         2.84      0.02     2.81     2.88 1.00    17692    11631
## alpha_id13         2.97      0.02     2.94     3.00 1.00    18451    10345
## alpha_id14         3.09      0.01     3.06     3.12 1.00    18387    10003
## alpha_id15         2.95      0.02     2.91     2.98 1.00    17682    10963
## alpha_id16         2.77      1.67    -0.52     6.01 1.00     2839     4781
## alpha_id17         2.54      1.67    -0.76     5.79 1.00     2840     4750
## alpha_id18         2.73      1.67    -0.57     5.97 1.00     2839     4798
## alpha_id19         2.76      1.67    -0.53     6.01 1.00     2839     4820
## alpha_id20         2.73      1.67    -0.56     5.98 1.00     2840     4771
## alpha_id21         2.71      1.67    -0.59     5.96 1.00     2840     4751
## alpha_id22         2.66      1.67    -0.64     5.91 1.00     2839     4807
## alpha_id23         2.65      1.67    -0.64     5.90 1.00     2840     4764
## alpha_id24         2.59      1.67    -0.70     5.84 1.00     2838     4762
## alpha_dose_adj     0.22      0.73    -1.19     1.65 1.00     2839     4785
## beta_id1           0.75      1.71    -2.66     4.10 1.00     2420     4179
## beta_id2           0.65      1.71    -2.76     4.01 1.00     2420     4181
## beta_id3           0.70      1.71    -2.72     4.04 1.00     2419     4158
## beta_id4           0.71      1.71    -2.70     4.06 1.00     2419     4155
## beta_id5           0.93      1.71    -2.48     4.28 1.00     2418     4167
## beta_id6           0.68      1.71    -2.73     4.03 1.00     2419     4175
## beta_id7           0.77      1.71    -2.64     4.13 1.00     2419     4155
## beta_id8           1.01      0.01     0.99     1.04 1.00    16977    10323
## beta_id9           0.91      0.02     0.88     0.94 1.00    17374    11382
## beta_id10          0.98      0.01     0.96     1.01 1.00    18009    10155
## beta_id11          1.15      0.01     1.13     1.18 1.00    18260    10293
## beta_id12          1.05      0.01     1.02     1.07 1.00    17891    11580
## beta_id13          1.01      0.01     0.98     1.04 1.00    18998    10824
## beta_id14          0.95      0.01     0.92     0.98 1.00    18321    10396
## beta_id15          0.79      0.02     0.75     0.82 1.00    17550    11046
## beta_id16          1.36      1.71    -1.99     4.77 1.00     2418     4208
## beta_id17          1.08      1.71    -2.27     4.49 1.00     2419     4159
## beta_id18          1.36      1.71    -2.00     4.77 1.00     2421     4150
## beta_id19          1.44      1.71    -1.92     4.85 1.00     2417     4173
## beta_id20          1.09      1.71    -2.25     4.50 1.00     2420     4083
## beta_id21          1.31      1.71    -2.04     4.73 1.00     2420     4118
## beta_id22          1.24      1.71    -2.10     4.65 1.00     2421     4122
## beta_id23          1.12      1.71    -2.23     4.53 1.00     2419     4157
## beta_id24          1.09      1.71    -2.26     4.51 1.00     2419     4209
## beta_dose_adj     -0.21      0.74    -1.69     1.24 1.00     2419     4163
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.06      0.05     0.97     1.17 1.00    16342    11307
## 
## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<pre class="r"><code>summary(fit3)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: outcome ~ exp(alpha) * log(time) - exp(beta) * time 
##          alpha ~ 0 + id + dose_adj
##          beta ~ 0 + id + dose_adj
##    Data: fitdat (Number of observations: 264) 
##   Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;
##          total post-warmup draws = 15000
## 
## Population-Level Effects: 
##                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## alpha_id1          3.31      0.25     2.83     3.80 1.00     2443     4189
## alpha_id2          3.27      0.25     2.79     3.75 1.00     2418     4115
## alpha_id3          3.07      0.25     2.59     3.55 1.00     2447     4120
## alpha_id4          3.00      0.25     2.52     3.48 1.00     2433     4196
## alpha_id5          3.05      0.25     2.56     3.53 1.00     2437     4206
## alpha_id6          3.14      0.25     2.66     3.62 1.00     2465     4179
## alpha_id7          3.09      0.25     2.61     3.57 1.00     2438     4311
## alpha_id8          2.98      0.02     2.95     3.01 1.00    18493    10858
## alpha_id9          2.91      0.02     2.87     2.94 1.00    18973    11242
## alpha_id10         2.98      0.02     2.95     3.01 1.00    18479    10529
## alpha_id11         2.94      0.02     2.91     2.97 1.00    18804    10454
## alpha_id12         2.84      0.02     2.81     2.87 1.00    18873    11236
## alpha_id13         2.97      0.02     2.94     3.00 1.00    19043    11438
## alpha_id14         3.09      0.01     3.06     3.12 1.00    19247    10690
## alpha_id15         2.95      0.02     2.91     2.98 1.00    19114    12099
## alpha_id16         2.96      0.25     2.48     3.44 1.00     2432     4256
## alpha_id17         2.73      0.25     2.25     3.21 1.00     2418     4214
## alpha_id18         2.92      0.25     2.43     3.39 1.00     2433     4335
## alpha_id19         2.95      0.25     2.47     3.43 1.00     2438     4308
## alpha_id20         2.92      0.25     2.43     3.40 1.00     2427     4161
## alpha_id21         2.90      0.25     2.41     3.37 1.00     2418     4311
## alpha_id22         2.85      0.25     2.36     3.33 1.00     2439     4182
## alpha_id23         2.84      0.25     2.36     3.32 1.00     2431     4213
## alpha_id24         2.78      0.25     2.30     3.26 1.00     2438     4170
## alpha_dose_adj     0.14      0.11    -0.07     0.35 1.00     2426     4307
## beta_id1           1.05      0.24     0.58     1.53 1.00     2953     5165
## beta_id2           0.96      0.24     0.49     1.43 1.00     2937     5242
## beta_id3           1.00      0.24     0.53     1.47 1.00     2944     5168
## beta_id4           1.01      0.24     0.54     1.49 1.00     2937     5142
## beta_id5           1.24      0.24     0.76     1.71 1.00     2939     5218
## beta_id6           0.99      0.24     0.52     1.46 1.00     2946     5117
## beta_id7           1.08      0.24     0.60     1.55 1.00     2941     5223
## beta_id8           1.01      0.01     0.99     1.04 1.00    18029    10844
## beta_id9           0.91      0.02     0.88     0.93 1.00    18953    11005
## beta_id10          0.98      0.01     0.96     1.01 1.00    18500    10509
## beta_id11          1.15      0.01     1.13     1.17 1.00    18599    10418
## beta_id12          1.05      0.01     1.02     1.07 1.00    19002    10853
## beta_id13          1.01      0.01     0.98     1.04 1.00    18714    11040
## beta_id14          0.95      0.01     0.92     0.98 1.00    19168    10286
## beta_id15          0.79      0.02     0.75     0.82 1.00    18771    11344
## beta_id16          1.06      0.24     0.58     1.53 1.00     2943     5165
## beta_id17          0.78      0.25     0.30     1.25 1.00     2941     5155
## beta_id18          1.05      0.24     0.58     1.53 1.00     2939     5207
## beta_id19          1.14      0.24     0.66     1.61 1.00     2951     5251
## beta_id20          0.79      0.25     0.31     1.26 1.00     2962     5253
## beta_id21          1.00      0.24     0.52     1.47 1.00     2944     5222
## beta_id22          0.94      0.24     0.46     1.41 1.00     2951     5207
## beta_id23          0.82      0.25     0.34     1.29 1.00     2943     5263
## beta_id24          0.79      0.25     0.31     1.26 1.00     2957     5311
## beta_dose_adj     -0.08      0.11    -0.29     0.13 1.00     2939     5258
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.06      0.05     0.97     1.17 1.00    15961    10880
## 
## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Note the different naming of the parameters in <code>brms</code>. It’s unfortunately not possible (as far as I know) to get the names match the mathematical model. The parameters that have <code>dose</code> in their names are the ones we called <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> in our models. The many <code>_id</code> parameters are our previous <span class="math inline">\(a_0\)</span> and <span class="math inline">\(b_0\)</span> parameters. Conceptually, the latter are on the individual level. But we don’t have a nested/multi-level structure here, which seems to lead <code>brms</code> to consider every parameter on the same level, and thus labeling them all <em>population level</em>.</p>
<p>Now, let’s look at priors and posteriors somewhat more. First, we extract priors and posteriors.</p>
<pre class="r"><code>#get priors and posteriors for models 1 and 3
m1prior &lt;- prior_draws(fit1)
m1post &lt;- as_draws_df(fit1)
m3prior &lt;- prior_draws(fit3)
m3post &lt;- as_draws_df(fit3)</code></pre>
<p>Now we can plot the distributions. I’m focusing on the <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> parameters since those are of more interest, and because I couldn’t figure out quickly how to get out and process all the individual level <span class="math inline">\(a_0\)</span> and <span class="math inline">\(b_0\)</span> parameters from <code>brms</code> 😁.</p>
<pre class="r"><code>#showing density plots for a1

#make a data frame and get it in shape for ggplot
a1df &lt;- data.frame(m1_prior = m1prior$b_alpha_dose_adj,
                   m1_post = m1post$b_alpha_dose_adj,
                   m3_prior = m3prior$b_alpha_dose_adj,
                   m3_post =  m3post$b_alpha_dose_adj) %&gt;%
        pivot_longer(cols = everything(), names_to = c(&quot;model&quot;,&quot;type&quot;), names_pattern = &quot;(.*)_(.*)&quot;, values_to = &quot;value&quot;)
# make plot
p1 &lt;- a1df %&gt;%
  ggplot() +
  geom_density(aes(x = value, color = model, linetype = type), size = 1) +
  theme_minimal()
plot(p1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_1_3_prior_plots-1.png" width="672" /></p>
<pre class="r"><code>#save for display on post
ggsave(file = paste0(&quot;featured.png&quot;), p1, dpi = 300, units = &quot;in&quot;, width = 6, height = 6)


#showing density plots for b1
b1df &lt;- data.frame(m1_prior = m1prior$b_beta_dose_adj,
                   m1_post = m1post$b_beta_dose_adj,
                   m3_prior = m3prior$b_beta_dose_adj,
                   m3_post =  m3post$b_beta_dose_adj) %&gt;%
  pivot_longer(cols = everything(), names_to = c(&quot;model&quot;,&quot;type&quot;), names_pattern = &quot;(.*)_(.*)&quot;, values_to = &quot;value&quot;)

p2 &lt;- b1df %&gt;%
  ggplot() +
  geom_density(aes(x = value, color = model, linetype = type), size = 1) +
  theme_minimal()
plot(p2)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_1_3_prior_plots-2.png" width="672" /></p>
<p>As before, the priors for the <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> parameters are the same. We only changed the <span class="math inline">\(a_0\)</span> and <span class="math inline">\(b_0\)</span> priors, but that change leads to different posteriors for <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span>. It’s basically the same result we found with <code>ulam/rethinking</code>.</p>
<p>It would be surprising if we did NOT find the same correlation structure again in the parameters, let’s check it.</p>
<pre class="r"><code># a few parameters for each dose
#low dose
pairs(fit1, variable = variables(fit1)[c(1:4,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_1_3_pair_plots-1.png" width="672" /></p>
<pre class="r"><code>#medium dose
pairs(fit1, variable = variables(fit1)[c(8:11,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_1_3_pair_plots-2.png" width="672" /></p>
<pre class="r"><code>#high dose
pairs(fit1, variable = variables(fit1)[c(16:19,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_1_3_pair_plots-3.png" width="672" /></p>
<p>Apart from the unfortunate naming of parameters in <code>brms</code>, these are the same plots as we made for the <code>ulam</code> fits and show the same patterns.</p>
<p>Let’s look at the posteriors in numerical form.</p>
<pre class="r"><code># model 1 first
fit1pars = posterior::summarize_draws(m1post, &quot;mean&quot;, &quot;sd&quot;, &quot;quantile2&quot;, default_convergence_measures())

#only entries for the a0 parameters
a0post &lt;- m1post %&gt;% dplyr::select(starts_with(&#39;b_alpha_id&#39;))
fit1a0mean &lt;- mean(colMeans(a0post))
#only entries for the b0 parameters
b0post &lt;- m1post %&gt;% dplyr::select(starts_with(&#39;b_beta_id&#39;))
fit1b0mean &lt;- mean(colMeans(b0post))
fit1otherpars &lt;- fit1pars %&gt;% dplyr::filter(!grepl(&#39;_id&#39;,variable)) %&gt;%
  dplyr::filter(!grepl(&#39;prior&#39;,variable))
print(fit1otherpars)</code></pre>
<pre><code>## # A tibble: 4 x 8
##   variable             mean     sd       q5     q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 b_alpha_dose_adj    0.224 0.726    -0.972    1.44  1.00    2839.    4785.
## 2 b_beta_dose_adj    -0.212 0.744    -1.44     1.01  1.00    2419.    4163.
## 3 sigma               1.06  0.0514    0.981    1.15  1.00   16342.   11307.
## 4 lp__             -549.    5.58   -559.    -540.    1.00    4974.    8286.</code></pre>
<pre class="r"><code>print(c(fit1a0mean,fit1b0mean))</code></pre>
<pre><code>## [1] 2.960140 1.006334</code></pre>
<pre class="r"><code># repeat for model 3
fit3pars = posterior::summarize_draws(m3post, &quot;mean&quot;, &quot;sd&quot;, &quot;quantile2&quot;, default_convergence_measures())
#only entries for the a0 parameters
a0post &lt;- m3post %&gt;% dplyr::select(starts_with(&#39;b_alpha_id&#39;))
fit3a0mean &lt;- mean(colMeans(a0post))
#only entries for the b0 parameters
b0post &lt;- m3post %&gt;% dplyr::select(starts_with(&#39;b_beta_id&#39;))
fit3b0mean &lt;- mean(colMeans(b0post))
fit3otherpars &lt;- fit3pars %&gt;% dplyr::filter(!grepl(&#39;_id&#39;,variable)) %&gt;%
  dplyr::filter(!grepl(&#39;prior&#39;,variable))
print(fit3otherpars)</code></pre>
<pre><code>## # A tibble: 4 x 8
##   variable              mean     sd        q5       q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 b_alpha_dose_adj    0.142  0.107    -0.0334    0.316   1.00    2426.    4307.
## 2 b_beta_dose_adj    -0.0811 0.106    -0.254     0.0921  1.00    2939.    5258.
## 3 sigma               1.06   0.0515    0.982     1.15    1.00   15961.   10880.
## 4 lp__             -453.     5.60   -463.     -444.      1.00    4605.    7829.</code></pre>
<pre class="r"><code>print(c(fit3a0mean,fit3b0mean))</code></pre>
<pre><code>## [1] 2.9756367 0.9808696</code></pre>
<p>Again, model 1 seems worse, with higher uncertainty intervals for the <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> parameters and the mean further away from the true value.</p>
<!-- I'm not sure why the mean estimates for $a_0$ and $b_0$ are also off. Maybe the way I computed the mean across all individuals of the means of the distribution skewed things? Not sure. Since neither of these models are that great anyway, I'm not trying to dig deeper for now (but if you know what's going on, let me know). -->
<p>We can also compare the models as we did for <code>rethinking</code> using these lines of code:</p>
<pre class="r"><code>fit13comp &lt;- loo_compare(add_criterion(fit1,&quot;waic&quot;),
            add_criterion(fit3,&quot;waic&quot;),
            criterion = &quot;waic&quot;)</code></pre>
<pre><code>## Warning: 
## 30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Warning: 
## 29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre class="r"><code>print(fit13comp, simplify = FALSE)</code></pre>
<pre><code>##                             elpd_diff se_diff elpd_waic se_elpd_waic p_waic
## add_criterion(fit1, &quot;waic&quot;)    0.0       0.0  -416.3      11.7         43.5
## add_criterion(fit3, &quot;waic&quot;)    0.0       0.2  -416.3      11.7         43.5
##                             se_p_waic waic   se_waic
## add_criterion(fit1, &quot;waic&quot;)    4.3     832.5   23.4 
## add_criterion(fit3, &quot;waic&quot;)    4.3     832.6   23.4</code></pre>
<p>Model performance is similar between models. Surprisingly, the WAIC values are not that close to those reported by <code>rethinking</code>. Also, the <code>pWAIC</code>, which is a measure for the effective number of parameters in each model, is not quite the same. I am not sure what the reason is for these differences.</p>
</div>
<div id="comparison-with-the-truth-and-ulam" class="section level2">
<h2>Comparison with the truth and <code>ulam</code></h2>
<p>The values used to generate the data are: <span class="math inline">\(\sigma =\)</span> 1, <span class="math inline">\(\mu_a =\)</span> 3, <span class="math inline">\(\mu_b =\)</span> 1, <span class="math inline">\(a_1 =\)</span> 0.1, <span class="math inline">\(b_1 =\)</span> -0.1.</p>
<p>Since the models are the same as those we previously fit with <code>ulam</code>, only a different <code>R</code> package is used to run them, we should expect very similar results. This is the case. We find that as for the <code>ulam</code> fits, the estimates for <span class="math inline">\(a_0\)</span>, <span class="math inline">\(b_0\)</span> and <span class="math inline">\(\sigma\)</span> are similar to the values used the generate the data, but estimates for <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> are not that great. The agreement with <code>ulam</code> is good, because we should expect that if we fit the same models, results should - up to numerical/sampling differences - be the same, no matter what software implementation we use. It also suggests that we did things right - or made the same mistake in both implementations! 😁.</p>
<p>Why the WAIC estimates are different is currently not clear to me. It could be that the 2 packages use different definitions/ways to compute it. Or something more fundamental is still different. I’m not sure.</p>
</div>
<div id="model-2a-1" class="section level2">
<h2>Model 2a</h2>
<p>This is the model with only population-level estimates. We already explored it somewhat above when we looked at traceplots and trankplots and the like.
Here is just another quick table for the posteriors.</p>
<pre class="r"><code>m2post &lt;- as_draws_df(fit2)
fit2pars = posterior::summarize_draws(m2post, &quot;mean&quot;, &quot;sd&quot;, &quot;quantile2&quot;, default_convergence_measures())
fit2otherpars &lt;- fit2pars %&gt;% dplyr::filter(!grepl(&#39;prior&#39;,variable))
print(fit2otherpars)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   variable               mean      sd        q5      q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 b_alpha_Intercept    2.98   0.0211     2.95    3.02e+0  1.00    6244.    6691.
## 2 b_alpha_dose_adj     0.0960 0.00967    0.0802  1.12e-1  1.00    6569.    7301.
## 3 b_beta_Intercept     0.992  0.0188     0.961   1.02e+0  1.00    6387.    6724.
## 4 b_beta_dose_adj     -0.0971 0.00862   -0.111  -8.29e-2  1.00    6947.    7786.
## 5 sigma                6.88   0.302      6.39    7.39e+0  1.00    8391.    7850.
## 6 lp__              -892.     1.59    -895.     -8.90e+2  1.00    4964.    7039.</code></pre>
<p>The parameters that have <code>_Intercept</code> in their name are what we called <span class="math inline">\(\mu_a\)</span> and <span class="math inline">\(\mu_b\)</span>, the ones containing <code>_dose</code> are our <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span>. We find pretty much the same results we found using <code>ulam</code>. Specifically, the main parameters are estimated well, but because the model is not very flexible, the estimate for <span class="math inline">\(\sigma\)</span> is much larger, since it needs to account for all the individual-level variation we ommitted from the model itself.</p>
</div>
<div id="model-4-1" class="section level2">
<h2>Model 4</h2>
<p>This is what I consider the most interesting and conceptually best model. It performed best in the <code>ulam</code> fits. Let’s see how it looks here. It is worth pointing out that this model ran much faster compared to models 1 and 3, it only took 10.5518333 minutes.</p>
<p>We’ll start with the summary for the model.</p>
<pre class="r"><code>fit4 &lt;- fl[[4]]$fit
m4prior &lt;- prior_draws(fit4)
m4post &lt;- as_draws_df(fit4)
summary(fit4)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: outcome ~ exp(alpha) * log(time) - exp(beta) * time 
##          alpha ~ (1 | id) + dose_adj
##          beta ~ (1 | id) + dose_adj
##    Data: fitdat (Number of observations: 264) 
##   Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;
##          total post-warmup draws = 15000
## 
## Group-Level Effects: 
## ~id (Number of levels: 24) 
##                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(alpha_Intercept)     0.09      0.02     0.07     0.13 1.00     3685     6514
## sd(beta_Intercept)      0.12      0.02     0.09     0.16 1.00     4048     5853
## 
## Population-Level Effects: 
##                 Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## alpha_Intercept     2.99      0.02     2.95     3.03 1.00     3771     5404
## alpha_dose_adj      0.09      0.01     0.07     0.11 1.00     3979     5040
## beta_Intercept      0.99      0.02     0.94     1.03 1.00     3486     5134
## beta_dose_adj      -0.11      0.01    -0.13    -0.08 1.00     3855     5732
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     1.06      0.05     0.97     1.17 1.00    10136    10314
## 
## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Next, the prior/posterior plots. To ensure one can see the priors, I’m cutting off the y-axis at 10, that’s why the posteriors look a bit weird. They do infected extend and peak like the distributions shown for models 1 and 3.</p>
<pre class="r"><code>#showing density plots for a1 and b1
#make a data frame and get it in shape for ggplot
m4df &lt;- data.frame(a1_prior = m4prior$b_alpha_dose_adj,
                   a1_post = m4post$b_alpha_dose_adj,
                   b1_prior = m4prior$b_beta_dose_adj,
                   b1_post = m4post$b_beta_dose_adj) %&gt;%
  pivot_longer(cols = everything(), names_to = c(&quot;parameter&quot;,&quot;type&quot;), names_pattern = &quot;(.*)_(.*)&quot;, values_to = &quot;value&quot;)
# make plot
p1 &lt;- m4df %&gt;%
  ggplot() +
  ylim(0, 10) + xlim(-2, 2) +
  geom_density(aes(x = value, color = parameter, linetype = type), adjust = 10, size = 1) +
  ggtitle(&#39;model 4, parameters a1 and b1&#39;) +
  theme_minimal()
plot(p1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_4_prior_plots-1.png" width="672" /></p>
<p>Numerical output for the posterior:</p>
<pre class="r"><code>fit4pars = posterior::summarize_draws(m4post, &quot;mean&quot;, &quot;sd&quot;, &quot;quantile2&quot;, default_convergence_measures())
fit4otherpars &lt;- fit4pars %&gt;% dplyr::filter(!grepl(&#39;_id&#39;,variable)) %&gt;%
  dplyr::filter(!grepl(&#39;prior&#39;,variable)) %&gt;%
  dplyr::filter(!grepl(&#39;z_&#39;,variable))

print(fit4otherpars)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   variable               mean     sd        q5       q95  rhat ess_bulk ess_tail
##   &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
## 1 b_alpha_Intercept    2.99   0.0197    2.95      3.02    1.00    3771.    5404.
## 2 b_alpha_dose_adj     0.0861 0.0106    0.0688    0.104   1.00    3979.    5040.
## 3 b_beta_Intercept     0.987  0.0247    0.946     1.03    1.00    3486.    5134.
## 4 b_beta_dose_adj     -0.106  0.0131   -0.127    -0.0844  1.00    3855.    5732.
## 5 sigma                1.06   0.0517    0.981     1.15    1.00   10136.   10314.
## 6 lp__              -468.     7.47   -481.     -457.      1.00    2720.    4987.</code></pre>
<p>These estimates look good, close to the truth.</p>
<p>Finishing with the pairs lots:</p>
<pre class="r"><code># a few parameters for each dose
#low dose
pairs(fit4, variable = variables(fit4)[c(1:4,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_4_pair_plots-1.png" width="672" /></p>
<pre class="r"><code>#medium dose
pairs(fit4, variable = variables(fit4)[c(8:11,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_4_pair_plots-2.png" width="672" /></p>
<pre class="r"><code>#high dose
pairs(fit4, variable = variables(fit4)[c(16:19,25)])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/mod_4_pair_plots-3.png" width="672" /></p>
<p>The strong correlations between parameters are reduced, the same we say with the <code>ulam</code> models.</p>
<p>As was the case for the <code>ulam</code> fits, model 4 seems to perform overall best.</p>
</div>
<div id="comparing-all-models" class="section level2">
<h2>Comparing all models</h2>
<p>We can repeat the model comparison we did above, now including all 4 models.</p>
<pre class="r"><code>fit1a &lt;- add_criterion(fit1,c(&quot;waic&quot;,&quot;loo&quot;))</code></pre>
<pre><code>## Warning: 
## 30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Warning: Found 6 observations with a pareto_k &gt; 0.7 in model &#39;fit1&#39;. It is
## recommended to set &#39;moment_match = TRUE&#39; in order to perform moment matching for
## problematic observations.</code></pre>
<pre class="r"><code>fit2a &lt;- add_criterion(fit2,c(&quot;waic&quot;,&quot;loo&quot;))</code></pre>
<pre><code>## Warning: 
## 5 (1.9%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre class="r"><code>fit3a &lt;- add_criterion(fit3,c(&quot;waic&quot;,&quot;loo&quot;))</code></pre>
<pre><code>## Warning: 
## 29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Warning: Found 5 observations with a pareto_k &gt; 0.7 in model &#39;fit3&#39;. It is
## recommended to set &#39;moment_match = TRUE&#39; in order to perform moment matching for
## problematic observations.</code></pre>
<pre class="r"><code>fit4a &lt;- add_criterion(fit4,c(&quot;waic&quot;,&quot;loo&quot;))</code></pre>
<pre><code>## Warning: 
## 27 (10.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.</code></pre>
<pre><code>## Warning: Found 6 observations with a pareto_k &gt; 0.7 in model &#39;fit4&#39;. It is
## recommended to set &#39;moment_match = TRUE&#39; in order to perform moment matching for
## problematic observations.</code></pre>
<pre class="r"><code>compall1 &lt;- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = &quot;waic&quot;)
compall2 &lt;- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = &quot;loo&quot;)
print(compall1, simplify = FALSE)</code></pre>
<pre><code>##       elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic
## fit4a    0.0       0.0  -415.7      11.7         42.7    4.3     831.4   23.4 
## fit1a   -0.6       1.2  -416.3      11.7         43.5    4.3     832.5   23.4 
## fit3a   -0.6       1.3  -416.3      11.7         43.5    4.3     832.6   23.4 
## fit2a -473.6      23.0  -889.4      22.4         10.3    3.0    1778.7   44.8</code></pre>
<pre class="r"><code>print(compall2, simplify = FALSE)</code></pre>
<pre><code>##       elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic
## fit4a    0.0       0.0  -419.8     12.1        46.8    5.0    839.6   24.2  
## fit3a   -0.6       1.4  -420.4     12.1        47.6    5.0    840.7   24.2  
## fit1a   -1.1       1.5  -421.0     12.2        48.2    5.1    841.9   24.4  
## fit2a -469.6      23.0  -889.4     22.4        10.3    3.1   1778.9   44.8</code></pre>
<p>Model 4 is considered best by WAIC, though not by much. The above results, namely faster runtime and better estimates, speak more convincingly to the fact that model 4 is the best of these. The WAIC are still different from those reported by `rethinking``. Again, I don’t know why.</p>
</div>
<div id="prior-exploration" class="section level2">
<h2>Prior exploration</h2>
<p>Since <code>brms</code> has a way of specifying the model and priors that makes direct mapping to the mathematical model a bit more opaque, it is useful to explore if the models we run are what we think we run. <code>brms</code> has two helpful functions for looking at priors. One can help set priors before fitting, the other shows priors after fitting.
To make the output manageable, we look at the simplest model, model 2. This looks as follows</p>
<pre class="r"><code>#defining model again
m2aeqs &lt;- bf(outcome ~ exp(alpha)*log(time) - exp(beta)*time,
  alpha ~ 1 + dose_adj,
  beta  ~  1 + dose_adj,
  nl = TRUE)
preprior2 &lt;- get_prior(m2aeqs,data=fitdat,family=gaussian())
postprior2 &lt;- prior_summary(fit2)
print(preprior2)</code></pre>
<pre><code>##                prior class      coef group resp dpar nlpar lb ub       source
##  student_t(3, 0, 23) sigma                                  0         default
##               (flat)     b                           alpha            default
##               (flat)     b  dose_adj                 alpha       (vectorized)
##               (flat)     b Intercept                 alpha       (vectorized)
##               (flat)     b                            beta            default
##               (flat)     b  dose_adj                  beta       (vectorized)
##               (flat)     b Intercept                  beta       (vectorized)</code></pre>
<pre class="r"><code>print(postprior2)</code></pre>
<pre><code>##            prior class      coef group resp dpar nlpar lb ub  source
##           (flat)     b                           alpha       default
##   normal(0.3, 1)     b  dose_adj                 alpha          user
##     normal(2, 2)     b Intercept                 alpha          user
##           (flat)     b                            beta       default
##  normal(-0.3, 1)     b  dose_adj                  beta          user
##   normal(0.5, 2)     b Intercept                  beta          user
##     cauchy(0, 1) sigma                                  0       user</code></pre>
<p>The first output shows the priors as the model sees them, before we apply any settings. It uses defaults. The second output shows the actual priors used when fitting the model, which are the ones we set. I find these functions and the information useful, but overall it’s still a bit confusing to me. For instance why are there those <code>flat</code> entries in there? I don’t know what they mean.</p>
<p>It gets worse for bigger models, and here things get confusing to me. This is looking at the priors for models 1,3 and 4. Recall that we expect <span class="math inline">\(2(N+1)+1\)</span> priors for models 1 and 3, and <span class="math inline">\(2(N+1+1)+1\)</span> for model 4. Since our data has 24 samples, we should find 51 and 53 priors.
Here is what we get:</p>
<pre class="r"><code>postprior1 &lt;- prior_summary(fit1)
postprior3 &lt;- prior_summary(fit3)
postprior4 &lt;- prior_summary(fit4)
print(paste(nrow(postprior1),nrow(postprior3),nrow(postprior4)))</code></pre>
<pre><code>## [1] &quot;53 53 13&quot;</code></pre>
<p>Closer inspection shows that for models 1 and 3, the priors include those strange <code>flat</code> ones that only have a class but no coefficient. My guess is those are not “real”, and thus we actually have the right number of priors/parameters. This can be checked by looking at the names of all the parameters for say model 1. Here they are:</p>
<pre class="r"><code>names(m1post)</code></pre>
<pre><code>##   [1] &quot;b_alpha_id1&quot;            &quot;b_alpha_id2&quot;            &quot;b_alpha_id3&quot;           
##   [4] &quot;b_alpha_id4&quot;            &quot;b_alpha_id5&quot;            &quot;b_alpha_id6&quot;           
##   [7] &quot;b_alpha_id7&quot;            &quot;b_alpha_id8&quot;            &quot;b_alpha_id9&quot;           
##  [10] &quot;b_alpha_id10&quot;           &quot;b_alpha_id11&quot;           &quot;b_alpha_id12&quot;          
##  [13] &quot;b_alpha_id13&quot;           &quot;b_alpha_id14&quot;           &quot;b_alpha_id15&quot;          
##  [16] &quot;b_alpha_id16&quot;           &quot;b_alpha_id17&quot;           &quot;b_alpha_id18&quot;          
##  [19] &quot;b_alpha_id19&quot;           &quot;b_alpha_id20&quot;           &quot;b_alpha_id21&quot;          
##  [22] &quot;b_alpha_id22&quot;           &quot;b_alpha_id23&quot;           &quot;b_alpha_id24&quot;          
##  [25] &quot;b_alpha_dose_adj&quot;       &quot;b_beta_id1&quot;             &quot;b_beta_id2&quot;            
##  [28] &quot;b_beta_id3&quot;             &quot;b_beta_id4&quot;             &quot;b_beta_id5&quot;            
##  [31] &quot;b_beta_id6&quot;             &quot;b_beta_id7&quot;             &quot;b_beta_id8&quot;            
##  [34] &quot;b_beta_id9&quot;             &quot;b_beta_id10&quot;            &quot;b_beta_id11&quot;           
##  [37] &quot;b_beta_id12&quot;            &quot;b_beta_id13&quot;            &quot;b_beta_id14&quot;           
##  [40] &quot;b_beta_id15&quot;            &quot;b_beta_id16&quot;            &quot;b_beta_id17&quot;           
##  [43] &quot;b_beta_id18&quot;            &quot;b_beta_id19&quot;            &quot;b_beta_id20&quot;           
##  [46] &quot;b_beta_id21&quot;            &quot;b_beta_id22&quot;            &quot;b_beta_id23&quot;           
##  [49] &quot;b_beta_id24&quot;            &quot;b_beta_dose_adj&quot;        &quot;sigma&quot;                 
##  [52] &quot;prior_b_alpha_id1&quot;      &quot;prior_b_alpha_id2&quot;      &quot;prior_b_alpha_id3&quot;     
##  [55] &quot;prior_b_alpha_id4&quot;      &quot;prior_b_alpha_id5&quot;      &quot;prior_b_alpha_id6&quot;     
##  [58] &quot;prior_b_alpha_id7&quot;      &quot;prior_b_alpha_id8&quot;      &quot;prior_b_alpha_id9&quot;     
##  [61] &quot;prior_b_alpha_id10&quot;     &quot;prior_b_alpha_id11&quot;     &quot;prior_b_alpha_id12&quot;    
##  [64] &quot;prior_b_alpha_id13&quot;     &quot;prior_b_alpha_id14&quot;     &quot;prior_b_alpha_id15&quot;    
##  [67] &quot;prior_b_alpha_id16&quot;     &quot;prior_b_alpha_id17&quot;     &quot;prior_b_alpha_id18&quot;    
##  [70] &quot;prior_b_alpha_id19&quot;     &quot;prior_b_alpha_id20&quot;     &quot;prior_b_alpha_id21&quot;    
##  [73] &quot;prior_b_alpha_id22&quot;     &quot;prior_b_alpha_id23&quot;     &quot;prior_b_alpha_id24&quot;    
##  [76] &quot;prior_b_alpha_dose_adj&quot; &quot;prior_b_beta_id1&quot;       &quot;prior_b_beta_id2&quot;      
##  [79] &quot;prior_b_beta_id3&quot;       &quot;prior_b_beta_id4&quot;       &quot;prior_b_beta_id5&quot;      
##  [82] &quot;prior_b_beta_id6&quot;       &quot;prior_b_beta_id7&quot;       &quot;prior_b_beta_id8&quot;      
##  [85] &quot;prior_b_beta_id9&quot;       &quot;prior_b_beta_id10&quot;      &quot;prior_b_beta_id11&quot;     
##  [88] &quot;prior_b_beta_id12&quot;      &quot;prior_b_beta_id13&quot;      &quot;prior_b_beta_id14&quot;     
##  [91] &quot;prior_b_beta_id15&quot;      &quot;prior_b_beta_id16&quot;      &quot;prior_b_beta_id17&quot;     
##  [94] &quot;prior_b_beta_id18&quot;      &quot;prior_b_beta_id19&quot;      &quot;prior_b_beta_id20&quot;     
##  [97] &quot;prior_b_beta_id21&quot;      &quot;prior_b_beta_id22&quot;      &quot;prior_b_beta_id23&quot;     
## [100] &quot;prior_b_beta_id24&quot;      &quot;prior_b_beta_dose_adj&quot;  &quot;prior_sigma&quot;           
## [103] &quot;lprior&quot;                 &quot;lp__&quot;                   &quot;.chain&quot;                
## [106] &quot;.iteration&quot;             &quot;.draw&quot;</code></pre>
<p>We can see that there are the right number of both priors and posterior parameters, namely 2 times 24 for the individual level parameters, plus 2 dose parameters and <span class="math inline">\(\sigma\)</span>.</p>
<p>I find model 4 more confusing. Here is the full list of priors:</p>
<pre class="r"><code>print(postprior4)</code></pre>
<pre><code>##            prior class      coef group resp dpar nlpar lb ub       source
##           (flat)     b                           alpha            default
##   normal(0.3, 1)     b  dose_adj                 alpha               user
##     normal(2, 1)     b Intercept                 alpha               user
##           (flat)     b                            beta            default
##  normal(-0.3, 1)     b  dose_adj                  beta               user
##   normal(0.5, 1)     b Intercept                  beta               user
##     cauchy(0, 1)    sd                           alpha  0            user
##     cauchy(0, 1)    sd                            beta  0            user
##     cauchy(0, 1)    sd              id           alpha  0    (vectorized)
##     cauchy(0, 1)    sd Intercept    id           alpha  0    (vectorized)
##     cauchy(0, 1)    sd              id            beta  0    (vectorized)
##     cauchy(0, 1)    sd Intercept    id            beta  0    (vectorized)
##     cauchy(0, 1) sigma                                  0            user</code></pre>
<p>And this shows the names of all parameters</p>
<pre class="r"><code>names(m4post)</code></pre>
<pre><code>##   [1] &quot;b_alpha_Intercept&quot;         &quot;b_alpha_dose_adj&quot;         
##   [3] &quot;b_beta_Intercept&quot;          &quot;b_beta_dose_adj&quot;          
##   [5] &quot;sd_id__alpha_Intercept&quot;    &quot;sd_id__beta_Intercept&quot;    
##   [7] &quot;sigma&quot;                     &quot;r_id__alpha[1,Intercept]&quot; 
##   [9] &quot;r_id__alpha[2,Intercept]&quot;  &quot;r_id__alpha[3,Intercept]&quot; 
##  [11] &quot;r_id__alpha[4,Intercept]&quot;  &quot;r_id__alpha[5,Intercept]&quot; 
##  [13] &quot;r_id__alpha[6,Intercept]&quot;  &quot;r_id__alpha[7,Intercept]&quot; 
##  [15] &quot;r_id__alpha[8,Intercept]&quot;  &quot;r_id__alpha[9,Intercept]&quot; 
##  [17] &quot;r_id__alpha[10,Intercept]&quot; &quot;r_id__alpha[11,Intercept]&quot;
##  [19] &quot;r_id__alpha[12,Intercept]&quot; &quot;r_id__alpha[13,Intercept]&quot;
##  [21] &quot;r_id__alpha[14,Intercept]&quot; &quot;r_id__alpha[15,Intercept]&quot;
##  [23] &quot;r_id__alpha[16,Intercept]&quot; &quot;r_id__alpha[17,Intercept]&quot;
##  [25] &quot;r_id__alpha[18,Intercept]&quot; &quot;r_id__alpha[19,Intercept]&quot;
##  [27] &quot;r_id__alpha[20,Intercept]&quot; &quot;r_id__alpha[21,Intercept]&quot;
##  [29] &quot;r_id__alpha[22,Intercept]&quot; &quot;r_id__alpha[23,Intercept]&quot;
##  [31] &quot;r_id__alpha[24,Intercept]&quot; &quot;r_id__beta[1,Intercept]&quot;  
##  [33] &quot;r_id__beta[2,Intercept]&quot;   &quot;r_id__beta[3,Intercept]&quot;  
##  [35] &quot;r_id__beta[4,Intercept]&quot;   &quot;r_id__beta[5,Intercept]&quot;  
##  [37] &quot;r_id__beta[6,Intercept]&quot;   &quot;r_id__beta[7,Intercept]&quot;  
##  [39] &quot;r_id__beta[8,Intercept]&quot;   &quot;r_id__beta[9,Intercept]&quot;  
##  [41] &quot;r_id__beta[10,Intercept]&quot;  &quot;r_id__beta[11,Intercept]&quot; 
##  [43] &quot;r_id__beta[12,Intercept]&quot;  &quot;r_id__beta[13,Intercept]&quot; 
##  [45] &quot;r_id__beta[14,Intercept]&quot;  &quot;r_id__beta[15,Intercept]&quot; 
##  [47] &quot;r_id__beta[16,Intercept]&quot;  &quot;r_id__beta[17,Intercept]&quot; 
##  [49] &quot;r_id__beta[18,Intercept]&quot;  &quot;r_id__beta[19,Intercept]&quot; 
##  [51] &quot;r_id__beta[20,Intercept]&quot;  &quot;r_id__beta[21,Intercept]&quot; 
##  [53] &quot;r_id__beta[22,Intercept]&quot;  &quot;r_id__beta[23,Intercept]&quot; 
##  [55] &quot;r_id__beta[24,Intercept]&quot;  &quot;prior_b_alpha_Intercept&quot;  
##  [57] &quot;prior_b_alpha_dose_adj&quot;    &quot;prior_b_beta_Intercept&quot;   
##  [59] &quot;prior_b_beta_dose_adj&quot;     &quot;prior_sigma&quot;              
##  [61] &quot;prior_sd_id&quot;               &quot;prior_sd_id__1&quot;           
##  [63] &quot;lprior&quot;                    &quot;lp__&quot;                     
##  [65] &quot;z_1[1,1]&quot;                  &quot;z_1[1,2]&quot;                 
##  [67] &quot;z_1[1,3]&quot;                  &quot;z_1[1,4]&quot;                 
##  [69] &quot;z_1[1,5]&quot;                  &quot;z_1[1,6]&quot;                 
##  [71] &quot;z_1[1,7]&quot;                  &quot;z_1[1,8]&quot;                 
##  [73] &quot;z_1[1,9]&quot;                  &quot;z_1[1,10]&quot;                
##  [75] &quot;z_1[1,11]&quot;                 &quot;z_1[1,12]&quot;                
##  [77] &quot;z_1[1,13]&quot;                 &quot;z_1[1,14]&quot;                
##  [79] &quot;z_1[1,15]&quot;                 &quot;z_1[1,16]&quot;                
##  [81] &quot;z_1[1,17]&quot;                 &quot;z_1[1,18]&quot;                
##  [83] &quot;z_1[1,19]&quot;                 &quot;z_1[1,20]&quot;                
##  [85] &quot;z_1[1,21]&quot;                 &quot;z_1[1,22]&quot;                
##  [87] &quot;z_1[1,23]&quot;                 &quot;z_1[1,24]&quot;                
##  [89] &quot;z_2[1,1]&quot;                  &quot;z_2[1,2]&quot;                 
##  [91] &quot;z_2[1,3]&quot;                  &quot;z_2[1,4]&quot;                 
##  [93] &quot;z_2[1,5]&quot;                  &quot;z_2[1,6]&quot;                 
##  [95] &quot;z_2[1,7]&quot;                  &quot;z_2[1,8]&quot;                 
##  [97] &quot;z_2[1,9]&quot;                  &quot;z_2[1,10]&quot;                
##  [99] &quot;z_2[1,11]&quot;                 &quot;z_2[1,12]&quot;                
## [101] &quot;z_2[1,13]&quot;                 &quot;z_2[1,14]&quot;                
## [103] &quot;z_2[1,15]&quot;                 &quot;z_2[1,16]&quot;                
## [105] &quot;z_2[1,17]&quot;                 &quot;z_2[1,18]&quot;                
## [107] &quot;z_2[1,19]&quot;                 &quot;z_2[1,20]&quot;                
## [109] &quot;z_2[1,21]&quot;                 &quot;z_2[1,22]&quot;                
## [111] &quot;z_2[1,23]&quot;                 &quot;z_2[1,24]&quot;                
## [113] &quot;.chain&quot;                    &quot;.iteration&quot;               
## [115] &quot;.draw&quot;</code></pre>
<p>To compare directly, this is the model we want:</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i,t}  &amp; \sim \mathrm{Normal}\left(\mu_{i,t}, \sigma\right) \\
\mu_{i,t} &amp;  =  \exp(\alpha_{i}) \log (t_{i}) -\exp(\beta_{i}) t_{i} \\
\alpha_{i} &amp;  =  a_{0,i} + a_1 \left(\log (D_i) - \log (D_m)\right)  \\
\beta_{i} &amp;  =  b_{0,i} + b_1 \left(\log (D_i) - \log (D_m)\right) \\
a_{0,i} &amp; \sim \mathrm{Normal}(\mu_a, \sigma_a) \\
b_{0,i} &amp; \sim \mathrm{Normal}(\mu_b, \sigma_a) \\
a_1 &amp; \sim \mathrm{Normal}(0.3, 1) \\
b_1 &amp; \sim \mathrm{Normal}(-0.3, 1) \\
\mu_a &amp; \sim \mathrm{Normal}(2, 1) \\
\mu_b &amp; \sim \mathrm{Normal}(0.5, 1) \\
\sigma &amp;  \sim \mathrm{HalfCauchy}(0,1)  \\
\sigma_a &amp; \sim \mathrm{HalfCauchy}(0,1)  \\
\sigma_b &amp; \sim \mathrm{HalfCauchy}(0,1)  
\end{aligned}
\]</span></p>
<p>If understand <code>brms</code> correctly, those <code>z_</code> parameters are internal adjustments to make things more efficient and can otherwise be ignored. That means we have 2 times 24 parameters for the individual levels that all start with <code>r_id</code>. Those correspond to the <span class="math inline">\(a_{0,i}\)</span> and <span class="math inline">\(b_{0,1}\)</span>, and they don’t have pre-defined priors, since they are computed based on other parameters. Then we have 2 dose parameters, which map to <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span>, both come with priors. We have 2 <code>_Intercept</code> parameters, which correspond to <span class="math inline">\(\mu_a\)</span> and <span class="math inline">\(\mu_b\)</span>, again with priors. We have <span class="math inline">\(\sigma\)</span> with prior, and the two <code>sd_id</code> parameters seem to be those we call <span class="math inline">\(\sigma_a\)</span> and <span class="math inline">\(\sigma_b\)</span> in our equations.</p>
<p>So it looks like there is a match between our mathematical model we want, and the way we implemented it in <code>brms</code>. Still, I find the <code>brms</code> notation confusing and not that easy to follow. In that respect I much prefer <code>ulam/rethinking</code>.</p>
<p>In any case, I somewhat convinced myself that I’m fitting the same models here with <code>brms</code> that I’m fitting with <code>ulam</code> (which still leaves me confused about the differences in WAIC).</p>
</div>
</div>
<div id="computing-predictions" class="section level1">
<h1>Computing predictions</h1>
<p>Looking at tables of estimates as we did so far is somewhat useful, but nothing can beat graphical inspection. So let’s plot the predictions implied by the fits for the models. The general strategy for that is to use the parameter estimates in the posterior, put them in the model, and compute the predictions.
While the <code>rethinking</code> package had <code>sim</code> and <code>link</code>, for <code>brms</code> those functions are <code>fitted</code> and <code>predict</code>.</p>
<p>The code below produces predictions, both for the deterministic mean trajectory <span class="math inline">\(\mu\)</span>, and the actual outcome, <span class="math inline">\(Y\)</span>, which has added variation.</p>
<pre class="r"><code>#this will contain all the predictions from the different models
fitpred = vector(mode = &quot;list&quot;, length = length(fl))

# load the data we used for fitting
simdat &lt;- readRDS(&quot;simdat.Rds&quot;)
#pull our the data set we used for fitting
#if you fit a different one of the simulated datasets, change accordingly
fitdat &lt;- simdat$m3
#small data adjustment for plotting
plotdat &lt;- fitdat %&gt;% data.frame() %&gt;% mutate(id = as.factor(id)) %&gt;% mutate(dose = dose_cat)


# we are looping over each fitted model
for (n in 1:length(fl))
{
  #get current model
  nowmodel = fl[[n]]$fit

  #make new data for which we want predictions
  #specifically, more time points so the curves are smoother
  timevec = seq(from = 0.1, to = max(fitdat$time), length=100)
  Ntot = max(fitdat$id)
  #data used for predictions
  preddat = data.frame( id = sort(rep(seq(1,Ntot),length(timevec))),
                        time = rep(timevec,Ntot),
                        dose_adj = 0
  )
  #add right dose information for each individual
  for (k in 1:Ntot)
  {
    #dose for a given individual
    nowdose = unique(fitdat$dose_adj[fitdat$id == k])
    nowdose_cat = unique(fitdat$dose_cat[fitdat$id == k])
    #assign that dose
    #the categorical values are just for plotting
    preddat[(preddat$id == k),&quot;dose_adj&quot;] = nowdose
    preddat[(preddat$id == k),&quot;dose_cat&quot;] = nowdose_cat
  }

  # estimate and CI for parameter variation
  #brms equivalent to rethinking::link
  #doing 89% CI
  meanpred &lt;- fitted(nowmodel, newdata = preddat, probs = c(0.055, 0.945) )

  # estimate and CI for prediction intervals
  # the predictions factor in additional uncertainty around the mean (mu)
  # as indicated by sigma
  # this is equivalent to rethinking::sim()
  outpred &lt;- predict(nowmodel, newdata = preddat, probs = c(0.055, 0.945) )


  #place all predictions into a data frame
  #and store in a list for each model
  fitpred[[n]] = data.frame(id = as.factor(preddat$id),
                            dose = as.factor(preddat$dose_cat),
                            predtime = preddat$time,
                            Estimate = meanpred[,&quot;Estimate&quot;],
                            Q89lo = meanpred[,&quot;Q5.5&quot;],
                            Q89hi = meanpred[,&quot;Q94.5&quot;],
                            Qsimlo = outpred[,&quot;Q5.5&quot;],
                            Qsimhi = outpred[,&quot;Q94.5&quot;]
  )
}


#########################
# generate plots showing data and model predictions
#########################</code></pre>
</div>
<div id="creating-plots-of-the-results" class="section level1">
<h1>Creating plots of the results</h1>
<p>Now that we got the predictions computed, we can plot them and compare with the data.
I’m showing the same uncertainty intervals I used for <code>rethinking</code> to make comparison easy.</p>
<pre class="r"><code>#storing all plots
plotlist = vector(mode = &quot;list&quot;, length = length(fl))

#adding titles to plots
titles = c(&#39;model 1&#39;,&#39;model 2a&#39;,&#39;model 3&#39;,&#39;model 4&#39;)

#again looping over all models, making a plot for each
for (n in 1:length(fl))
{
  # ===============================================
  plotlist[[n]] &lt;- ggplot(data = fitpred[[n]], aes(x = predtime, y = Estimate, group = id, color = dose ) ) +
    geom_line() +
    geom_ribbon(aes(x=predtime, ymin=Q89lo, ymax=Q89hi, fill = dose, color = NULL), alpha=0.3, show.legend = F) +
    geom_ribbon(aes(x=predtime, ymin=Qsimlo, ymax=Qsimhi, fill = dose, color = NULL), alpha=0.1, show.legend = F) +
    geom_point(data = plotdat, aes(x = time, y = outcome, group = id, color = dose), shape = 1, size = 2) +
    scale_y_continuous(limits = c(-30,50)) +
    labs(y = &quot;Virus load&quot;,
         x = &quot;days post infection&quot;) +
    theme_minimal() +
    ggtitle(titles[n])
  ggsave(file = paste0(titles[n],&quot;.png&quot;), plotlist[[n]], dpi = 300, units = &quot;in&quot;, width = 7, height = 7)
}



#########################
# show the plots
#########################</code></pre>
</div>
<div id="showing-the-plots" class="section level1">
<h1>Showing the plots</h1>
<p>Here are the plots for all models we considered.</p>
<p>It’s a bit hard to see, but each plot contains for each individual the data as symbols, the estimated mean as line, and the 89% credible interval and prediction interval as shaded areas.</p>
<pre class="r"><code>plot(plotlist[[1]])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/showplots-1.png" width="672" /></p>
<pre class="r"><code>plot(plotlist[[3]])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/showplots-2.png" width="672" /></p>
<pre class="r"><code>plot(plotlist[[2]])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/showplots-3.png" width="672" /></p>
<pre class="r"><code>plot(plotlist[[4]])</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/showplots-4.png" width="672" /></p>
<p>The figures reflect what we found above numerically.</p>
<!-- Mirroring the findings from above, the models are very similar. In fact, it's hard to tell any difference by just looking at the plots (but they are slightly different, I checked).  So overall, the figures make sense and indicates that apart from models 2/2a, the other models are performing well. I consider model 4/4a the most suitable one. But now looking at these figures and seeing the nice matches between data and models, I'm still a bit puzzled by the fact that $a_1$ and $b_1$ are not that well estimated... -->
</div>
<div id="summary-and-continuation" class="section level1">
<h1>Summary and continuation</h1>
<p>To sum it up, we repeated our previous fitting, now using the <code>brms</code> package instead of <code>rethinking</code>. While the two packages have different syntax, the models we fit are the same and thus the results are very close too. That’s comforting. If one approach had produced very different results, it would have meant something was wrong. Of course, as I was writing this series of posts, that happened many times and it took me a while to figure out how to get <code>brms</code> to do what I wanted it to 😁.</p>
<p>I like the approach of using both packages. It adds an extra layer of robustness. The <code>rethinking</code> code is very close to the math and thus quickly implemented and probably a good first step. <code>brms</code> has some features that go beyond what <code>rethinking</code> can (easily) do, so moving on to re-implementing models in <code>brms</code> and using that code for producing the final results can make sense.</p>
<p>This ends the main part of the tutorial (for now). There were several topics I wanted to discuss that didn’t fit here. If you are interested in some further musings, you can hop to <a href="/posts/longitudinal-multilevel-bayesian-analysis-4/">this post</a>, where I discuss a few further topics and variations.</p>
</div>
