---
title: Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3  
summary: Part 3 of a tutorial showing how to fit Bayesian models using the `brms` package.
author: Andreas Handel
date: '2022-02-24'
lastMod: "2022-04-18"
slug: longitudinal-multilevel-bayesian-analysis-3
categories: 
- R
- Data Analysis
tags: 
- R
- Data Analysis
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
---

```{r setup, include=FALSE}
library(emoji)
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE, cache=FALSE}
knitr::read_chunk('brmsfitmodels.R')
knitr::read_chunk('brmsexploremodels.R')
```


**This is work in progress!**

This is part 3 of a tutorial illustrating how one can use the `brms` and `rethinking` R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.

I assume you've read both [part 1](/posts/longitudinal-multilevel-bayesian-analysis-1/), and [part 2](/posts/longitudinal-multilevel-bayesian-analysis-2/) otherwise this post won't make much sense.


# Introduction

In the previous post, I showed how to fit the data using the `rethinking` package. Now I'm re-doing it using `brms`. The [`brms` package](https://paul-buerkner.github.io/brms/) is a widely used and very powerful tool to interface with Stan. It has overall more capabilities compared to `rethinking`. In my opinion, the main disadvantage is that it is often not as obvious how to go from mathematical model to code, unless one has a good bit of experience jumping between at times terse formula notation of `brms` and the model equations. I'm not there yet, so I currently prefer to start with `rethinking`. But since `brms` can do things that are not as easy (or impossible) with `rethinking`, it seems good to know how to use both.

Also, comparing results using two different numerical packages is always good (even though both use `Stan` underneath, so in some sense those are not truly independent implementations).

Again as previously, because fitting the models can take a good bit of time, I wrote separate `R` scripts for the fitting and the exploring parts. The code chunks from those scripts are shown below. The manual effort and slower pace of copying and pasting the code chunks from this tutorial and re-produce them can help in learning, but if you just want to get all the code from this post you can find it [here](/posts/longitudinal-multilevel-bayesian-analysis-3/brmsfitmodels.R) and [here](/posts/longitudinal-multilevel-bayesian-analysis-3/brmsexploremodels.R).



# R Setup

As always, make sure these packages are installed. `brms` uses the [Stan Bayesian modeling engine](https://mc-stan.org/). If you did the fitting with `rethinking` tutorial, you'll have it already installed, otherwise you'll need to install it. It is in my experience mostly seamless, but at times it seems to be tricky. I generally follow the instructions on the [`rethinking` website](https://github.com/rmcelreath/rethinking) and it has so far always worked for me. It might need some fiddling, but you should be able to get them all to work.


```{r, packages, message = FALSE, warning = FALSE}
```



# Data loading

We'll jump right in and load the data we generated in the previous tutorial.


```{r, data}
```


# Fitting with `brms`

We'll fit the different models we discussed in [part 1](/posts/longitudinal-multilevel-bayesian-analysis-1/), now using the `brms` package. The main function in that package, which does the fitting using Stan, is `brm`. 

First, we'll specify each model. We'll do that first, then run them all in a single loop. 
Since we determined when using `ulam`/`rethinking` that our model 2 was a bad model, and model 4 and 4a didn't lead to much of a difference, I'm skipping those here and only do models 1, 2a, 3 and 4.


## Model 1

The model with individual-level and dose-level effects, all priors fixed. This model has $2N+2+1$ parameters. $N$ each for the individual-level intercept for $\alpha$ and $\beta$, one dose-level parameter for each, and 1 overall deviation, $\sigma$ for the outcome distribution. 

```{r, model-1}
```


Notice how this notation in `brms` looks quite a bit different than the mathematical equations or the `ulam` implementation. That's a part I don't particularly like about `brms`, the very condensed formula notation. It takes time getting used to and it always requires extra checking to ensure the model implemented in code corresponds to the mathematical model. One can check by looking at the priors and make sure they look as expected. We'll do that below after we fit.

## Model 2a 

This is the easiest model, with only population level effects for intercept and dose, so only 2+2+1 parameters.

```{r, model-2a}
```


## Model 3

This is the same as model 1 but with different values for the priors.


```{r, model-3}
```


## Model 4

This is the adaptive-pooling multi-level model where priors are estimated.
Here we have for each main parameter ($\alpha$ and $\beta$) an overall mean and standard deviation, and N individual intercepts, so 2 times 1+1+N. And of course we still have the 2 dose-related parameters and the overall standard deviation, so a total of 2*(1+1+N)+2+1 parameters.

```{r, model-4}
```


## Combine models

To make our lives easier below, we combine all models and priors into lists.

```{r, combinemodels}
```



## Fitting setup

We define some general values for the fitting. Since the starting values depend on number of chains, we need to do this setup first.


```{r, fittingsetup}
```


## Setting starting values

We'll again set starting values, as we did for `ulam/rethinking`.
Note that `brms` needs them in a somewhat different form, namely as list of lists for each model, one list for each chain.
Ideally, one should set different values for each chain, and then check that each chain ends up at the same posterior. For simplicity, I'm not doing that here but instead I'm starting each chain with the same value.

```{r, startvalues}
```



## Model fitting

We'll use the same strategy to loop though all models and fit them. 
The fitting code is pretty much the same as the previous one for `rethinking/ulam`, only now the fitting is done calling the `brm`.

```{r, modelfitting, eval = FALSE}
```

You'll likely find that model 1 takes the longest, the other ones run faster. You can check the runtime for each model by looking at `fl[[n]]$runtime`. It's useful to first run with few iterations (100s instead of 1000s), make sure everything works in principle, then do a "final" long run with longer chains.


# Explore model fits

```{r, loadfits}
```

As was the case for `ulam/rethinking`, you can inspect the model fits by looking at traceplots using the `plot` function. You also get useful information using `summary`, e.g. `summary(fl[[1]]$fit)`. Again, explore your model fits carefully. Since those functions produce a lot of output, I'm skipping them again and go straight to something that produces outputs similar to those we got using the `precis` function in `rethinking`. 

We are using the `posterior` package for this, which is used to process results (i.e., posterior distributions) of a `brms` fit model. The `posterior` package replaces functionality that used to be part of `brms`, e.g., the `posterior_summary()` and `posterior_samples()` functions.

## Models 1 and 3

Those are the same models, just with different priors. So we should expect the posterior to be similar.

```{r mod_1_3_exploration}
```

Note the different naming of the parameters in `brms`. It's unfortunately not possible (as far as I know) to get the names match the mathematical model. The parameters that have `dose` in their names are the ones we called $a_1$ and $b_1$ in our models. You can ignore the other entries. 

Some of the R-hat values for model 1 are not at 1, and the ESS (the effective sample size) is small. Model 1 also took long to run. That all indicates that the priors for model 3 are better.

We can also compare the models as we did for `rethinking` using these lines of code:

```{r mod_1_3_comparison}
```

We find that as for the `ulam` fits, the estimates for $a_0$, $b_0$ and $\sigma$ are similar, but that's not the case for $a_1$ and $b_1$. The $b_1$ estimates are similar to those for `ulam` model 3, and not close to the true value. I'm not really sure what to make of that. 



## Comparison with the truth and `ulam`

Since the models are the same as those we previously fit with `ulam`, only a different `R` package is used to run them, we should expect very similar results. That is indeed what we find, the parameter estimates are very similar. The same holds for model performance as measured by WAIC. 

That's good, because we should expect that if we fit the same models, results should - up to numerical/sampling differences - be the same, no matter what software implementation we use. It also suggests that we did things right - or made the same mistake in both implementations! `r emoji::emoji('grin')`.



## Model 2a

This is the model with only population-level estimates.

```{r mod_2a_exploration}
```

The parameters that have `_Intercept` in their name are what we called $\mu_a$ and $\mu_b$, the onse containing `_dose` are our $a_1$ and $b_1$. We find pretty much the same results we found using `ulam`.


## Model 4


```{r mod_4_exploration}
```




## Comparing all models


We can repeat the model comparison we did above, now including all 4 models.

```{r mod_all_comparison}
```



## Prior exploration

Since `brms` has a way of specifying the model and priors that makes direct mapping to the mathematical model a bit more opaque, it is useful to explore if the models we run are what we think we run. `brms` as two helpful functions for looking at priors. One can help set priors before fitting, the other shows priors after fitting. 
To make the output managable, we look at the simplest model, model 2. This looks as follows

```{r priorexploration}
```

The first output shows the priors as the model sees them, before we apply any settings. It uses defaults. The second output shows the actual priors used when fitting the model, which are the ones we set. 


# Computing predictions

Looking at tables of estimates as we did so far is somewhat useful, but nothing can beat graphical inspection. So let's plot the predictions implied by the fits for the models. The general strategy for that is to use the parameter estimates in the posterior, put them in the model, and compute the predictions. 
While the `rethinking` package had `sim` and `link`, for `brms` those functions are `fitted` and `predict`. 

The code below produces predictions, both for the deterministic mean trajectory $\mu$, and the actual outcome, $Y$, which has added variation.

```{r computepredictions}
```


# Creating plots of the results

Now that we got the predictions computed, we can plot them and compare with the data.
I'm showing the same uncertainty intervals I used for `rethinking` to make comparison easy.

```{r, makeplots, warning = FALSE, message = FALSE}
```


# Showing the plots 

Here are the plots for all models we considered.

It's a bit hard to see, but each plot contains for each individual the data as symbols, the estimated mean as line, and the 89% credible interval and prediction interval as shaded areas.


```{r, showplots, warning=FALSE}
```


The figures reflect what we found above numerically. 


<!-- Mirroring the findings from above, the models are very similar. In fact, it's hard to tell any difference by just looking at the plots (but they are slightly different, I checked).  So overall, the figures make sense and indicates that apart from models 2/2a, the other models are performing well. I consider model 4/4a the most suitable one. But now looking at these figures and seeing the nice matches between data and models, I'm still a bit puzzled by the fact that $a_1$ and $b_1$ are not that well estimated... -->




# Summary and continuation

To sum it up, we repeated our previous fitting, now using the `brms` package instead of `rethinking`. While the two packages have different syntax, the models we fit are the same and thus the results are very close too. That's comforting. If one approach had produced very different results, it would have meant something was wrong. Of course, as I was writing this series of posts, that happened many times and it took me a while to figure out how to get `brms` to do what I wanted it to `r emoji::emoji('grin')`.

I like the approach of using both packages. It adds an extra layer of robustness. The `rethinking` code is very close to the math and thus quickly implemented and probably a good first step. `brms` has some features that go beyond what `rethinking` can (easily) do, so moving on to re-implementing models in `brms` and using that code for producing the final results can make sense.

This ends the main part of the tutorial (for now). There were several topics I wanted to discuss that didn't fit here. If you are interested in some further musings, you can hop to [this post](/posts/longitudinal-multilevel-bayesian-analysis-4/), where I discuss a few further topics and variations. 







