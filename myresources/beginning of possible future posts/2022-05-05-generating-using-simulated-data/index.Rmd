---
title: Generating and using simulated data  
description: A brief discussion on generating simulated data.
author: Andreas Handel
date: '2022-05-05'
lastMod: "2022-05-05"
aliases: \r\n  -  generating-using-data
categories: 
- R
- Data Analysis
categories: 
- R
- Data Analysis
- Bayesian
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
---


```{r, include=FALSE, cache=FALSE}
#R script that contains code chunks shown below
knitr::read_chunk('example-code-artificial-data.R')
```


# Overview
In this tutorial, we discuss why and how you might want to create simulated (artificial/fake) data as part of your analysis.


# Goals
By the end of the tutorial, you should:

* Know about reasons why one might want to use simulated data.
* Be familiar with some ways to create and use simulated data.


# Introduction

Your goal is generally to analyze some real-world data to answer a question. So what's the point of producing simulated (also often referred to as artificial or fake) data? There are several good reasons why this can sometimes be useful.


# Reasons to use simulated data

## Knowing the truth

For your simulated data, you know exactly how it is created (because you do it). Let's say for your data analysis, you want to look at the relation between the daily dose of some [statin drug](https://en.wikipedia.org/wiki/Statin) a person takes, and if that has some association with their [LDL cholesterol](https://en.wikipedia.org/wiki/Low-density_lipoprotein). Depending on the exact structure of your data, you might fit various simple or complex models and get some answers. You can - and should! - look carefully at the results to see if they make sense. But you never know for sure if the model "got it right" since you don't know what the _true_ (whatever that might mean) answer is. In contrast, if you created the data, you know exactly what the true answer should be. We'll explore that below.


## No problems with sharing

Another important reason can be confidentiality. There are often limitations (real or perceived) for sharing the actual data. That's not a problem with simulated data. So if you want to be able to share your analysis with collaborators or readers when you publish, and for some reason you can't share the actual data, you can share simulated data. This allows others to run all your code and reproduce your analysis workflow, even if they won't get quite the same results since they are using simulated and not the real data.


## Easier to play around with

Often, real-world datasets are very messy and require a good bit of time to clean them up (wrangle them) to get data in a form that allows analysis. If you are not even sure if your idea/analysis makes sense, it would be a waste to spend a lot of time cleaning data that ends up not being useful. What you can do instead is simulate data that has similar structure and content to the real data, but since you create it, you can make sure it's clean and easy to work with. Then you try your analysis ideas on that simulated data. If it works ok, there's a chance (unfortunately no guarantee!) that it might also work for the real data. As you play around with your simulated data, you might realize that certain ideas/approaches you initially had won't work, and you can modify accordingly. Once your explorations using the simulated data suggest that your analysis plan can work, then you can start the often arduous task of cleaning and wrangling your real data.


# Ways to produce simulated data


## Make it all up

Completely making up data is the most flexible approach. You can try to produce simulated versions of the full dataset, or just parts that are of interest. Let's say in the statin-cholesterol example above, you real data set also contains information on the age and sex of the individuals, and if they have any pre-existing heart condition. Maybe in a first pass, you don't want to look at those and just explore the main statin-cholesterol relation. Then you can just simulate data for that part. You can add further information - including simulated characteristics that are not in the original data! - as needed. 

Of course, you want your made-up data to resemble the original one. Thus, you might want to structure things similarly, and produce simulated data values that are similar to those found in the original data. An easy way to do that is to look at the distribution for the original data (e.g., draw histograms or use R's `summary` function). Then create data that has values which are distributed similar to the original data.


## Scramble the data

If you already have the data in a clean form that you can work with, you can use that data and scramble things to make new data. Basically you can randomly re-arrange variable values for different individuals, such that the new data does not correspond to the real data anymore, but has the overall same structure, and the same values (just re-arranged between individuals). 

For instance you can take the age of each person, and randomly re-assign it to someone else. Note that this breaks potential patterns. For instance if in the original data, there happen to be an association between cholesterol and age, once you re-shuffled age values, this pattern will change. So the results you get from scrambled data will possibly be different, but since it has exactly the same structure and the same values as the original data, your whole analysis pipeline should work on the scrambled data. Of course, since you didn't create the data, you don't know the "truth" and as such can't assess fully if your analysis gives you the right results. 

If you have already done your analysis and want/need scrambled data for sharing, e.g. as part of a submission to a journal where you can't submit the original data, you can try to do the reshuffling in a way that preserves patterns you identified. For instance if you saw that age was correlated with LDL, you can do your reshuffle such that age and LDL pairs stay together, while other variables (e.g., sex) get randomly reassigned. Of course you need to change at least some values for each individual, you can't just move all variables for an individual from one row to another, it will still be the same data for that person, thus associated with potential confidentiality issues.


## Do a mix

It is of course entirely possible to combine the two approaches above. For instance you can start with the original data, do some scrambling if needed. Then you can replace some variables with simulated data you generate. This allows you to test your analysis more thoroughly since now you know what you put in, so you can check if you get that out.


# Examples

It's time to show these ideas with some examples. Itâ€™s often useful to go slow and type your own code, or copy and paste the code chunks below and execute one at a time. However, if you are in a hurry, you can find the code shown below in [this file](example-code-artificial-data.R).

## R Setup

We'll start by loading the packages used in the code below. As always, make sure these packages are installed.

```{r, packages, message=FALSE}
```


## Human Influenza infections

For the first example, we'll look at some real data from [this paper](https://royalsocietypublishing.org/doi/10.1098/rspb.2020.0496) which we previously published. As is good habit (and should be the standard), we supplied the data we used in that paper as part of the supplementary materials, which can be found [here](https://datadryad.org/stash/dataset/doi:10.5061/dryad.51c59zw4v). 

If you want to work along, go ahead and download the supplement, which is a zip file. Inside the zip file, find the _Clean Data_ folder and the `SympAct_Any_Pos.Rda` file. Copy that file to the location where you'll be placing your `R` script. Then start a new R script in that folder and we can fill it with code.

First, we load the data. Note that the authors (that would be us `r emoji::emoji('smirking face')`) used the wrong file ending, they called it an `.Rda` file, even though it is an `.Rds` file (for a discussion of the differences, see e.g. [here](http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata)).


### The data 

```{r, loaddata}
```

Next, we take a peek.


```{r, checkdata}
```

So it looks like these are 735 individuals (rows) and 63 variables (columns). A lot of them have names of symptoms and are coded as Yes/No. Some variables are harder to  understand, for instance without some meta-data/explanation, it is impossible to guess what `TransScore3F` stands for. Hopefully, your data came with some codebook/data dictionary/information sheet that explains what exactly everything means. For this specific data set, you can look through the supplementary materials to learn more. We won't delve into it now, and just pick out a few variables to illustrate the data generation process.


:::note
You might be wondering what the `dplyr::glimpse()` notation is about. This notation indicates explicitly from which package the function comes. Once you loaded a package, that's not necessary anymore, just using `glimpse()` would work equally. But it is often nice to quickly see what package a function comes from. Thus, you'll see us use the explicit notation often, though not always. It's a matter of being more explicit versus typing less. We switch back and forth.
:::




### Model testing






<!-- ## Analysis with a continuous outcome -->

<!-- For this example, we use the [LDL Cholesterol dataset](). This is in fact made-up data, as you can [read here.]() However, for the sake of this example, we pretend that this is real data and as part of the example below, we create new simulated data. Hopefully, this is not too confusing. -->

<!-- If you want to work along, [download the data]() and place it into the same folder as your R script (if you plan on placing it somewhere else, e.g. a dedicated `data` folder, adjust the path for loading the data accordingly). -->



<!-- ```{r} -->
<!-- # load the (made up - but we pretend it's real) data -->
<!-- d <- read.csv("ldldata.csv") -->
<!-- # take a quick look at the data -->
<!-- summary(d) -->
<!-- ``` -->

<!-- We see that there are various variables, some that are easy to guess what they are (e.g., `age`) some harder (e.g., `previous`). In general, together with your data, you should get a **codebook** or some other form of documentation that explains what all the data means. This meta-data is very important to prevent you from doing silly things (e.g. assuming that 0 is male and 1 is female if it's the other way around). [This can lead to serious problems](). For this dataset, you can get the codebook [on the site for the data](). -->

<!-- CONTINUE HERE -->



<!-- ## Analysis with a categorical outcome -->




# Further Resources

While it is generally not that difficult to write a bit of R code to generate your own data, sometimes it can be useful to reach for dedicated packages. R has a number of such packages. 

* The [coxed package](https://cran.r-project.org/web/packages/coxed/index.html) allows you - among other things - to generate longitudinal survival (time-to-event) data using its [`sim.survdata()` function](https://cran.r-project.org/web/packages/coxed/vignettes/simulating_survival_data.html).







